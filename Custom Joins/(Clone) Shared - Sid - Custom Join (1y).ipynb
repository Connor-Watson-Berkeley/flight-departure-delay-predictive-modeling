{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7158cf59-2a99-4dc3-9ecf-e1f11da9c4fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Implementing a Custom Join \n",
    "=============="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01dfa598-4202-432e-9698-5cc9a9bdafa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7249721-d156-4a4a-a9ed-316e38f5b37d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_version = \"1y\" # \"3m\", \"6m\", \"1y\", \"\" -> blank is full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a517ba1-c78f-4993-bfeb-3908644b9019",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from pyspark.sql.functions import col, regexp_replace, split, trim, to_timestamp, date_format, broadcast\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32f0a472-5907-4859-95ac-9f011e86a55e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Spark settings for efficient execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e8adcc5-935c-44ca-9eb9-72422152c637",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.get(\"spark.sql.session.timeZone\")\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7b5f0f9-1ac6-4fde-a737-f197eb2d83e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Enable Adaptive Query Execution (AQE) and optimizations for joins\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")                  # Adaptive Query Execution (auto optimizes joins/shuffles)\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")         # Automatically handles skewed joins\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")  # Merge small partitions post-shuffle\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 50 * 1024 * 1024) # Optional: increase broadcast threshold to 50MB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4ace4f9-5da3-448b-8c35-2d8986a118db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Reading back to see the spark settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a7a08d3-7302-4775-a6e3-680091759c26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Spark version:\", spark.version)\n",
    "print(\"AQE:\", spark.conf.get(\"spark.sql.adaptive.enabled\"))\n",
    "print(\"Skew join:\", spark.conf.get(\"spark.sql.adaptive.skewJoin.enabled\"))\n",
    "print(\"Coalesce:\", spark.conf.get(\"spark.sql.adaptive.coalescePartitions.enabled\"))\n",
    "print(\"AutoBroadcastJoinThreshold:\", spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3c636e3-473d-4dcf-a831-f3f9645252de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Paths & Defines\n",
    "\n",
    "data_BASE_DIR = \"dbfs:/mnt/mids-w261/\"\n",
    "display(dbutils.fs.ls(f\"{data_BASE_DIR}\")) \n",
    "\n",
    "# Section Folder\n",
    "section = \"4\"\n",
    "number = \"2\"\n",
    "section_DIR = f\"dbfs:/mnt/mids-w261/student-groups/Group_{section}_{number}\"\n",
    "\n",
    "# Subdirectories for organization\n",
    "processed_DIR = f\"{section_DIR}/processed\"\n",
    "checkpoints_DIR = f\"{section_DIR}/checkpoints\"\n",
    "intermediate_DIR = f\"{section_DIR}/intermediate\"\n",
    "\n",
    "# Output filenames (using variables for maintainability)\n",
    "FLIGHTS_WITH_STATION = f\"flights_with_station_{data_version}\"\n",
    "FLIGHTS_WEATHER_JOINED = f\"flights_weather_joined_{data_version}\"\n",
    "FLIGHTS_WITH_AIRPORTS = f\"flights_with_airports_{data_version}\"\n",
    "\n",
    "# Full paths for outputs\n",
    "flights_with_station_path = f\"{intermediate_DIR}/{FLIGHTS_WITH_STATION}\"\n",
    "flights_weather_joined_path = f\"{processed_DIR}/{FLIGHTS_WEATHER_JOINED}\"\n",
    "flights_with_airports_path = f\"{intermediate_DIR}/{FLIGHTS_WITH_AIRPORTS}\"\n",
    "\n",
    "# Check if section_DIR exists, print contents or create it\n",
    "try:\n",
    "    print(f\"✓ Section directory exists: {section_DIR}\")\n",
    "    print(\"\\nContents:\")\n",
    "    contents = dbutils.fs.ls(section_DIR)\n",
    "    for item in contents:\n",
    "        print(f\"  - {item.name} ({'DIR' if item.isDir() else 'FILE'}) - {item.size} bytes\")\n",
    "    print(f\"\\nTotal items: {len(contents)}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Section directory does not exist: {section_DIR}\")\n",
    "    print(\"Creating directory structure...\")\n",
    "    dbutils.fs.mkdirs(section_DIR)\n",
    "    print(f\"✓ Base directory created: {section_DIR}\")\n",
    "\n",
    "# Create subdirectories\n",
    "print(\"\\nCreating/verifying subdirectories...\")\n",
    "for subdir_name, subdir_path in [\n",
    "    (\"processed\", processed_DIR),\n",
    "    (\"checkpoints\", checkpoints_DIR),\n",
    "    (\"intermediate\", intermediate_DIR)\n",
    "]:\n",
    "    try:\n",
    "        dbutils.fs.mkdirs(subdir_path)\n",
    "        print(f\"✓ {subdir_name}: {subdir_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error creating {subdir_name}: {e}\")\n",
    "\n",
    "# Set checkpoint directory for Spark\n",
    "spark.sparkContext.setCheckpointDir(checkpoints_DIR)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Directory structure ready!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nKey paths:\")\n",
    "print(f\"  Intermediate: {intermediate_DIR}\")\n",
    "print(f\"  Processed: {processed_DIR}\")\n",
    "print(f\"  Checkpoints: {checkpoints_DIR}\")\n",
    "print(f\"\\nOutput files:\")\n",
    "print(f\"  Flights+Station: {flights_with_station_path}\")\n",
    "print(f\"  Flights+Weather: {flights_weather_joined_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "807564dc-a1e3-40bb-914e-26c663702933",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fb27d74-8c5e-4400-b8e5-658e2c1d307a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Provided Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cf4b907-b00f-4cda-b3b0-c67271d8cae7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"dbfs:/mnt/mids-w261/datasets_final_project_2022/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "175f8375-b5de-407b-9cd4-bf9088621452",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Airline Data    \n",
    "if data_version == \"\":\n",
    "    df_flights = spark.read.parquet(f\"dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_airlines_data/\") # full dataset\n",
    "else:\n",
    "    df_flights = spark.read.parquet(f\"dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_airlines_data_{data_version}/\") \n",
    "\n",
    "# Stations data      \n",
    "df_stations = spark.read.parquet(f\"dbfs:/mnt/mids-w261/datasets_final_project_2022/stations_data/stations_with_neighbors.parquet/\")\n",
    "\n",
    "# Weather data\n",
    "if data_version == \"\":\n",
    "    df_weather = spark.read.parquet(f\"dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_weather_data/\") # full dataset\n",
    "else:\n",
    "    df_weather = spark.read.parquet(f\"dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_weather_data_{data_version}/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e34801c3-0597-452d-ac48-efa66f73512d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_flights.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbfc9246-924c-484e-bbc8-2729e7c5cf20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_weather.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e00f5a70-9092-4f1f-b1e0-0e49fd81b8f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_stations.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de7fc3fa-75c4-47cf-824c-3de01c6871f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Airport Codes - Additional \n",
    "\n",
    "This adds missing data in DF Flights, like Latitude & Longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64e3f61d-6b12-49a7-872a-750ab560c4eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Download the CSV file to DBFS\n",
    "import requests\n",
    "\n",
    "url = \"https://datahub.io/core/airport-codes/r/airport-codes.csv\"\n",
    "local_path = \"/dbfs/tmp/airport-codes.csv\"\n",
    "\n",
    "response = requests.get(url)\n",
    "with open(local_path, \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "\n",
    "# Use the corresponding DBFS path for Spark\n",
    "dbfs_path = \"dbfs:/tmp/airport-codes.csv\"\n",
    "\n",
    "# Copy to DBFS\n",
    "dbutils.fs.cp(\"file:\" + local_path, dbfs_path)\n",
    "\n",
    "# Read the CSV file from DBFS\n",
    "df_airport_codes = spark.read.option(\"header\", \"true\").csv(\"/tmp/airport-codes.csv\")\n",
    "\n",
    "display(df_airport_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4caaeab0-0c3a-4b5c-856f-d7accada077e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_airport_codes.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffe65f7f-6fbb-47c7-a78b-53d10c924280",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_airport_codes.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc1d7623-65dc-434d-937d-b4c2fee01774",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762642832794}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_airport_codes.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a56751a-d99a-4953-b43e-ce153fb8349b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Airport - Timezone string - Additional Dataset\n",
    "\n",
    "Source: https://github.com/opentraveldata/opentraveldata/blob/master/opentraveldata/optd_por_public.csv \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f6f7aec-3164-4d6f-8d8a-bbb88c89289c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/opentraveldata/opentraveldata/master/opentraveldata/optd_por_public.csv\"\n",
    "\n",
    "local_path = \"/dbfs/tmp/airport-timezones.csv\"\n",
    "\n",
    "with open(local_path, \"wb\") as f:\n",
    "    f.write(requests.get(url).content)\n",
    "\n",
    "# Use the corresponding DBFS path for Spark\n",
    "dbfs_path = \"dbfs:/tmp/airport-timezones.csv\"\n",
    "\n",
    "# Copy to DBFS\n",
    "dbutils.fs.cp(\"file:\" + local_path, dbfs_path)\n",
    "\n",
    "df_airport_timezones = (\n",
    "    spark.read\n",
    "        .option(\"header\", True)\n",
    "        .option(\"delimiter\", \"^\")\n",
    "        .option(\"inferSchema\", True)\n",
    "        .csv(dbfs_path)\n",
    ")\n",
    "\n",
    "cols_to_keep = [\n",
    "    \"iata_code\",\n",
    "    \"icao_code\",\n",
    "    \"faa_code\",\n",
    "    \"timezone\",\n",
    "    \"latitude\",\n",
    "    \"longitude\"\n",
    "]\n",
    "\n",
    "df_airport_timezones = df_airport_timezones.select(cols_to_keep)\n",
    "\n",
    "display(df_airport_timezones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a492736a-e204-4557-8026-763aa78e3dab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_airport_timezones.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed5466e9-6bfd-495d-839e-d362fc392593",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join airport_codes table with airport_timezones \n",
    "\n",
    "# Perform a left join on both iata_code and icao_code\n",
    "df_airport_joined = (\n",
    "    df_airport_codes.alias(\"a\")\n",
    "    .join(\n",
    "        df_airport_timezones.alias(\"b\"),\n",
    "        (\n",
    "            (F.col(\"a.iata_code\") == F.col(\"b.iata_code\")) |\n",
    "            (F.col(\"a.icao_code\") == F.col(\"b.icao_code\"))\n",
    "        ),\n",
    "        how=\"left\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display result\n",
    "display(df_airport_joined) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23c8cc7e-11cc-437a-ab24-8e15c0c03ee8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Row count before join:\", df_airport_codes.count())\n",
    "print(\"Row count after join:\", df_airport_joined.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2f83c80-922f-4c2a-9d44-6818291b49d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_airport_joined_clean = df_airport_joined.select(\n",
    "    \"ident\", \"type\", \"name\", \"elevation_ft\",\n",
    "    \"continent\", \"iso_country\", \"iso_region\", \"municipality\",\n",
    "    \"a.icao_code\", \"a.iata_code\", \"gps_code\", \"local_code\",\n",
    "    \"coordinates\",\n",
    "    F.col(\"b.latitude\").alias(\"latitude\"),\n",
    "    F.col(\"b.longitude\").alias(\"longitude\"),\n",
    "    F.col(\"b.timezone\").alias(\"timezone\"),\n",
    "    F.col(\"b.faa_code\").alias(\"faa_code_otd\")\n",
    ")\n",
    "\n",
    "display(df_airport_joined_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6aaecd71-6324-44dc-a275-5ab2e21afaf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_airport_joined = df_airport_joined_clean\n",
    "\n",
    "# Define window partitioned by your key columns\n",
    "w = Window.partitionBy(\"iata_code\", \"icao_code\").orderBy(\n",
    "    F.when(F.col(\"latitude\").isNotNull(), 1).otherwise(2),  # prefer non-null lat\n",
    "    F.when(F.col(\"timezone\").isNotNull(), 1).otherwise(2)   # then prefer non-null tz\n",
    ")\n",
    "\n",
    "# Add row number within each partition and keep the first\n",
    "df_airport_dedup = (\n",
    "    df_airport_joined\n",
    "    .withColumn(\"row_num\", F.row_number().over(w))\n",
    "    .filter(F.col(\"row_num\") == 1)\n",
    "    .drop(\"row_num\")\n",
    ")\n",
    "\n",
    "print(\"Row count before de-dup:\", df_airport_joined.count())\n",
    "print(\"Row count after de-dup:\", df_airport_dedup.count()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a257efb-0971-471a-95ed-439b15a22391",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add timestamp column\n",
    "\n",
    "df_flights = df_flights.withColumn(\n",
    "    \"fl_date_timestamp\",\n",
    "    to_timestamp(col(\"fl_date\"), \"yyyy-MM-dd\")\n",
    ")\n",
    "\n",
    "# Parse full datetime string (keep hours/minutes)\n",
    "df_weather = df_weather.withColumn(\n",
    "    \"date_timestamp\",\n",
    "    F.to_timestamp(F.col(\"date\"), \"yyyy-MM-dd'T'HH:mm:ss\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b5c6c4b-f5b8-4841-97b2-3471637a47e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_airport_dedup.select(\"timezone\").groupBy(\"timezone\").count().orderBy(\"count\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "890f8201-d210-4338-af06-625ba481f0dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_airports = df_airport_dedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b38fbc20-6610-426b-9241-d91c5313c020",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sizes = {\n",
    "    \"Stations\": df_stations.count() * len(df_stations.columns),\n",
    "    \"Weather\": df_weather.count() * len(df_weather.columns),\n",
    "    \"Flights\": df_flights.count() * len(df_flights.columns),\n",
    "    \"Airports\": df_airports.count() * len(df_airports.columns)\n",
    "}\n",
    "\n",
    "print(\"\\n DataFrame Size Summary\\n\" + \"-\" * 30)\n",
    "for name, size in sizes.items():\n",
    "    print(f\"{name:<10}: {size:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cfbd2a4-808c-42ff-ab95-352e1f7dab8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    df_airports.filter(F.col(\"timezone\").isNull())\n",
    "    .select(\"iso_region\")\n",
    "    .distinct()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cddc4826-2934-4e5e-a4a1-fd315cf8f077",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Pre-processing\n",
    "\n",
    "Convert column names into lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77ffc19e-3760-452b-a741-ca43e18c8c2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Convert all column names to lowercase\n",
    "df_flights = df_flights.toDF(*[c.lower() for c in df_flights.columns])\n",
    "df_weather = df_weather.toDF(*[c.lower() for c in df_weather.columns])\n",
    "df_stations = df_stations.toDF(*[c.lower() for c in df_stations.columns])\n",
    "df_airports = df_airports.toDF(*[c.lower() for c in df_airports.columns])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb5d774d-63ba-4533-b382-61dbc9db88cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Split coordinates column into latitude and longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "813a2070-85d6-4570-b434-f074d808a188",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create latitude and longitude colums in Airport Codes\n",
    "df_airports = df_airports.withColumn(\n",
    "    \"coordinates\",\n",
    "    regexp_replace(\"coordinates\", \"[()]\", \"\")\n",
    ")\n",
    "\n",
    "df_airports = df_airports.withColumn(\n",
    "    \"lat_lon\",\n",
    "    split(\"coordinates\", \",\")\n",
    ")\n",
    "\n",
    "df_airports = df_airports.withColumn(\n",
    "    \"latitude\",\n",
    "    trim(df_airports[\"lat_lon\"].getItem(0))\n",
    ").withColumn(\n",
    "    \"longitude\",\n",
    "    trim(df_airports[\"lat_lon\"].getItem(1))\n",
    ").drop(\"lat_lon\")\n",
    "\n",
    "display(df_airports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75817a98-3e06-455f-81a8-3d6e38984fb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_stations.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3834e81-807a-437f-93db-557f75e2dbb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### create neighbor_iata column\n",
    "\n",
    "Since Stations data's contains 4 digit ICAO codes, we need to  convert it to 3 digit IATA codes for US airports.  \n",
    "\n",
    "Example: \n",
    "ICAO (4-letter): e.g., KJFK, KLAX, EGLL\n",
    "→ This is what neighbor_call contains.\n",
    "\n",
    "IATA (3-letter): e.g., JFK, LAX, LHR\n",
    "→ This is what flight data uses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9915a3db-edf2-4ae5-8290-d9a3511e8a23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_stations = df_stations.withColumn(\n",
    "    \"neighbor_iata\",\n",
    "    F.expr(\"substring(neighbor_call, 2, 3)\")  # remove leading 'K' for US airports\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "000fb933-4a07-4872-ba7b-2ef5d93df8d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_stations.select(\"neighbor_call\",\"neighbor_iata\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14890342-25fb-4554-ab13-d1b766f81299",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### create scheduled departure time  (local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1283d32b-60d7-43c2-aa2f-9d4091eed521",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lpad, concat, to_timestamp, expr\n",
    "\n",
    "# Combine fl_date_timestamp and crs_dep_time to create sched_depart_date_time\n",
    "df_flights = df_flights.withColumn(\n",
    "    \"sched_depart_date_time\",\n",
    "    to_timestamp(\n",
    "        concat(\n",
    "            col(\"fl_date\"),\n",
    "            lpad(col(\"crs_dep_time\"), 4, \"0\")\n",
    "        ),\n",
    "        \"yyyy-MM-ddHHmm\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ff18ff0-412e-4255-8ed1-62097d7424e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_airport_codes.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0e03150-7b55-4dee-8c51-38bd34fe7adc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Implement Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1d33036-a17b-47e1-9991-577f3dde35f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Step 1: Join Flights data with Airports\n",
    "\n",
    "This join is done using IATA code of the airport. We do this join to get data like latitude/longitude of the airport - both origin & destination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90fa56c6-83cc-416c-a7aa-cc0fe4f5c948",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify airports table is small enough to broadcast\n",
    "print(f\"Airports count: {df_airports.count()}\")\n",
    "print(f\"Airports size: {df_airports.rdd.map(lambda x: len(str(x))).sum() / 1024 / 1024} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaa9c2be-40bc-4889-bd0f-5ca58b922f92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Prepare airports dataframe - select only needed columns\n",
    "df_airports_clean = df_airports.select(\n",
    "    col(\"iata_code\"),\n",
    "    col(\"name\"),\n",
    "    col(\"latitude\").cast(\"double\").alias(\"latitude\"),\n",
    "    col(\"longitude\").cast(\"double\").alias(\"longitude\"),\n",
    "    col(\"iso_country\"),\n",
    "    col(\"timezone\")\n",
    ").filter(\n",
    "    col(\"iata_code\").isNotNull() &\n",
    "    col(\"latitude\").isNotNull() &\n",
    "    col(\"longitude\").isNotNull()\n",
    ").cache()\n",
    "\n",
    "print(f\"Airports count: {df_airports_clean.count()}\")\n",
    "\n",
    "# Broadcast airports (it's small enough)\n",
    "airports_broadcast = broadcast(df_airports_clean)\n",
    "\n",
    "# Perform the join\n",
    "df_flights_with_airports = (\n",
    "    df_flights.alias(\"f\")\n",
    "    .join(\n",
    "        airports_broadcast.alias(\"ao\"),\n",
    "        col(\"f.origin\") == col(\"ao.iata_code\"),\n",
    "        \"left\"\n",
    "    )\n",
    "    .join(\n",
    "        airports_broadcast.alias(\"ad\"),\n",
    "        col(\"f.dest\") == col(\"ad.iata_code\"),\n",
    "        \"left\"\n",
    "    )\n",
    "    .select(\n",
    "        col(\"f.*\"),\n",
    "        # Origin airport info\n",
    "        col(\"ao.name\").alias(\"origin_airport_name\"),\n",
    "        col(\"ao.latitude\").alias(\"origin_latitude\"),\n",
    "        col(\"ao.longitude\").alias(\"origin_longitude\"),\n",
    "        col(\"ao.iso_country\").alias(\"origin_country\"),\n",
    "        col(\"ao.timezone\").alias(\"origin_timezone\"),\n",
    "        # Destination airport info\n",
    "        col(\"ad.name\").alias(\"destination_airport_name\"),\n",
    "        col(\"ad.latitude\").alias(\"destination_latitude\"),\n",
    "        col(\"ad.longitude\").alias(\"destination_longitude\"),\n",
    "        col(\"ad.iso_country\").alias(\"destination_country\"),\n",
    "        col(\"ad.timezone\").alias(\"destination_timezone\")\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9daaa9ea-142a-4769-88fb-391431980f5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Get all timestamps in UTC using timezone info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc436143-4a54-4774-9939-bd43ecc6dccf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_flights_with_airports = df_flights_with_airports.withColumn(\n",
    "    \"sched_depart_date_time_UTC\",\n",
    "    F.to_utc_timestamp(\"sched_depart_date_time\", F.col(\"origin_timezone\"))\n",
    ")\n",
    "\n",
    "# Two hours prior to scheduled departure in UTC\n",
    "df_flights_with_airports = df_flights_with_airports.withColumn(\n",
    "    \"two_hours_prior_depart_UTC\",\n",
    "    expr(\"sched_depart_date_time_UTC - INTERVAL 2 HOURS\")\n",
    ")\n",
    "\n",
    "# Four hours prior to scheduled departure in UTC\n",
    "df_flights_with_airports = df_flights_with_airports.withColumn(\n",
    "    \"four_hours_prior_depart_UTC\",\n",
    "    expr(\"sched_depart_date_time_UTC - INTERVAL 4 HOURS\")\n",
    ")\n",
    "\n",
    "display(df_flights_with_airports.select(\n",
    "    \"fl_date\", \"crs_dep_time\", \"sched_depart_date_time\",\n",
    "    \"sched_depart_date_time_UTC\", \"two_hours_prior_depart_UTC\", \"four_hours_prior_depart_UTC\"\n",
    ").limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18aa404c-3cea-4e1b-897a-ff287b678328",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_weather.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d69dd1a6-18e2-4e14-9780-c3ab9a6123dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Verify join quality\n",
    "print(f\"\\nOriginal flights: {df_flights.count():,}\")\n",
    "print(f\"After join: {df_flights_with_airports.count():,}\")\n",
    "print(f\"Missing origin coords: {df_flights_with_airports.filter(col('origin_latitude').isNull()).count():,}\")\n",
    "\n",
    "print(\"\\n✓ Join 1 Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e55f6f40-2b1b-46b9-b8f2-9ef48b4b509e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Checkpoint 1: Store after first join "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84d9e342-3e17-4035-9d2f-7296ce5e5211",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_flights_combined \n",
    "\n",
    "# df_flights_with_station_notnull = spark.read.parquet(f\"{section_DIR}/df_flights_with_station_{data_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0d3fb07-6d07-4a19-9679-2d0b78e39795",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Step 2: Join Flights+Airport with Stations \n",
    "\n",
    "Here we try two different approaches:\n",
    " - Using neighbor_call/neighbor_iata \n",
    " - Using latitude & longitude\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79b34e2b-0272-4a53-a79c-2033bc7c2da0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_stations.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fb25b40-f3c5-4478-8d1f-3bac20bba6dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "So that shows all of them match! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4de84d5f-ec87-406d-91c5-c40aad3f5b62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Join type 2: get stationID from stations table using lat/long\n",
    "\n",
    "Join using Origin airport code and neighbor_iata column of stations data and distance to neighbor < 250km. \n",
    "\n",
    "Then check for nulls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93de77c9-5aaf-48cb-b319-a870f41b7045",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Join type 2: get stationID from stations table using lat/long\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0773d09-c3ac-4196-97dd-0ac877dd011c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Doing a cross-join with the whole flights data & station data will create too many combinations. It's better we only get the join done with distinct airports and later combine it with original flights data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc162287-cbb1-4874-a2ab-10c834637b5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "TODO: Try Haversine approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "920c4e06-f0e9-498a-bef7-56aadff179dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Get distinct origin airports\n",
    "df_distinct_origins = (\n",
    "    df_flights_with_airports\n",
    "    .select(\"origin\", \"origin_latitude\", \"origin_longitude\")\n",
    "    .distinct()\n",
    "    .filter(\n",
    "        col(\"origin_latitude\").isNotNull() & \n",
    "        col(\"origin_longitude\").isNotNull()\n",
    "    )\n",
    "    .repartition(200, \"origin\")\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "print(f\"Distinct origins: {df_distinct_origins.count()}\")\n",
    "\n",
    "# Step 2: Prepare stations dataframe\n",
    "df_stations_clean = df_stations.select(\n",
    "    col(\"station_id\"),\n",
    "    col(\"lat\").cast(\"double\"),\n",
    "    col(\"lon\").cast(\"double\")\n",
    ").filter(\n",
    "    col(\"station_id\").isNotNull() &\n",
    "    col(\"lat\").isNotNull() &\n",
    "    col(\"lon\").isNotNull()\n",
    ")\n",
    "\n",
    "stations_broadcast = broadcast(df_stations_clean)\n",
    "\n",
    "# Step 3: Perform spatial join with bounding box filter\n",
    "df_candidates = (\n",
    "    df_distinct_origins.alias(\"a\")\n",
    "    .join(\n",
    "        stations_broadcast.alias(\"s\"),\n",
    "        (col(\"s.lat\").between(col(\"a.origin_latitude\") - 0.5, col(\"a.origin_latitude\") + 0.5)) &\n",
    "        (col(\"s.lon\").between(col(\"a.origin_longitude\") - 0.5, col(\"a.origin_longitude\") + 0.5)),\n",
    "        \"inner\"\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"distance_km\",\n",
    "        F.expr(\"\"\"\n",
    "            6371 * acos(\n",
    "                least(1.0,\n",
    "                    cos(radians(a.origin_latitude)) * cos(radians(s.lat)) *\n",
    "                    cos(radians(s.lon) - radians(a.origin_longitude)) +\n",
    "                    sin(radians(a.origin_latitude)) * sin(radians(s.lat))\n",
    "                )\n",
    "            )\n",
    "        \"\"\")\n",
    "    )\n",
    "    .filter(col(\"distance_km\") < 50)\n",
    ")\n",
    "\n",
    "# Step 4: Get nearest station per airport\n",
    "window_spec = Window.partitionBy(\"a.origin\").orderBy(\"distance_km\")\n",
    "\n",
    "df_nearest_stations = (\n",
    "    df_candidates\n",
    "    .withColumn(\"rank\", F.row_number().over(window_spec))\n",
    "    .filter(col(\"rank\") == 1)\n",
    "    .select(\n",
    "        col(\"a.origin\").alias(\"origin\"),\n",
    "        col(\"s.station_id\").alias(\"origin_station_id\"),\n",
    "        col(\"distance_km\").alias(\"station_distance_km\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Airports with stations: {df_nearest_stations.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "947317fb-c56a-4e8c-9b93-bd7e6dcb5ed5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Step 5: Join back to main flights dataframe\n",
    "df_flights_with_station = (\n",
    "    df_flights_with_airports\n",
    "    .join(df_nearest_stations, on=\"origin\", how=\"left\")\n",
    ")\n",
    "\n",
    "# Check quality\n",
    "null_count = df_flights_with_station.filter(col(\"origin_station_id\").isNull()).count()\n",
    "print(f\"Flights without station: {null_count:,}\")\n",
    "\n",
    "# Filter out flights without stations\n",
    "df_flights_with_station_clean = df_flights_with_station.filter(\n",
    "    col(\"origin_station_id\").isNotNull()\n",
    ")\n",
    "\n",
    "print(f\"Final flights with stations: {df_flights_with_station_clean.count():,}\")\n",
    "print(\"\\n✓ Join 2 Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb62db97-37b3-44c8-a34d-e978696162c6",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762721649224}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Unpersist cached data\n",
    "df_distinct_origins.unpersist()\n",
    "\n",
    "display(df_flights_with_station_clean.limit(5))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5027387b-786d-437f-a300-651ef8bf55d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Inspect the columns where origin_station_id is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7434a38-3cf1-49e2-a9ab-b79b3d63f98f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(df_flights_with_station.filter(F.col(\"origin_station_id\").isNull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c08a188f-f36d-42cd-a87c-d1f5e3b81eae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(df_flights_with_station.filter(F.col(\"origin_station_id\").isNull()).select('origin').distinct())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f104beb2-e601-4203-9cbc-6d3d1d54ce4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Most of the ones which are null are:\n",
    "\n",
    "- PSE — Mercedita Airport, Ponce, Puerto Rico 🇵🇷\n",
    "- GUM — Antonio B. Won Pat International Airport, Guam 🇬🇺\n",
    "- PPG — Pago Pago International Airport (Tafuna Airport), American Samoa 🇦🇸\n",
    "- ISN — Sloulin Field International Airport (now replaced by Williston Basin International Airport), Williston, North Dakota, USA 🇺🇸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0ede3fa-6c62-4507-98c3-2acd4fd0507d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_flights_with_station_notnull = df_flights_with_station.filter(F.col(\"origin_station_id\").isNotNull())\n",
    "# display(df_flights_with_station_notnull.limit(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38ceb277-d26f-4c18-a994-81f88a597544",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Checkpoint 2: Before Final Join\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a0ecf1b-a328-4f90-84db-93480599bdf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_flights_with_station_notnull.write.mode(\"overwrite\").parquet(f\"{section_DIR}/df_flights_with_station_{data_version}\")\n",
    "\n",
    "# Save intermediate result\n",
    "#df_flights_with_station_notnull.write.mode(\"overwrite\").parquet(flights_with_station_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0f8a59c-d1ab-495c-bde6-6e765a5a9da9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load intermediate result later\n",
    "#df_flights_with_station_notnull = spark.read.parquet(flights_with_station_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60708117-04fe-4598-9987-57e11e84692f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Step 3: Join Flights+Airports+Stations with Weather "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b8e09c9-0c33-4479-b805-b0b5450c6701",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Run the full join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aa5d23a-8eb0-42da-a50d-5814fe244435",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Add time-based partitioning key (6-hour buckets)\n",
    "df_flights_keyed = (\n",
    "    df_flights_with_station_clean\n",
    "    .withColumn(\n",
    "        \"time_bucket\",\n",
    "        F.expr(\"floor(unix_timestamp(two_hours_prior_depart_UTC) / 21600)\")  # 6-hour buckets\n",
    "    )\n",
    ")\n",
    "\n",
    "df_weather_keyed = (\n",
    "    df_weather\n",
    "    .withColumn(\n",
    "        \"time_bucket\",\n",
    "        F.expr(\"floor(unix_timestamp(date_timestamp) / 21600)\")  # 6-hour buckets\n",
    "    )\n",
    "    .filter(\n",
    "        F.col(\"station\").isNotNull() &\n",
    "        col(\"date_timestamp\").isNotNull()\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Time bucketing complete\")\n",
    "\n",
    "# Step 2: Repartition by station and time bucket\n",
    "df_flights_partitioned = df_flights_keyed.repartition(400, \"origin_station_id\", \"time_bucket\")\n",
    "df_weather_partitioned = df_weather_keyed.repartition(400, \"station\", \"time_bucket\")\n",
    "\n",
    "print(\"Partitioning complete\")\n",
    "\n",
    "# Step 3: Join with time bucket AND adjacent buckets (handles midnight crossover)\n",
    "df_weather_join = (\n",
    "    df_flights_partitioned.alias(\"f\")\n",
    "    .join(\n",
    "        df_weather_partitioned.alias(\"w\"),\n",
    "        (col(\"f.origin_station_id\") == col(\"w.station\")) &\n",
    "        (\n",
    "            (col(\"f.time_bucket\") == col(\"w.time_bucket\")) |\n",
    "            #(col(\"f.time_bucket\") == col(\"w.time_bucket\") + 1) |\n",
    "            (col(\"f.time_bucket\") == col(\"w.time_bucket\") - 1)\n",
    "        ) &\n",
    "        (col(\"w.date_timestamp\").between(\n",
    "            col(\"f.two_hours_prior_depart_UTC\") - F.expr(\"INTERVAL 1 HOURS\"),\n",
    "            col(\"f.two_hours_prior_depart_UTC\")\n",
    "        )),\n",
    "        \"left\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Join complete, ranking weather records...\")\n",
    "\n",
    "# Step 4: Get closest weather record\n",
    "df_weather_ranked = (\n",
    "    df_weather_join\n",
    "    .withColumn(\n",
    "        \"time_diff_sec\",\n",
    "        F.abs(\n",
    "            F.unix_timestamp(\"f.two_hours_prior_depart_UTC\") - \n",
    "            F.unix_timestamp(\"w.date_timestamp\")\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "window_closest = Window.partitionBy(\n",
    "    \"f.origin\",\n",
    "    \"f.fl_date\",\n",
    "    \"f.crs_dep_time\",\n",
    "    \"f.op_carrier_fl_num\",\n",
    "    \"f.two_hours_prior_depart_UTC\"\n",
    ").orderBy(col(\"time_diff_sec\").asc_nulls_last())\n",
    "\n",
    "df_final = (\n",
    "    df_weather_ranked\n",
    "    .withColumn(\"weather_rank\", F.row_number().over(window_closest))\n",
    "    .filter(col(\"weather_rank\") == 1)\n",
    "    .drop(\"weather_rank\", \"time_diff_sec\", \"time_bucket\")\n",
    ")\n",
    "\n",
    "\n",
    "# CRITICAL: Checkpoint to break lineage\n",
    "print(\"\\n⚡ Checkpointing result to break lineage...\")\n",
    "spark.sparkContext.setCheckpointDir(checkpoints_DIR)\n",
    "\n",
    "# - Break lineage immediately after join\n",
    "print(\"\\nBreaking lineage with checkpoint...\")\n",
    "df_final = df_final.checkpoint(eager=True)\n",
    "\n",
    "print(f\"Final row count: {df_final.count():,}\")\n",
    "print(\"\\n✓ Join 3 Complete (checkpointed)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5aeafe76-4da9-4ef5-89e2-41fde558dcbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Verifying midnight crossover handling:\")\n",
    "\n",
    "midnight_flights = (\n",
    "    df_final\n",
    "    .filter(\n",
    "        (F.hour(\"sched_depart_date_time_utc\") == 0) |\n",
    "        (F.hour(\"sched_depart_date_time_utc\") == 1)\n",
    "    )\n",
    "    .select(\n",
    "        \"origin\",\n",
    "        \"sched_depart_date_time_utc\",\n",
    "        \"two_hours_prior_depart_utc\",\n",
    "        \"w.date_timestamp\",\n",
    "        F.to_date(\"sched_depart_date_time_utc\").alias(\"flight_date\"),\n",
    "        F.to_date(\"w.date_timestamp\").alias(\"weather_date\"),\n",
    "        F.datediff(\n",
    "            F.to_date(\"w.date_timestamp\"),\n",
    "            F.to_date(\"sched_depart_date_time_utc\")\n",
    "        ).alias(\"date_diff\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Sample midnight flights:\")\n",
    "display(midnight_flights.limit(10))\n",
    "\n",
    "prev_day_weather = midnight_flights.filter(col(\"date_diff\") == -1).count()\n",
    "print(f\"\\nFlights with weather from previous day: {prev_day_weather:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab51c57a-4026-4263-9c3c-7264193ff992",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"Final Data Quality Summary:\")\n",
    "df_final.select(\n",
    "    F.count(\"*\").alias(\"total_rows\"),\n",
    "    F.count(\"w.station\").alias(\"has_weather\"),\n",
    "    F.sum(F.when(col(\"w.station\").isNull(), 1).otherwise(0)).alias(\"missing_weather\"),\n",
    "    F.count(\"w.hourlyvisibility\").alias(\"has_visibility\"),\n",
    "    F.round(F.count(\"w.station\") / F.count(\"*\") * 100, 2).alias(\"weather_coverage_pct\")\n",
    ").show()\n",
    "\n",
    "# COMMAND ----------\n",
    "display(df_final.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32ae1773-5ec6-4e49-93a1-755b2ed984e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Check what's currently cached\n",
    "print(\"Currently Cached DataFrames:\")\n",
    "print(\"=\"*60)\n",
    "for (id, rdd) in spark.sparkContext._jsc.getPersistentRDDs().items():\n",
    "    print(f\"  RDD ID: {id}\")\n",
    "\n",
    "# Unpersist specific dataframes\n",
    "df_airports_clean.unpersist()\n",
    "df_distinct_origins.unpersist()\n",
    "\n",
    "# If you're not sure what's cached, unpersist everything\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "print(\"✓ Cache cleared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e5ae01d-8050-4693-8589-67b89235ee97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def check_memory_usage():\n",
    "    \"\"\"Check current memory usage\"\"\"\n",
    "    \n",
    "    print(\"Memory Usage Report\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Check cached data\n",
    "    try:\n",
    "        cached_tables = spark.sql(\"SHOW TABLES\").filter(col(\"isTemporary\") == True)\n",
    "        print(f\"\\n1. Cached Tables: {cached_tables.count()}\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # 2. Check RDD cache\n",
    "    cached_rdds = spark.sparkContext._jsc.getPersistentRDDs()\n",
    "    print(f\"\\n2. Cached RDDs: {len(cached_rdds)}\")\n",
    "    \n",
    "    # 3. Check broadcast variables\n",
    "    try:\n",
    "        broadcast_count = len(spark.sparkContext._jsc.sc().getBroadcastVariables())\n",
    "        print(f\"\\n3. Broadcast Variables: {broadcast_count}\")\n",
    "    except:\n",
    "        print(f\"\\n3. Broadcast Variables: Unknown\")\n",
    "    \n",
    "    # 4. Storage memory\n",
    "    try:\n",
    "        storage_status = spark.sparkContext._jsc.sc().getExecutorStorageStatus()\n",
    "        total_memory = sum([s.maxMem() for s in storage_status])\n",
    "        used_memory = sum([s.memUsed() for s in storage_status])\n",
    "        \n",
    "        print(f\"\\n4. Storage Memory:\")\n",
    "        print(f\"   Used: {used_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"   Total: {total_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"   Usage: {used_memory / total_memory * 100:.1f}%\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n4. Storage Memory: Unable to retrieve\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Check before and after cleanup\n",
    "check_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64637d9d-9e4c-4279-8bfa-31f8b5d35dab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import gc\n",
    "\n",
    "# Python garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# JVM garbage collection (more aggressive)\n",
    "spark.sparkContext._jvm.System.gc()\n",
    "\n",
    "print(\"✓ Garbage collection complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "530a9432-8918-4009-a25a-70b07a944f53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check for duplicate columns\n",
    "print(\"Checking for duplicate columns...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "columns = df_final.columns\n",
    "duplicates = [col for col in set(columns) if columns.count(col) > 1]\n",
    "\n",
    "if duplicates:\n",
    "    print(f\"❌ Found duplicate columns: {duplicates}\")\n",
    "    print(f\"\\nAll columns ({len(columns)}):\")\n",
    "    for i, col in enumerate(columns):\n",
    "        print(f\"  {i+1}. {col}\")\n",
    "else:\n",
    "    print(\"✓ No duplicates found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b7a50a8-f766-4ab3-ad3e-50a5a0f76cc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Save Final Result\n",
    "\n",
    "# COMMAND ----------\n",
    "print(\"=\"*60)\n",
    "print(\"SAVING FINAL RESULT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check current partitioning\n",
    "num_partitions = df_final.rdd.getNumPartitions()\n",
    "print(f\"Current partitions: {num_partitions}\")\n",
    "\n",
    "# Only repartition if necessary\n",
    "if num_partitions > 500:\n",
    "    print(f\"⚠ Too many partitions, coalescing to 200\")\n",
    "    df_final = df_final.coalesce(200)\n",
    "elif num_partitions < 10:\n",
    "    print(f\"⚠ Too few partitions, repartitioning to 50\")\n",
    "    df_final = df_final.repartition(50)\n",
    "else:\n",
    "    print(f\"✓ Partition count looks good\")\n",
    "\n",
    "df_final = df_final.drop(\"year\") \n",
    "# Write\n",
    "df_final.write.mode(\"overwrite\").parquet(flights_weather_joined_path)\n",
    "print(f\"✓ Data saved to: {flights_weather_joined_path}\")\n",
    "\n",
    "# Quick validation (optional - comment out if too slow)\n",
    "# df_verification = spark.read.parquet(flights_weather_joined_path)\n",
    "# print(f\"Verification count: {df_verification.count():,}\")\n",
    "\n",
    "# Cleanup\n",
    "df_airports_clean.unpersist()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ALL JOINS COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc32921d-555e-4d7b-a294-e3b6481444ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Quick validation (optional - comment out if too slow)\n",
    "df_verification = spark.read.parquet(flights_weather_joined_path)\n",
    "print(f\"Verification count: {df_verification.count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a33ad331-88c7-4fd7-b29b-e09e7a60dfd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# OTPW Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb9620a5-ee6b-4a99-8499-4a8acf7076b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#df_otpw = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"dbfs:/mnt/mids-w261/OTPW_3M_2015.csv\")\n",
    "\n",
    "df_otpw = spark.read.format(\"parque\").load(\"dbfs:/mnt/mids-w261/OTPW_12M/\")\n",
    "\n",
    "df_otpw = df_otpw.toDF(*[c.lower() for c in df_otpw.columns])\n",
    "\n",
    "\n",
    "df_otpw_parsed = df_otpw.withColumn(\n",
    "    \"sched_depart_date_time_ts\",\n",
    "    F.to_timestamp(\"sched_depart_date_time\", \"yyyy-MM-dd'T'HH:mm:ss\")\n",
    ").withColumn(\n",
    "    \"sched_depart_date_time_utc_ts\",\n",
    "    F.to_timestamp(\"sched_depart_date_time_utc\", \"yyyy-MM-dd'T'HH:mm:ss\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7166434-7cef-440d-b89c-9c3ba91d4be7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_otpw.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54525191-9e97-403f-bd77-6a6a08766596",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Compare OTPW and Custom Join - 1y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a121ca3-e2f5-414e-b04c-12ce2e0816ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, sum as _sum\n",
    "\n",
    "def compare_dataframes(df1: DataFrame, df2: DataFrame, name1: str = \"df1\", name2: str = \"df2\"):\n",
    "    print(f\"===== 📊 Comparing {name1} and {name2} =====\\n\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 1. Row Count\n",
    "    # -----------------------------\n",
    "    print(\"➡ Row Counts:\")\n",
    "    print(f\"{name1}: {df1.count()}\")\n",
    "    print(f\"{name2}: {df2.count()}\\n\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2. Column Names\n",
    "    # -----------------------------\n",
    "    cols1 = set(df1.columns)\n",
    "    cols2 = set(df2.columns)\n",
    "\n",
    "    print(\"➡ Columns Present in Both:\")\n",
    "    print(sorted(cols1 & cols2), \"\\n\")\n",
    "\n",
    "    print(\"➡ Columns Only in\", name1)\n",
    "    print(sorted(cols1 - cols2), \"\\n\")\n",
    "\n",
    "    print(\"➡ Columns Only in\", name2)\n",
    "    print(sorted(cols2 - cols1), \"\\n\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3. Schema Comparison\n",
    "    # -----------------------------\n",
    "    print(\"➡ Schema Differences:\")\n",
    "    schema1 = {f.name: f.dataType for f in df1.schema.fields}\n",
    "    schema2 = {f.name: f.dataType for f in df2.schema.fields}\n",
    "\n",
    "    common = cols1 & cols2\n",
    "\n",
    "    diffs = {c: (schema1[c], schema2[c]) for c in common if schema1[c] != schema2[c]}\n",
    "\n",
    "    if diffs:\n",
    "        for col_name, (t1, t2) in diffs.items():\n",
    "            print(f\" - Column '{col_name}': {name1}={t1}, {name2}={t2}\")\n",
    "    else:\n",
    "        print(\"No schema differences.\\n\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4. Null Count Comparison\n",
    "    # -----------------------------\n",
    "    print(\"\\n➡ Null Counts Per Column:\")\n",
    "\n",
    "    print(f\"\\n{name1} Null Counts:\")\n",
    "    nulls1 = df1.select([_sum(col(c).isNull().cast(\"int\")).alias(c) for c in df1.columns])\n",
    "    nulls1.show(truncate=False)\n",
    "\n",
    "    print(f\"\\n{name2} Null Counts:\")\n",
    "    nulls2 = df2.select([_sum(col(c).isNull().cast(\"int\")).alias(c) for c in df2.columns])\n",
    "    nulls2.show(truncate=False)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 5. Numeric Summary Comparison (optional)\n",
    "    # -----------------------------\n",
    "    numeric_cols1 = [f.name for f in df1.schema.fields if \"int\" in str(f.dataType) or \"double\" in str(f.dataType)]\n",
    "    numeric_cols2 = [f.name for f in df2.schema.fields if \"int\" in str(f.dataType) or \"double\" in str(f.dataType)]\n",
    "    numeric_common = list(set(numeric_cols1) & set(numeric_cols2))\n",
    "\n",
    "    if numeric_common:\n",
    "        print(\"\\n➡ Summary Statistics (common numeric columns):\")\n",
    "        print(f\"Common numeric columns: {numeric_common}\\n\")\n",
    "\n",
    "        print(f\"{name1} summary:\")\n",
    "        df1.select(numeric_common).summary().show(truncate=False)\n",
    "\n",
    "        print(f\"{name2} summary:\")\n",
    "        df2.select(numeric_common).summary().show(truncate=False)\n",
    "    else:\n",
    "        print(\"\\nNo common numeric columns to summarize.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37fe654a-5751-490d-b8f9-8f947c2562d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "compare_dataframes(df_otpw, df_final, \"df_otpw\", \"df_final\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [
    {
     "elements": [],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "848a23e1-4a38-4b97-b84c-de53113ea481",
     "origId": 1792055957781377,
     "title": "Untitled",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) Shared - Sid - Custom Join (1y)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
