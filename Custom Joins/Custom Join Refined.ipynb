{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7158cf59-2a99-4dc3-9ecf-e1f11da9c4fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Implementing a Custom Join \n",
    "=============="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Checks Added\n",
    "\n",
    "This refined version includes comprehensive sanity checks at each major step to track row counts and identify where flights are being dropped.\n",
    "\n",
    "**Checkpoints:**\n",
    "1. After Initial Load - Baseline row count\n",
    "2. After dropDuplicates - Check for duplicate removal impact\n",
    "3. After Airport Join - Verify airport data join (LEFT JOIN preserves all flights)\n",
    "4. After Station Join (Left) - Check station matching (LEFT JOIN preserves all flights)\n",
    "5. After Station Filter - **CRITICAL**: This is where flights without stations are dropped\n",
    "6. After Weather Join - Check weather data join (INNER JOIN may drop flights)\n",
    "7. Final Before Save - Final row count before saving\n",
    "\n",
    "**Summary Cell:** A comparison table showing row counts and losses at each step is included before the final save.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01dfa598-4202-432e-9698-5cc9a9bdafa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7249721-d156-4a4a-a9ed-316e38f5b37d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_version = \"3m\" # \"3m\", \"6m\", \"1y\", \"\" -> blank is full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a517ba1-c78f-4993-bfeb-3908644b9019",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from pyspark.sql.functions import col, regexp_replace, split, trim, to_timestamp, date_format, broadcast\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32f0a472-5907-4859-95ac-9f011e86a55e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Spark settings for efficient execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e8adcc5-935c-44ca-9eb9-72422152c637",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.get(\"spark.sql.session.timeZone\")\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7b5f0f9-1ac6-4fde-a737-f197eb2d83e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Enable Adaptive Query Execution (AQE) and optimizations for joins\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")                  # Adaptive Query Execution (auto optimizes joins/shuffles)\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")         # Automatically handles skewed joins\n",
    "#spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")  # Merge small partitions post-shuffle\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 50 * 1024 * 1024) # Optional: increase broadcast threshold to 50MB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4ace4f9-5da3-448b-8c35-2d8986a118db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Reading back to see the spark settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a7a08d3-7302-4775-a6e3-680091759c26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Spark version:\", spark.version)\n",
    "print(\"AQE:\", spark.conf.get(\"spark.sql.adaptive.enabled\"))\n",
    "print(\"Skew join:\", spark.conf.get(\"spark.sql.adaptive.skewJoin.enabled\"))\n",
    "print(\"Coalesce:\", spark.conf.get(\"spark.sql.adaptive.coalescePartitions.enabled\"))\n",
    "print(\"AutoBroadcastJoinThreshold:\", spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\"))\n",
    "print(\"spark.sql.shuffle.partitions:\", spark.conf.get(\"spark.sql.shuffle.partitions\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3c636e3-473d-4dcf-a831-f3f9645252de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Paths & Defines\n",
    "\n",
    "data_BASE_DIR = \"dbfs:/mnt/mids-w261/\"\n",
    "display(dbutils.fs.ls(f\"{data_BASE_DIR}\")) \n",
    "\n",
    "# Section Folder\n",
    "section = \"4\"\n",
    "number = \"2\"\n",
    "section_DIR = f\"dbfs:/mnt/mids-w261/student-groups/Group_{section}_{number}\"\n",
    "\n",
    "# Subdirectories for organization\n",
    "raw_DIR = f\"{section_DIR}/raw\"\n",
    "processed_DIR = f\"{section_DIR}/processed\"\n",
    "checkpoints_DIR = f\"{section_DIR}/checkpoints\"\n",
    "intermediate_DIR = f\"{section_DIR}/intermediate\"\n",
    "\n",
    "# Output filenames (using variables for maintainability)\n",
    "FLIGHTS_WITH_STATION = f\"flights_with_station_{data_version}\"\n",
    "FLIGHTS_WEATHER_JOINED = f\"flights_weather_joined_{data_version}\"\n",
    "FLIGHTS_WITH_AIRPORTS = f\"flights_with_airports_{data_version}\"\n",
    "\n",
    "# Full paths for outputs\n",
    "flights_with_station_path = f\"{intermediate_DIR}/{FLIGHTS_WITH_STATION}\"\n",
    "flights_weather_joined_path = f\"{processed_DIR}/{FLIGHTS_WEATHER_JOINED}\"\n",
    "flights_with_airports_path = f\"{intermediate_DIR}/{FLIGHTS_WITH_AIRPORTS}\"\n",
    "\n",
    "# Check if section_DIR exists, print contents or create it\n",
    "try:\n",
    "    print(f\"\u2713 Section directory exists: {section_DIR}\")\n",
    "    print(\"\\nContents:\")\n",
    "    contents = dbutils.fs.ls(section_DIR)\n",
    "    for item in contents:\n",
    "        print(f\"  - {item.name} ({'DIR' if item.isDir() else 'FILE'}) - {item.size} bytes\")\n",
    "    print(f\"\\nTotal items: {len(contents)}\")\n",
    "except Exception as e:\n",
    "    print(f\"\u2717 Section directory does not exist: {section_DIR}\")\n",
    "    print(\"Creating directory structure...\")\n",
    "    dbutils.fs.mkdirs(section_DIR)\n",
    "    print(f\"\u2713 Base directory created: {section_DIR}\")\n",
    "\n",
    "# Create subdirectories\n",
    "print(\"\\nCreating/verifying subdirectories...\")\n",
    "for subdir_name, subdir_path in [\n",
    "    (\"raw\", raw_DIR),\n",
    "    (\"processed\", processed_DIR),\n",
    "    (\"checkpoints\", checkpoints_DIR),\n",
    "    (\"intermediate\", intermediate_DIR)\n",
    "]:\n",
    "    try:\n",
    "        dbutils.fs.mkdirs(subdir_path)\n",
    "        print(f\"\u2713 {subdir_name}: {subdir_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\u2717 Error creating {subdir_name}: {e}\")\n",
    "\n",
    "# Set checkpoint directory for Spark\n",
    "spark.sparkContext.setCheckpointDir(checkpoints_DIR)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Directory structure ready!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nKey paths:\")\n",
    "print(f\"  Raw: {raw_DIR}\")\n",
    "print(f\"  Intermediate: {intermediate_DIR}\")\n",
    "print(f\"  Processed: {processed_DIR}\")\n",
    "print(f\"  Checkpoints: {checkpoints_DIR}\")\n",
    "print(f\"\\nOutput files:\")\n",
    "print(f\"  Flights+Station: {flights_with_station_path}\")\n",
    "print(f\"  Flights+Weather: {flights_weather_joined_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "807564dc-a1e3-40bb-914e-26c663702933",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fb27d74-8c5e-4400-b8e5-658e2c1d307a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Provided Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cf4b907-b00f-4cda-b3b0-c67271d8cae7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"dbfs:/mnt/mids-w261/datasets_final_project_2022/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "175f8375-b5de-407b-9cd4-bf9088621452",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Airline Data    \n",
    "if data_version == \"\":\n",
    "    df_flights = spark.read.parquet(f\"dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_airlines_data/\") # full dataset\n",
    "else:\n",
    "    df_flights = spark.read.parquet(f\"dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_airlines_data_{data_version}/\") \n",
    "\n",
    "# Stations data      \n",
    "df_stations = spark.read.parquet(f\"dbfs:/mnt/mids-w261/datasets_final_project_2022/stations_data/stations_with_neighbors.parquet/\")\n",
    "\n",
    "# Weather data\n",
    "if data_version == \"\":\n",
    "    df_weather = spark.read.parquet(f\"dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_weather_data/\") # full dataset\n",
    "else:\n",
    "    df_weather = spark.read.parquet(f\"dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_weather_data_{data_version}/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SANITY CHECK: After Initial Load\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"SANITY CHECK: After Initial Load\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Row count\n",
    "try:\n",
    "    row_count = df_flights.count()\n",
    "    print(f\"\\n\u2713 Row count: {row_count:,}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n\u2717 Error counting rows: {e}\")\n",
    "    row_count = None\n",
    "\n",
    "# Check for NULLs in key columns\n",
    "print(\"\\n--- NULL Analysis ---\")\n",
    "key_columns = []\n",
    "if 'origin' in df_flights.columns:\n",
    "    key_columns.append('origin')\n",
    "if 'dest' in df_flights.columns:\n",
    "    key_columns.append('dest')\n",
    "if 'origin_station_id' in df_flights.columns:\n",
    "    key_columns.append('origin_station_id')\n",
    "if 'origin_latitude' in df_flights.columns:\n",
    "    key_columns.append('origin_latitude')\n",
    "if 'origin_longitude' in df_flights.columns:\n",
    "    key_columns.append('origin_longitude')\n",
    "\n",
    "for col_name in key_columns:\n",
    "    try:\n",
    "        null_count = df_flights.filter(F.col(col_name).isNull()).count()\n",
    "        null_pct = (null_count / row_count * 100) if row_count else 0\n",
    "        print(f\"  {col_name}: {null_count:,} NULLs ({null_pct:.2f}%)\")\n",
    "    except Exception as e:\n",
    "        print(f\"  {col_name}: Error - {e}\")\n",
    "\n",
    "# Identify dropped airports (if applicable)\n",
    "if 'origin_station_id' in df_flights.columns:\n",
    "    try:\n",
    "        dropped_airports = df_flights.filter(\n",
    "            F.col('origin_station_id').isNull()\n",
    "        ).select('origin').distinct()\n",
    "        dropped_count = dropped_airports.count()\n",
    "        if dropped_count > 0:\n",
    "            print(f\"\\n--- Airports Without Stations: {dropped_count} ---\")\n",
    "            print(\"Sample airports without stations:\")\n",
    "            dropped_airports.show(20, truncate=False)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError analyzing dropped airports: {e}\")\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e34801c3-0597-452d-ac48-efa66f73512d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_flights.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbfc9246-924c-484e-bbc8-2729e7c5cf20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_weather.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e00f5a70-9092-4f1f-b1e0-0e49fd81b8f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_stations.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de7fc3fa-75c4-47cf-824c-3de01c6871f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Airport Codes - Additional \n",
    "\n",
    "This adds missing data in DF Flights, like Latitude & Longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b9beed3-515c-4f79-9440-b8de9f90aaf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Download the CSV file to a local temp path on the driver\n",
    "import requests\n",
    "import os\n",
    "\n",
    "url = \"https://datahub.io/core/airport-codes/r/airport-codes.csv\"\n",
    "local_tmp_path = \"/tmp/airport-codes.csv\"   # driver-local temp file\n",
    "\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # optional but recommended\n",
    "\n",
    "with open(local_tmp_path, \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# Target DBFS directory and file path (stable location)\n",
    "target_dir = \"/mnt/mids-w261/student-groups/Group_4_2/raw\"\n",
    "target_path = f\"{target_dir}/airport-codes.csv\"\n",
    "\n",
    "# Make sure the directory exists\n",
    "dbutils.fs.mkdirs(target_dir)\n",
    "\n",
    "# If the file already exists, you can remove it first (optional but avoids cp errors)\n",
    "dbutils.fs.rm(target_path, recurse=False)\n",
    "\n",
    "# Copy from driver-local file system to DBFS mount\n",
    "dbutils.fs.cp(\"file://\" + local_tmp_path, target_path)\n",
    "\n",
    "# Read the CSV file from the stable DBFS path\n",
    "df_airport_codes = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .csv('/dbfs/mnt/mids-w261/student-groups/Group_4_2/raw/airport-codes.csv')\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "# Force materialization of the cache so later stages don't re-read the CSV\n",
    "df_airport_codes.count()\n",
    "\n",
    "# display(df_airport_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4caaeab0-0c3a-4b5c-856f-d7accada077e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_airport_codes.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffe65f7f-6fbb-47c7-a78b-53d10c924280",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_airport_codes.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc1d7623-65dc-434d-937d-b4c2fee01774",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762642832794}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(df_airport_codes.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a56751a-d99a-4953-b43e-ce153fb8349b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Airport - Timezone string - Additional Dataset\n",
    "\n",
    "Source: https://github.com/opentraveldata/opentraveldata/blob/master/opentraveldata/optd_por_public.csv \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f6f7aec-3164-4d6f-8d8a-bbb88c89289c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/opentraveldata/opentraveldata/master/opentraveldata/optd_por_public.csv\"\n",
    "\n",
    "local_path = \"/dbfs/tmp/airport-timezones.csv\"\n",
    "\n",
    "with open(local_path, \"wb\") as f:\n",
    "    f.write(requests.get(url).content)\n",
    "\n",
    "# Use the corresponding DBFS path for Spark\n",
    "dbfs_path = \"dbfs:/tmp/airport-timezones.csv\"\n",
    "\n",
    "# Copy to DBFS\n",
    "dbutils.fs.cp(\"file:\" + local_path, dbfs_path)\n",
    "\n",
    "df_airport_timezones = (\n",
    "    spark.read\n",
    "        .option(\"header\", True)\n",
    "        .option(\"delimiter\", \"^\")\n",
    "        .option(\"inferSchema\", True)\n",
    "        .csv(dbfs_path)\n",
    ")\n",
    "\n",
    "cols_to_keep = [\n",
    "    \"iata_code\",\n",
    "    \"icao_code\",\n",
    "    \"faa_code\",\n",
    "    \"timezone\",\n",
    "    \"latitude\",\n",
    "    \"longitude\"\n",
    "]\n",
    "\n",
    "df_airport_timezones = df_airport_timezones.select(cols_to_keep)\n",
    "\n",
    "# Write to Parquet and read back to avoid file re-read issues\n",
    "parquet_path = \"dbfs:/tmp/airport-timezones-cached.parquet\"\n",
    "\n",
    "# Check if Parquet file already exists (avoid re-reading CSV on subsequent runs)\n",
    "    # Try to read existing Parquet file first\n",
    "    df_airport_timezones = spark.read.parquet(parquet_path)\n",
    "    df_airport_timezones.count()  # Materialize to break lineage\n",
    "    print(\"\u2713 Using existing Parquet file\")\n",
    "except:\n",
    "    # Parquet doesn't exist, create it from CSV DataFrame\n",
    "    print(\"Creating Parquet file from CSV...\")\n",
    "    df_airport_timezones.write.mode(\"overwrite\").parquet(parquet_path)\n",
    "    df_airport_timezones = spark.read.parquet(parquet_path)\n",
    "    df_airport_timezones.count()  # Materialize to break lineage from CSV\n",
    "    print(\"\u2713 Parquet file created and materialized\")\n",
    "\n",
    "\n",
    "\n",
    "    # Try to read existing Parquet file first\n",
    "except:\n",
    "    # Parquet doesn't exist, create it from CSV\n",
    "\n",
    "\n",
    "# display(df_airport_timezones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5aec3705-4b5a-4b63-b77f-c1ec95e08d35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_df_stats(df, sample_size_estimate=False, tmp_path=\"/tmp/df_profile_tmp\"):\n",
    "    \"\"\"\n",
    "    Returns number of rows, columns, and size of a Spark DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): Input Spark DataFrame.\n",
    "        sample_size_estimate (bool): \n",
    "            If True -> estimate size using string length of rows.\n",
    "            If False -> write dataframe to parquet to get accurate size.\n",
    "        tmp_path (str): Temporary path for size calculation if sample_size_estimate=False.\n",
    "    \n",
    "    Returns:\n",
    "        dict: {\n",
    "            \"rows\": <int>,\n",
    "            \"columns\": <int>,\n",
    "            \"size_bytes\": <int>\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    # Number of rows\n",
    "    rows = df.count()\n",
    "    \n",
    "    # Number of columns\n",
    "    cols = len(df.columns)\n",
    "    \n",
    "    # Size calculation\n",
    "    if sample_size_estimate:\n",
    "        size_bytes = df.rdd.map(lambda row: len(str(row))).sum()\n",
    "    else:\n",
    "        # Remove tmp directory if exists\n",
    "        try:\n",
    "            dbutils.fs.rm(tmp_path, recurse=True)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Write to temp parquet\n",
    "        df.write.mode(\"overwrite\").parquet(tmp_path)\n",
    "        \n",
    "        # Sum file sizes\n",
    "        size_bytes = sum([f.size for f in dbutils.fs.ls(tmp_path)])\n",
    "        \n",
    "        # Clean up temp files\n",
    "        dbutils.fs.rm(tmp_path, recurse=True)\n",
    "    \n",
    "    return {\n",
    "        \"rows\": rows,\n",
    "        \"columns\": cols,\n",
    "        \"size_bytes\": size_bytes\n",
    "    }\n",
    "\n",
    "print(get_df_stats(df_airport_timezones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a492736a-e204-4557-8026-763aa78e3dab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(get_df_stats(df_airport_codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed5466e9-6bfd-495d-839e-d362fc392593",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join airport_codes table with airport_timezones \n",
    "\n",
    "# Perform a left join on both iata_code and icao_code\n",
    "df_airport_joined = (\n",
    "    df_airport_codes.alias(\"a\")\n",
    "    .join(\n",
    "        df_airport_timezones.alias(\"b\"),\n",
    "        (\n",
    "            (F.col(\"a.iata_code\") == F.col(\"b.iata_code\")) |\n",
    "            (F.col(\"a.icao_code\") == F.col(\"b.icao_code\"))\n",
    "        ),\n",
    "        how=\"left\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display result\n",
    "# display(df_airport_joined) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23c8cc7e-11cc-437a-ab24-8e15c0c03ee8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# print(\"Row count before join:\", df_airport_codes.count())\n",
    "# print(\"Row count after join:\", df_airport_joined.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2f83c80-922f-4c2a-9d44-6818291b49d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_airport_joined_clean = df_airport_joined.select(\n",
    "    \"ident\", \"type\", \"name\", \"elevation_ft\",\n",
    "    \"continent\", \"iso_country\", \"iso_region\", \"municipality\",\n",
    "    \"a.icao_code\", \"a.iata_code\", \"gps_code\", \"local_code\",\n",
    "    \"coordinates\",\n",
    "    F.col(\"b.latitude\").alias(\"latitude\"),\n",
    "    F.col(\"b.longitude\").alias(\"longitude\"),\n",
    "    F.col(\"b.timezone\").alias(\"timezone\"),\n",
    "    F.col(\"b.faa_code\").alias(\"faa_code_otd\")\n",
    ")\n",
    "\n",
    "# display(df_airport_joined_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6aaecd71-6324-44dc-a275-5ab2e21afaf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_airport_joined = df_airport_joined_clean\n",
    "\n",
    "# Define window partitioned by your key columns\n",
    "w = Window.partitionBy(\"iata_code\", \"icao_code\").orderBy(\n",
    "    F.when(F.col(\"latitude\").isNotNull(), 1).otherwise(2),  # prefer non-null lat\n",
    "    F.when(F.col(\"timezone\").isNotNull(), 1).otherwise(2)   # then prefer non-null tz\n",
    ")\n",
    "\n",
    "# Add row number within each partition and keep the first\n",
    "df_airport_dedup = (\n",
    "    df_airport_joined\n",
    "    .withColumn(\"row_num\", F.row_number().over(w))\n",
    "    .filter(F.col(\"row_num\") == 1)\n",
    "    .drop(\"row_num\")\n",
    ")\n",
    "\n",
    "# print(\"Row count before de-dup:\", df_airport_joined.count())\n",
    "# print(\"Row count after de-dup:\", df_airport_dedup.count()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a257efb-0971-471a-95ed-439b15a22391",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add timestamp column\n",
    "\n",
    "df_flights = df_flights.withColumn(\n",
    "    \"fl_date_timestamp\",\n",
    "    to_timestamp(col(\"fl_date\"), \"yyyy-MM-dd\")\n",
    ")\n",
    "\n",
    "# Parse full datetime string (keep hours/minutes)\n",
    "df_weather = df_weather.withColumn(\n",
    "    \"date_timestamp\",\n",
    "    F.to_timestamp(F.col(\"date\"), \"yyyy-MM-dd'T'HH:mm:ss\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b5c6c4b-f5b8-4841-97b2-3471637a47e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(df_airport_dedup.select(\"timezone\").groupBy(\"timezone\").count().orderBy(\"count\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "890f8201-d210-4338-af06-625ba481f0dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_airports = df_airport_dedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d25a33b1-a332-41ab-af40-bb17d400d0d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# De-duplicate \n",
    "df_flights = df_flights.dropDuplicates() # This has known duplicates\n",
    "df_weather = df_weather.dropDuplicates()\n",
    "df_stations = df_stations.dropDuplicates()\n",
    "df_airports = df_airports.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SANITY CHECK: After dropDuplicates\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"SANITY CHECK: After dropDuplicates\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Row count\n",
    "try:\n",
    "    row_count = df_flights.count()\n",
    "    print(f\"\\n\u2713 Row count: {row_count:,}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n\u2717 Error counting rows: {e}\")\n",
    "    row_count = None\n",
    "\n",
    "# Check for NULLs in key columns\n",
    "print(\"\\n--- NULL Analysis ---\")\n",
    "key_columns = []\n",
    "if 'origin' in df_flights.columns:\n",
    "    key_columns.append('origin')\n",
    "if 'dest' in df_flights.columns:\n",
    "    key_columns.append('dest')\n",
    "if 'origin_station_id' in df_flights.columns:\n",
    "    key_columns.append('origin_station_id')\n",
    "if 'origin_latitude' in df_flights.columns:\n",
    "    key_columns.append('origin_latitude')\n",
    "if 'origin_longitude' in df_flights.columns:\n",
    "    key_columns.append('origin_longitude')\n",
    "\n",
    "for col_name in key_columns:\n",
    "    try:\n",
    "        null_count = df_flights.filter(F.col(col_name).isNull()).count()\n",
    "        null_pct = (null_count / row_count * 100) if row_count else 0\n",
    "        print(f\"  {col_name}: {null_count:,} NULLs ({null_pct:.2f}%)\")\n",
    "    except Exception as e:\n",
    "        print(f\"  {col_name}: Error - {e}\")\n",
    "\n",
    "# Identify dropped airports (if applicable)\n",
    "if 'origin_station_id' in df_flights.columns:\n",
    "    try:\n",
    "        dropped_airports = df_flights.filter(\n",
    "            F.col('origin_station_id').isNull()\n",
    "        ).select('origin').distinct()\n",
    "        dropped_count = dropped_airports.count()\n",
    "        if dropped_count > 0:\n",
    "            print(f\"\\n--- Airports Without Stations: {dropped_count} ---\")\n",
    "            print(\"Sample airports without stations:\")\n",
    "            dropped_airports.show(20, truncate=False)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError analyzing dropped airports: {e}\")\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SANITY CHECK: After dropDuplicates\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"SANITY CHECK: After dropDuplicates\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Row count\n",
    "try:\n",
    "    row_count = df_flights.count()\n",
    "    print(f\"\\n\u2713 Row count: {row_count:,}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n\u2717 Error counting rows: {e}\")\n",
    "    row_count = None\n",
    "\n",
    "# Check for NULLs in key columns\n",
    "print(\"\\n--- NULL Analysis ---\")\n",
    "key_columns = []\n",
    "if 'origin' in df_flights.columns:\n",
    "    key_columns.append('origin')\n",
    "if 'dest' in df_flights.columns:\n",
    "    key_columns.append('dest')\n",
    "if 'origin_station_id' in df_flights.columns:\n",
    "    key_columns.append('origin_station_id')\n",
    "if 'origin_latitude' in df_flights.columns:\n",
    "    key_columns.append('origin_latitude')\n",
    "if 'origin_longitude' in df_flights.columns:\n",
    "    key_columns.append('origin_longitude')\n",
    "\n",
    "for col_name in key_columns:\n",
    "    try:\n",
    "        null_count = df_flights.filter(F.col(col_name).isNull()).count()\n",
    "        null_pct = (null_count / row_count * 100) if row_count else 0\n",
    "        print(f\"  {col_name}: {null_count:,} NULLs ({null_pct:.2f}%)\")\n",
    "    except Exception as e:\n",
    "        print(f\"  {col_name}: Error - {e}\")\n",
    "\n",
    "# Identify dropped airports (if applicable)\n",
    "if 'origin_station_id' in df_flights.columns:\n",
    "    try:\n",
    "        dropped_airports = df_flights.filter(\n",
    "            F.col('origin_station_id').isNull()\n",
    "        ).select('origin').distinct()\n",
    "        dropped_count = dropped_airports.count()\n",
    "        if dropped_count > 0:\n",
    "            print(f\"\\n--- Airports Without Stations: {dropped_count} ---\")\n",
    "            print(\"Sample airports without stations:\")\n",
    "            dropped_airports.show(20, truncate=False)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError analyzing dropped airports: {e}\")\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77ffc19e-3760-452b-a741-ca43e18c8c2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Convert all column names to lowercase\n",
    "df_flights = df_flights.toDF(*[c.lower() for c in df_flights.columns])\n",
    "df_weather = df_weather.toDF(*[c.lower() for c in df_weather.columns])\n",
    "df_stations = df_stations.toDF(*[c.lower() for c in df_stations.columns])\n",
    "df_airports = df_airports.toDF(*[c.lower() for c in df_airports.columns])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb5d774d-63ba-4533-b382-61dbc9db88cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Split coordinates column into latitude and longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "813a2070-85d6-4570-b434-f074d808a188",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create latitude and longitude colums in Airport Codes\n",
    "df_airports = df_airports.withColumn(\n",
    "    \"coordinates\",\n",
    "    regexp_replace(\"coordinates\", \"[()]\", \"\")\n",
    ")\n",
    "\n",
    "df_airports = df_airports.withColumn(\n",
    "    \"lat_lon\",\n",
    "    split(\"coordinates\", \",\")\n",
    ")\n",
    "\n",
    "df_airports = df_airports.withColumn(\n",
    "    \"latitude\",\n",
    "    trim(df_airports[\"lat_lon\"].getItem(0))\n",
    ").withColumn(\n",
    "    \"longitude\",\n",
    "    trim(df_airports[\"lat_lon\"].getItem(1))\n",
    ").drop(\"lat_lon\")\n",
    "\n",
    "df_airports.cache()\n",
    "# display(df_airports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75817a98-3e06-455f-81a8-3d6e38984fb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(df_stations.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3834e81-807a-437f-93db-557f75e2dbb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### create neighbor_iata column\n",
    "\n",
    "Since Stations data's contains 4 digit ICAO codes, we need to  convert it to 3 digit IATA codes for US airports.  \n",
    "\n",
    "Example: \n",
    "ICAO (4-letter): e.g., KJFK, KLAX, EGLL\n",
    "\u2192 This is what neighbor_call contains.\n",
    "\n",
    "IATA (3-letter): e.g., JFK, LAX, LHR\n",
    "\u2192 This is what flight data uses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9915a3db-edf2-4ae5-8290-d9a3511e8a23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_stations = df_stations.withColumn(\n",
    "    \"neighbor_iata\",\n",
    "    F.expr(\"substring(neighbor_call, 2, 3)\")  # remove leading 'K' for US airports\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "000fb933-4a07-4872-ba7b-2ef5d93df8d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(df_stations.select(\"neighbor_call\",\"neighbor_iata\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14890342-25fb-4554-ab13-d1b766f81299",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### create scheduled departure time  (local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1283d32b-60d7-43c2-aa2f-9d4091eed521",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lpad, concat, to_timestamp, expr\n",
    "\n",
    "# Combine fl_date_timestamp and crs_dep_time to create sched_depart_date_time\n",
    "df_flights = df_flights.withColumn(\n",
    "    \"sched_depart_date_time\",\n",
    "    to_timestamp(\n",
    "        concat(\n",
    "            col(\"fl_date\"),\n",
    "            lpad(col(\"crs_dep_time\"), 4, \"0\")\n",
    "        ),\n",
    "        \"yyyy-MM-ddHHmm\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ff18ff0-412e-4255-8ed1-62097d7424e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(df_airport_codes.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0e03150-7b55-4dee-8c51-38bd34fe7adc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Implement Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1d33036-a17b-47e1-9991-577f3dde35f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Step 1: Join Flights data with Airports\n",
    "\n",
    "This join is done using IATA code of the airport. We do this join to get data like latitude/longitude of the airport - both origin & destination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaa9c2be-40bc-4889-bd0f-5ca58b922f92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Prepare airports dataframe - select only needed columns\n",
    "df_airports_clean = df_airports.select(\n",
    "    col(\"iata_code\"),\n",
    "    col(\"name\"),\n",
    "    col(\"latitude\").cast(\"double\").alias(\"latitude\"),\n",
    "    col(\"longitude\").cast(\"double\").alias(\"longitude\"),\n",
    "    col(\"iso_country\"),\n",
    "    col(\"timezone\")\n",
    ").filter(\n",
    "    col(\"iata_code\").isNotNull() &\n",
    "    col(\"latitude\").isNotNull() &\n",
    "    col(\"longitude\").isNotNull()\n",
    ")\n",
    "\n",
    "# print(f\"Airports count: {df_airports_clean.count()}\")\n",
    "\n",
    "# Broadcast airports (it's small enough)\n",
    "airports_broadcast = broadcast(df_airports_clean)\n",
    "\n",
    "# Perform the join\n",
    "df_flights_with_airports = (\n",
    "    df_flights.alias(\"f\")\n",
    "    .join(\n",
    "        airports_broadcast.alias(\"ao\"),\n",
    "        col(\"f.origin\") == col(\"ao.iata_code\"),\n",
    "        \"left\"\n",
    "    )\n",
    "    .join(\n",
    "        airports_broadcast.alias(\"ad\"),\n",
    "        col(\"f.dest\") == col(\"ad.iata_code\"),\n",
    "        \"left\"\n",
    "    )\n",
    "    .select(\n",
    "        col(\"f.*\"),\n",
    "        # Origin airport info\n",
    "        col(\"ao.name\").alias(\"origin_airport_name\"),\n",
    "        col(\"ao.latitude\").alias(\"origin_latitude\"),\n",
    "        col(\"ao.longitude\").alias(\"origin_longitude\"),\n",
    "        col(\"ao.iso_country\").alias(\"origin_country\"),\n",
    "        col(\"ao.timezone\").alias(\"origin_timezone\"),\n",
    "        # Destination airport info\n",
    "        col(\"ad.name\").alias(\"destination_airport_name\"),\n",
    "        col(\"ad.latitude\").alias(\"destination_latitude\"),\n",
    "        col(\"ad.longitude\").alias(\"destination_longitude\"),\n",
    "        col(\"ad.iso_country\").alias(\"destination_country\"),\n",
    "        col(\"ad.timezone\").alias(\"destination_timezone\")\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SANITY CHECK: After Airport Join\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"SANITY CHECK: After Airport Join\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Row count\n",
    "try:\n",
    "    row_count = df_flights_with_airports.count()\n",
    "    print(f\"\\n\u2713 Row count: {row_count:,}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n\u2717 Error counting rows: {e}\")\n",
    "    row_count = None\n",
    "\n",
    "# Check for NULLs in key columns\n",
    "print(\"\\n--- NULL Analysis ---\")\n",
    "key_columns = []\n",
    "if 'origin' in df_flights_with_airports.columns:\n",
    "    key_columns.append('origin')\n",
    "if 'dest' in df_flights_with_airports.columns:\n",
    "    key_columns.append('dest')\n",
    "if 'origin_station_id' in df_flights_with_airports.columns:\n",
    "    key_columns.append('origin_station_id')\n",
    "if 'origin_latitude' in df_flights_with_airports.columns:\n",
    "    key_columns.append('origin_latitude')\n",
    "if 'origin_longitude' in df_flights_with_airports.columns:\n",
    "    key_columns.append('origin_longitude')\n",
    "\n",
    "for col_name in key_columns:\n",
    "    try:\n",
    "        null_count = df_flights_with_airports.filter(F.col(col_name).isNull()).count()\n",
    "        null_pct = (null_count / row_count * 100) if row_count else 0\n",
    "        print(f\"  {col_name}: {null_count:,} NULLs ({null_pct:.2f}%)\")\n",
    "    except Exception as e:\n",
    "        print(f\"  {col_name}: Error - {e}\")\n",
    "\n",
    "# Identify dropped airports (if applicable)\n",
    "if 'origin_station_id' in df_flights_with_airports.columns:\n",
    "    try:\n",
    "        dropped_airports = df_flights_with_airports.filter(\n",
    "            F.col('origin_station_id').isNull()\n",
    "        ).select('origin').distinct()\n",
    "        dropped_count = dropped_airports.count()\n",
    "        if dropped_count > 0:\n",
    "            print(f\"\\n--- Airports Without Stations: {dropped_count} ---\")\n",
    "            print(\"Sample airports without stations:\")\n",
    "            dropped_airports.show(20, truncate=False)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError analyzing dropped airports: {e}\")\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20708473-61d5-4fe3-afa9-7ec6ee8a0155",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(df_flights_combined.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9daaa9ea-142a-4769-88fb-391431980f5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Get all timestamps in UTC using timezone info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc436143-4a54-4774-9939-bd43ecc6dccf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_flights_with_airports = df_flights_with_airports.withColumn(\n",
    "    \"sched_depart_date_time_UTC\",\n",
    "    F.to_utc_timestamp(\"sched_depart_date_time\", F.col(\"origin_timezone\"))\n",
    ")\n",
    "\n",
    "# Two hours prior to scheduled departure in UTC\n",
    "df_flights_with_airports = df_flights_with_airports.withColumn(\n",
    "    \"two_hours_prior_depart_UTC\",\n",
    "    expr(\"sched_depart_date_time_UTC - INTERVAL 2 HOURS\")\n",
    ")\n",
    "\n",
    "# Four hours prior to scheduled departure in UTC\n",
    "df_flights_with_airports = df_flights_with_airports.withColumn(\n",
    "    \"four_hours_prior_depart_UTC\",\n",
    "    expr(\"sched_depart_date_time_UTC - INTERVAL 4 HOURS\")\n",
    ")\n",
    "\n",
    "# display(df_flights_with_airports.select(\n",
    "#     \"fl_date\", \"crs_dep_time\", \"sched_depart_date_time\",\n",
    "#     \"sched_depart_date_time_UTC\", \"two_hours_prior_depart_UTC\", \"four_hours_prior_depart_UTC\"\n",
    "# ).limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SANITY CHECK: After Airport Join\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"SANITY CHECK: After Airport Join\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Row count\n",
    "try:\n",
    "    row_count = df_flights_with_airports.count()\n",
    "    print(f\"\\n\u2713 Row count: {row_count:,}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n\u2717 Error counting rows: {e}\")\n",
    "    row_count = None\n",
    "\n",
    "# Check for NULLs in key columns\n",
    "print(\"\\n--- NULL Analysis ---\")\n",
    "key_columns = []\n",
    "if 'origin' in df_flights_with_airports.columns:\n",
    "    key_columns.append('origin')\n",
    "if 'dest' in df_flights_with_airports.columns:\n",
    "    key_columns.append('dest')\n",
    "if 'origin_station_id' in df_flights_with_airports.columns:\n",
    "    key_columns.append('origin_station_id')\n",
    "if 'origin_latitude' in df_flights_with_airports.columns:\n",
    "    key_columns.append('origin_latitude')\n",
    "if 'origin_longitude' in df_flights_with_airports.columns:\n",
    "    key_columns.append('origin_longitude')\n",
    "\n",
    "for col_name in key_columns:\n",
    "    try:\n",
    "        null_count = df_flights_with_airports.filter(F.col(col_name).isNull()).count()\n",
    "        null_pct = (null_count / row_count * 100) if row_count else 0\n",
    "        print(f\"  {col_name}: {null_count:,} NULLs ({null_pct:.2f}%)\")\n",
    "    except Exception as e:\n",
    "        print(f\"  {col_name}: Error - {e}\")\n",
    "\n",
    "# Identify dropped airports (if applicable)\n",
    "if 'origin_station_id' in df_flights_with_airports.columns:\n",
    "    try:\n",
    "        dropped_airports = df_flights_with_airports.filter(\n",
    "            F.col('origin_station_id').isNull()\n",
    "        ).select('origin').distinct()\n",
    "        dropped_count = dropped_airports.count()\n",
    "        if dropped_count > 0:\n",
    "            print(f\"\\n--- Airports Without Stations: {dropped_count} ---\")\n",
    "            print(\"Sample airports without stations:\")\n",
    "            dropped_airports.show(20, truncate=False)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError analyzing dropped airports: {e}\")\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d69dd1a6-18e2-4e14-9780-c3ab9a6123dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"\\n\u2713 Join 1 Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e55f6f40-2b1b-46b9-b8f2-9ef48b4b509e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Checkpoint 1: Store after first join "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84d9e342-3e17-4035-9d2f-7296ce5e5211",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_flights_combined \n",
    "\n",
    "# df_flights_with_station_notnull = spark.read.parquet(f\"{section_DIR}/df_flights_with_station_{data_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0d3fb07-6d07-4a19-9679-2d0b78e39795",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Step 2: Join Flights+Airport with Stations \n",
    "\n",
    "Here we try two different approaches:\n",
    " - Using neighbor_call/neighbor_iata \n",
    " - Using latitude & longitude\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79b34e2b-0272-4a53-a79c-2033bc7c2da0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(df_stations.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fb25b40-f3c5-4478-8d1f-3bac20bba6dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "So that shows all of them match! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93de77c9-5aaf-48cb-b319-a870f41b7045",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Join type 2: get stationID from stations table using lat/long\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0773d09-c3ac-4196-97dd-0ac877dd011c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Doing a cross-join with the whole flights data & station data will create too many combinations. It's better we only get the join done with distinct airports and later combine it with original flights data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc162287-cbb1-4874-a2ab-10c834637b5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "TODO: Try Haversine approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "920c4e06-f0e9-498a-bef7-56aadff179dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Get distinct origin airports\n",
    "df_distinct_origins = (\n",
    "    df_flights_with_airports\n",
    "    .select(\"origin\", \"origin_latitude\", \"origin_longitude\")\n",
    "    .distinct()\n",
    "    .filter(\n",
    "        col(\"origin_latitude\").isNotNull() & \n",
    "        col(\"origin_longitude\").isNotNull()\n",
    "    )\n",
    "    .repartition(200, \"origin\")\n",
    "    #.cache()\n",
    ")\n",
    "\n",
    "print(f\"Distinct origins: {df_distinct_origins.count()}\")\n",
    "\n",
    "# Step 2: Prepare stations dataframe\n",
    "df_stations_clean = df_stations.select(\n",
    "    col(\"station_id\"),\n",
    "    col(\"lat\").cast(\"double\"),\n",
    "    col(\"lon\").cast(\"double\")\n",
    ").filter(\n",
    "    col(\"station_id\").isNotNull() &\n",
    "    col(\"lat\").isNotNull() &\n",
    "    col(\"lon\").isNotNull()\n",
    ")\n",
    "\n",
    "stations_broadcast = broadcast(df_stations_clean)\n",
    "\n",
    "# Step 3: Perform spatial join with bounding box filter\n",
    "df_candidates = (\n",
    "    df_distinct_origins.alias(\"a\")\n",
    "    .join(\n",
    "        stations_broadcast.alias(\"s\"),\n",
    "        (col(\"s.lat\").between(col(\"a.origin_latitude\") - 0.5, col(\"a.origin_latitude\") + 0.5)) &\n",
    "        (col(\"s.lon\").between(col(\"a.origin_longitude\") - 0.5, col(\"a.origin_longitude\") + 0.5)),\n",
    "        \"inner\"\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"distance_km\",\n",
    "        F.expr(\"\"\"\n",
    "            6371 * acos(\n",
    "                least(1.0,\n",
    "                    cos(radians(a.origin_latitude)) * cos(radians(s.lat)) *\n",
    "                    cos(radians(s.lon) - radians(a.origin_longitude)) +\n",
    "                    sin(radians(a.origin_latitude)) * sin(radians(s.lat))\n",
    "                )\n",
    "            )\n",
    "        \"\"\")\n",
    "    )\n",
    "    .filter(col(\"distance_km\") < 50)\n",
    ")\n",
    "\n",
    "# Step 4: Get nearest station per airport\n",
    "window_spec = Window.partitionBy(\"a.origin\").orderBy(\"distance_km\")\n",
    "\n",
    "df_nearest_stations = (\n",
    "    df_candidates\n",
    "    .withColumn(\"rank\", F.row_number().over(window_spec))\n",
    "    .filter(col(\"rank\") == 1)\n",
    "    .select(\n",
    "        col(\"a.origin\").alias(\"origin\"),\n",
    "        col(\"s.station_id\").alias(\"origin_station_id\"),\n",
    "        col(\"distance_km\").alias(\"station_distance_km\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# print(f\"Airports with stations: {df_nearest_stations.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "947317fb-c56a-4e8c-9b93-bd7e6dcb5ed5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Step 5: Join back to main flights dataframe\n",
    "df_flights_with_station = (\n",
    "    df_flights_with_airports\n",
    "    .join(df_nearest_stations, on=\"origin\", how=\"left\")\n",
    ")\n",
    "\n",
    "# Check quality\n",
    "null_count = df_flights_with_station.filter(col(\"origin_station_id\").isNull()).count()\n",
    "print(f\"Flights without station: {null_count:,}\")\n",
    "\n",
    "# Filter out flights without stations\n",
    "df_flights_with_station_clean = df_flights_with_station.filter(\n",
    "    col(\"origin_station_id\").isNotNull()\n",
    ")\n",
    "\n",
    "# print(f\"Final flights with stations: {df_flights_with_station_clean.count():,}\")\n",
    "print(\"\\n\u2713 Join 2 Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SANITY CHECK: After Station Filter (DROPS ROWS)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"SANITY CHECK: After Station Filter (DROPS ROWS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Row count\n",
    "try:\n",
    "    row_count = df_flights_with_station_clean.count()\n",
    "    print(f\"\\n\u2713 Row count: {row_count:,}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n\u2717 Error counting rows: {e}\")\n",
    "    row_count = None\n",
    "\n",
    "# Check for NULLs in key columns\n",
    "print(\"\\n--- NULL Analysis ---\")\n",
    "key_columns = []\n",
    "if 'origin' in df_flights_with_station_clean.columns:\n",
    "    key_columns.append('origin')\n",
    "if 'dest' in df_flights_with_station_clean.columns:\n",
    "    key_columns.append('dest')\n",
    "if 'origin_station_id' in df_flights_with_station_clean.columns:\n",
    "    key_columns.append('origin_station_id')\n",
    "if 'origin_latitude' in df_flights_with_station_clean.columns:\n",
    "    key_columns.append('origin_latitude')\n",
    "if 'origin_longitude' in df_flights_with_station_clean.columns:\n",
    "    key_columns.append('origin_longitude')\n",
    "\n",
    "for col_name in key_columns:\n",
    "    try:\n",
    "        null_count = df_flights_with_station_clean.filter(F.col(col_name).isNull()).count()\n",
    "        null_pct = (null_count / row_count * 100) if row_count else 0\n",
    "        print(f\"  {col_name}: {null_count:,} NULLs ({null_pct:.2f}%)\")\n",
    "    except Exception as e:\n",
    "        print(f\"  {col_name}: Error - {e}\")\n",
    "\n",
    "# Identify dropped airports (if applicable)\n",
    "if 'origin_station_id' in df_flights_with_station_clean.columns:\n",
    "    try:\n",
    "        dropped_airports = df_flights_with_station_clean.filter(\n",
    "            F.col('origin_station_id').isNull()\n",
    "        ).select('origin').distinct()\n",
    "        dropped_count = dropped_airports.count()\n",
    "        if dropped_count > 0:\n",
    "            print(f\"\\n--- Airports Without Stations: {dropped_count} ---\")\n",
    "            print(\"Sample airports without stations:\")\n",
    "            dropped_airports.show(20, truncate=False)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError analyzing dropped airports: {e}\")\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SANITY CHECK: After Station Join (Left)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"SANITY CHECK: After Station Join (Left)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Row count\n",
    "try:\n",
    "    row_count = df_flights_with_station.count()\n",
    "    print(f\"\\n\u2713 Row count: {row_count:,}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n\u2717 Error counting rows: {e}\")\n",
    "    row_count = None\n",
    "\n",
    "# Check for NULLs in key columns\n",
    "print(\"\\n--- NULL Analysis ---\")\n",
    "key_columns = []\n",
    "if 'origin' in df_flights_with_station.columns:\n",
    "    key_columns.append('origin')\n",
    "if 'dest' in df_flights_with_station.columns:\n",
    "    key_columns.append('dest')\n",
    "if 'origin_station_id' in df_flights_with_station.columns:\n",
    "    key_columns.append('origin_station_id')\n",
    "if 'origin_latitude' in df_flights_with_station.columns:\n",
    "    key_columns.append('origin_latitude')\n",
    "if 'origin_longitude' in df_flights_with_station.columns:\n",
    "    key_columns.append('origin_longitude')\n",
    "\n",
    "for col_name in key_columns:\n",
    "    try:\n",
    "        null_count = df_flights_with_station.filter(F.col(col_name).isNull()).count()\n",
    "        null_pct = (null_count / row_count * 100) if row_count else 0\n",
    "        print(f\"  {col_name}: {null_count:,} NULLs ({null_pct:.2f}%)\")\n",
    "    except Exception as e:\n",
    "        print(f\"  {col_name}: Error - {e}\")\n",
    "\n",
    "# Identify dropped airports (if applicable)\n",
    "if 'origin_station_id' in df_flights_with_station.columns:\n",
    "    try:\n",
    "        dropped_airports = df_flights_with_station.filter(\n",
    "            F.col('origin_station_id').isNull()\n",
    "        ).select('origin').distinct()\n",
    "        dropped_count = dropped_airports.count()\n",
    "        if dropped_count > 0:\n",
    "            print(f\"\\n--- Airports Without Stations: {dropped_count} ---\")\n",
    "            print(\"Sample airports without stations:\")\n",
    "            dropped_airports.show(20, truncate=False)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError analyzing dropped airports: {e}\")\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb62db97-37b3-44c8-a34d-e978696162c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Unpersist cached data\n",
    "df_distinct_origins.unpersist()\n",
    "\n",
    "# display(df_flights_with_station_clean.limit(5))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5027387b-786d-437f-a300-651ef8bf55d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Inspect the columns where origin_station_id is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SANITY CHECK: After Station Filter (DROPS ROWS)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"SANITY CHECK: After Station Filter (DROPS ROWS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Row count\n",
    "try:\n",
    "    row_count = df_flights_with_station_clean.count()\n",
    "    print(f\"\\n\u2713 Row count: {row_count:,}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n\u2717 Error counting rows: {e}\")\n",
    "    row_count = None\n",
    "\n",
    "# Check for NULLs in key columns\n",
    "print(\"\\n--- NULL Analysis ---\")\n",
    "key_columns = []\n",
    "if 'origin' in df_flights_with_station_clean.columns:\n",
    "    key_columns.append('origin')\n",
    "if 'dest' in df_flights_with_station_clean.columns:\n",
    "    key_columns.append('dest')\n",
    "if 'origin_station_id' in df_flights_with_station_clean.columns:\n",
    "    key_columns.append('origin_station_id')\n",
    "if 'origin_latitude' in df_flights_with_station_clean.columns:\n",
    "    key_columns.append('origin_latitude')\n",
    "if 'origin_longitude' in df_flights_with_station_clean.columns:\n",
    "    key_columns.append('origin_longitude')\n",
    "\n",
    "for col_name in key_columns:\n",
    "    try:\n",
    "        null_count = df_flights_with_station_clean.filter(F.col(col_name).isNull()).count()\n",
    "        null_pct = (null_count / row_count * 100) if row_count else 0\n",
    "        print(f\"  {col_name}: {null_count:,} NULLs ({null_pct:.2f}%)\")\n",
    "    except Exception as e:\n",
    "        print(f\"  {col_name}: Error - {e}\")\n",
    "\n",
    "# Identify dropped airports (if applicable)\n",
    "if 'origin_station_id' in df_flights_with_station_clean.columns:\n",
    "    try:\n",
    "        dropped_airports = df_flights_with_station_clean.filter(\n",
    "            F.col('origin_station_id').isNull()\n",
    "        ).select('origin').distinct()\n",
    "        dropped_count = dropped_airports.count()\n",
    "        if dropped_count > 0:\n",
    "            print(f\"\\n--- Airports Without Stations: {dropped_count} ---\")\n",
    "            print(\"Sample airports without stations:\")\n",
    "            dropped_airports.show(20, truncate=False)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError analyzing dropped airports: {e}\")\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SANITY CHECK: After Station Join (Left)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"SANITY CHECK: After Station Join (Left)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Row count\n",
    "try:\n",
    "    row_count = df_flights_with_station.count()\n",
    "    print(f\"\\n\u2713 Row count: {row_count:,}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n\u2717 Error counting rows: {e}\")\n",
    "    row_count = None\n",
    "\n",
    "# Check for NULLs in key columns\n",
    "print(\"\\n--- NULL Analysis ---\")\n",
    "key_columns = []\n",
    "if 'origin' in df_flights_with_station.columns:\n",
    "    key_columns.append('origin')\n",
    "if 'dest' in df_flights_with_station.columns:\n",
    "    key_columns.append('dest')\n",
    "if 'origin_station_id' in df_flights_with_station.columns:\n",
    "    key_columns.append('origin_station_id')\n",
    "if 'origin_latitude' in df_flights_with_station.columns:\n",
    "    key_columns.append('origin_latitude')\n",
    "if 'origin_longitude' in df_flights_with_station.columns:\n",
    "    key_columns.append('origin_longitude')\n",
    "\n",
    "for col_name in key_columns:\n",
    "    try:\n",
    "        null_count = df_flights_with_station.filter(F.col(col_name).isNull()).count()\n",
    "        null_pct = (null_count / row_count * 100) if row_count else 0\n",
    "        print(f\"  {col_name}: {null_count:,} NULLs ({null_pct:.2f}%)\")\n",
    "    except Exception as e:\n",
    "        print(f\"  {col_name}: Error - {e}\")\n",
    "\n",
    "# Identify dropped airports (if applicable)\n",
    "if 'origin_station_id' in df_flights_with_station.columns:\n",
    "    try:\n",
    "        dropped_airports = df_flights_with_station.filter(\n",
    "            F.col('origin_station_id').isNull()\n",
    "        ).select('origin').distinct()\n",
    "        dropped_count = dropped_airports.count()\n",
    "        if dropped_count > 0:\n",
    "            print(f\"\\n--- Airports Without Stations: {dropped_count} ---\")\n",
    "            print(\"Sample airports without stations:\")\n",
    "            dropped_airports.show(20, truncate=False)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError analyzing dropped airports: {e}\")\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f104beb2-e601-4203-9cbc-6d3d1d54ce4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Most of the ones which are null are:\n",
    "\n",
    "- PSE \u2014 Mercedita Airport, Ponce, Puerto Rico \ud83c\uddf5\ud83c\uddf7\n",
    "- GUM \u2014 Antonio B. Won Pat International Airport, Guam \ud83c\uddec\ud83c\uddfa\n",
    "- PPG \u2014 Pago Pago International Airport (Tafuna Airport), American Samoa \ud83c\udde6\ud83c\uddf8\n",
    "- ISN \u2014 Sloulin Field International Airport (now replaced by Williston Basin International Airport), Williston, North Dakota, USA \ud83c\uddfa\ud83c\uddf8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0ede3fa-6c62-4507-98c3-2acd4fd0507d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_flights_with_station_notnull = df_flights_with_station.filter(F.col(\"origin_station_id\").isNotNull())\n",
    "# display(df_flights_with_station_notnull.limit(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38ceb277-d26f-4c18-a994-81f88a597544",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Checkpoint 2: Before Final Join\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0f8a59c-d1ab-495c-bde6-6e765a5a9da9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load intermediate result later\n",
    "#df_flights_with_station_notnull = spark.read.parquet(flights_with_station_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60708117-04fe-4598-9987-57e11e84692f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Step 3: Join Flights+Airports+Stations with Weather "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b8e09c9-0c33-4479-b805-b0b5450c6701",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Run the full join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "746f3c49-2bbe-497f-af5b-f87b46a0f132",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sparkContext.setCheckpointDir(\n",
    "    checkpoints_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cdb0ee3-4fbc-4887-a6fc-477616f3ddcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 3: Weather Join with Station + Bucket Co-Partitioning\n",
    "# MAGIC \n",
    "# MAGIC *Goal:*\n",
    "# MAGIC 1. Partition both flights and weather by (station, bucket).\n",
    "# MAGIC 2. Ensure each partition sees weather from the current and previous bucket.\n",
    "# MAGIC 3. Within each partition, pick the closest weather record **before** the flight's two-hours-prior time.\n",
    "# MAGIC 4. Retain all flights, even if no weather is found.\n",
    "\n",
    "# COMMAND ----------\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"JOIN 3: Flights + Weather (Station + Bucket Co-Partitioned)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Disable Photon just for this complex join step\n",
    "spark.conf.set(\"spark.databricks.photon.enabled\", \"false\")\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Time bucketing configuration\n",
    "# -------------------------------------------------------------------\n",
    "BUCKET_INTERVAL_MINUTES = 30\n",
    "BUCKET_INTERVAL_SECONDS = BUCKET_INTERVAL_MINUTES * 60\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. Normalize types and add time buckets\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Ensure station IDs have consistent types\n",
    "df_weather = df_weather.withColumn(\"station\", col(\"station\").cast(\"string\"))\n",
    "df_flights_with_station = df_flights_with_station.withColumn(\n",
    "    \"origin_station_id\", col(\"origin_station_id\").cast(\"string\")\n",
    ")\n",
    "\n",
    "# Weather: compute base bucket\n",
    "df_weather_bucketed = (\n",
    "    df_weather\n",
    "    .withColumn(\"weather_ts\", col(\"date_timestamp\").cast(\"timestamp\"))\n",
    "    .withColumn(\n",
    "        \"bucket\",\n",
    "        (\n",
    "            col(\"weather_ts\").cast(\"long\") / F.lit(BUCKET_INTERVAL_SECONDS)\n",
    "        ).cast(\"long\") * F.lit(BUCKET_INTERVAL_SECONDS)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create a shifted version so that for bucket B we also have weather\n",
    "# from the previous bucket (B - interval).\n",
    "# We do this by shifting weather from bucket (original) to bucket + interval.\n",
    "# So for any bucket B, it contains:\n",
    "#   - weather originally in B\n",
    "#   - weather originally in B - interval (now shifted to B)\n",
    "df_weather_shifted_for_prev = df_weather_bucketed.withColumn(\n",
    "    \"bucket\",\n",
    "    col(\"bucket\") + F.lit(BUCKET_INTERVAL_SECONDS)\n",
    ")\n",
    "\n",
    "# Union original + shifted weather\n",
    "df_weather_for_join = df_weather_bucketed.unionByName(df_weather_shifted_for_prev)\n",
    "\n",
    "# Repartition weather by (station, bucket) so each partition has\n",
    "# the station and its two relevant buckets' weather.\n",
    "df_weather_for_join = df_weather_for_join.repartition(\"station\", \"bucket\")\n",
    "\n",
    "\n",
    "# Flights: compute bucket using \"two_hours_prior_depart_utc\"\n",
    "df_flights_with_buckets = (\n",
    "    df_flights_with_station\n",
    "    .withColumn(\"flight_ts\", col(\"two_hours_prior_depart_utc\").cast(\"timestamp\"))\n",
    "    .withColumn(\n",
    "        \"bucket\",\n",
    "        (\n",
    "            col(\"flight_ts\").cast(\"long\") / F.lit(BUCKET_INTERVAL_SECONDS)\n",
    "        ).cast(\"long\") * F.lit(BUCKET_INTERVAL_SECONDS)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Repartition flights by (origin_station_id, bucket)\n",
    "# so they align with weather partitions.\n",
    "df_flights_with_buckets = df_flights_with_buckets.repartition(\n",
    "    \"origin_station_id\", \"bucket\"\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. Join on station + bucket (current + previous weather bucket present)\n",
    "# -------------------------------------------------------------------\n",
    "print(\"\\nExecuting station + bucket join...\")\n",
    "\n",
    "# remove duplicate column\n",
    "df_weather_for_join = df_weather_for_join.withColumnRenamed(\"year\", \"weather_year\")\n",
    "\n",
    "df_joined = (\n",
    "    df_flights_with_buckets.alias(\"f\")\n",
    "    .join(\n",
    "        df_weather_for_join.alias(\"w\"),\n",
    "        (\n",
    "            (col(\"f.origin_station_id\") == col(\"w.station\")) &\n",
    "            (col(\"f.bucket\") == col(\"w.bucket\"))\n",
    "        ),\n",
    "        \"left\"  # retain all flights\n",
    "    )\n",
    "    .select(\n",
    "        col(\"f.*\"),\n",
    "        col(\"w.*\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. On each partition: keep weather only if it is BEFORE the flight,\n",
    "#    and retain the closest one.\n",
    "# -------------------------------------------------------------------\n",
    "print(\"\\nFiltering weather <= flight time and ranking...\")\n",
    "\n",
    "# Filter: keep rows where weather is null (no match) OR\n",
    "# weather_ts <= two_hours_prior_depart_utc.\n",
    "df_joined_filtered = df_joined.filter(\n",
    "    (col(\"w.weather_ts\").isNull()) |\n",
    "    (col(\"w.weather_ts\") <= col(\"f.flight_ts\"))\n",
    ")\n",
    "\n",
    "# Compute time difference in seconds (flight_ts - weather_ts).\n",
    "# For null weather_ts we keep time_diff_sec as null.\n",
    "df_with_diff = (\n",
    "    df_joined_filtered\n",
    "    .withColumn(\n",
    "        \"time_diff_sec\",\n",
    "        F.when(\n",
    "            col(\"w.weather_ts\").isNull(),\n",
    "            F.lit(None).cast(\"long\")\n",
    "        ).otherwise(\n",
    "            col(\"f.flight_ts\").cast(\"long\") - col(\"w.weather_ts\").cast(\"long\")\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Define a window per unique flight. Adjust keys if needed\n",
    "# to match the flight's natural primary key in your data.\n",
    "window_closest = Window.partitionBy(\n",
    "    \"f.origin_station_id\",\n",
    "    \"f.fl_date\",\n",
    "    \"f.crs_dep_time\",\n",
    "    \"f.op_carrier_fl_num\"\n",
    ").orderBy(\n",
    "    col(\"time_diff_sec\").isNull().asc(),  # non-null weather first\n",
    "    col(\"time_diff_sec\").asc_nulls_last()\n",
    ")\n",
    "\n",
    "df_ranked = (\n",
    "    df_with_diff\n",
    "    .withColumn(\"has_weather\", col(\"w.weather_ts\").isNotNull())\n",
    "    .withColumn(\"weather_rank\", F.row_number().over(window_closest))\n",
    ")\n",
    "\n",
    "# Keep:\n",
    "#   - all flights with no weather (has_weather = false)\n",
    "#   - the single closest weather row (rank = 1) when weather exists\n",
    "df_final = (\n",
    "    df_ranked\n",
    "    .filter(\n",
    "        (~col(\"has_weather\")) | (col(\"weather_rank\") == 1)\n",
    "    )\n",
    "    .drop(\n",
    "        \"weather_rank\",\n",
    "        \"time_diff_sec\",\n",
    "        \"bucket\",            # bucket (same name on both sides) no longer needed\n",
    "        \"station\",           # weather station if you don't need it; keep if required\n",
    "        \"has_weather\",\n",
    "        \"weather_ts\",        # drop if date_timestamp already present & sufficient\n",
    "        \"flight_ts\"          # drop if two_hours_prior_depart_utc is kept\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\u2713 Closest-weather selection complete (all flights retained).\")\n",
    "\n",
    "# (Optional) Persist or write df_final; no count() here to avoid full scan.\n",
    "# Example:\n",
    "# df_final.write.format(\"delta\").mode(\"overwrite\").save(\"/path/to/output\")\n",
    "\n",
    "# Re-enable Photon for subsequent (simpler) operations\n",
    "spark.conf.set(\"spark.databricks.photon.enabled\", \"true\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\u2713 JOIN 3 COMPLETE (Station + Bucket Co-Partitioned)\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SANITY CHECK: After Weather Join\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"SANITY CHECK: After Weather Join\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Row count\n",
    "try:\n",
    "    row_count = df_joined.count()\n",
    "    print(f\"\\n\u2713 Row count: {row_count:,}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n\u2717 Error counting rows: {e}\")\n",
    "    row_count = None\n",
    "\n",
    "# Check for NULLs in key columns\n",
    "print(\"\\n--- NULL Analysis ---\")\n",
    "key_columns = []\n",
    "if 'origin' in df_joined.columns:\n",
    "    key_columns.append('origin')\n",
    "if 'dest' in df_joined.columns:\n",
    "    key_columns.append('dest')\n",
    "if 'origin_station_id' in df_joined.columns:\n",
    "    key_columns.append('origin_station_id')\n",
    "if 'origin_latitude' in df_joined.columns:\n",
    "    key_columns.append('origin_latitude')\n",
    "if 'origin_longitude' in df_joined.columns:\n",
    "    key_columns.append('origin_longitude')\n",
    "\n",
    "for col_name in key_columns:\n",
    "    try:\n",
    "        null_count = df_joined.filter(F.col(col_name).isNull()).count()\n",
    "        null_pct = (null_count / row_count * 100) if row_count else 0\n",
    "        print(f\"  {col_name}: {null_count:,} NULLs ({null_pct:.2f}%)\")\n",
    "    except Exception as e:\n",
    "        print(f\"  {col_name}: Error - {e}\")\n",
    "\n",
    "# Identify dropped airports (if applicable)\n",
    "if 'origin_station_id' in df_joined.columns:\n",
    "    try:\n",
    "        dropped_airports = df_joined.filter(\n",
    "            F.col('origin_station_id').isNull()\n",
    "        ).select('origin').distinct()\n",
    "        dropped_count = dropped_airports.count()\n",
    "        if dropped_count > 0:\n",
    "            print(f\"\\n--- Airports Without Stations: {dropped_count} ---\")\n",
    "            print(\"Sample airports without stations:\")\n",
    "            dropped_airports.show(20, truncate=False)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError analyzing dropped airports: {e}\")\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flight Lineage Join - Final Step\n",
    "\n",
    "This section adds flight lineage features by joining each flight to its previous flight in the lineage (same aircraft, previous flight). \n",
    "\n",
    "**Key Features Added:**\n",
    "- Previous flight information (origin, dest, times, delays, etc.)\n",
    "- Turnover time (time from arrival to departure)\n",
    "- Cumulative delays\n",
    "- Sequence information (lineage rank)\n",
    "- Jump detection\n",
    "\n",
    "**CRITICAL: NO ROWS ARE DROPPED** - All flights are preserved. Flights without previous flight data get NULL values which are handled via imputation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FLIGHT LINEAGE JOIN - FINAL STEP\n",
    "# ============================================================================\n",
    "# All flight lineage feature engineering code grouped together here\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FLIGHT LINEAGE JOIN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Identify tail_num column (handle variations)\n",
    "tail_num_candidates = ['tail_num', 'TAIL_NUM', 'tail_number', 'TAIL_NUMBER', 'op_unique_carrier_tail_num']\n",
    "tail_num_col = None\n",
    "\n",
    "for candidate in tail_num_candidates:\n",
    "    if candidate in df_final.columns:\n",
    "        tail_num_col = candidate\n",
    "        print(f\"\u2713 Found tail_num column: {tail_num_col}\")\n",
    "        break\n",
    "\n",
    "if tail_num_col is None:\n",
    "    # Try pattern matching\n",
    "    tail_cols = [c for c in df_final.columns if 'tail' in c.lower()]\n",
    "    if tail_cols:\n",
    "        tail_num_col = tail_cols[0]\n",
    "        print(f\"\u2713 Found tail_num column via pattern matching: {tail_num_col}\")\n",
    "    else:\n",
    "        raise ValueError(f\"Could not find tail_num column. Available columns: {df_final.columns[:20]}...\")\n",
    "\n",
    "# Step 2: Prepare arrival timestamp for ranking\n",
    "# Use scheduled arrival time as fallback if actual unavailable (preserves all rows)\n",
    "print(\"\\nStep 2: Creating arrival timestamp for ranking...\")\n",
    "df_final = df_final.withColumn(\n",
    "    'arrival_time_for_ranking',\n",
    "    F.coalesce(col('arr_time'), col('crs_arr_time'))\n",
    ")\n",
    "\n",
    "# Convert to timestamp for proper temporal ordering\n",
    "df_final = df_final.withColumn(\n",
    "    'arrival_timestamp',\n",
    "    F.when(\n",
    "        col('arrival_time_for_ranking').isNotNull() & col('fl_date').isNotNull(),\n",
    "        F.to_timestamp(\n",
    "            F.concat(\n",
    "                col('fl_date'),\n",
    "                F.lpad(col('arrival_time_for_ranking').cast('string'), 4, '0')\n",
    "            ),\n",
    "            'yyyy-MM-ddHHmm'\n",
    "        )\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "print(\"\u2713 Arrival timestamp created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create window specification and rank flights\n",
    "print(\"\\nStep 3: Creating window specification and ranking flights...\")\n",
    "\n",
    "# Window: partition by tail_num, order by arrival timestamp (ASCENDING)\n",
    "# ASCENDING order is critical: earliest flights first, so LAG gets the previous flight\n",
    "window_spec = Window.partitionBy(tail_num_col).orderBy(F.col('arrival_timestamp').asc_nulls_last())\n",
    "\n",
    "# Rank flights by arrival time (1 = earliest, higher = more recent)\n",
    "# lineage_rank is highly predictive: indicates how many flights aircraft has completed\n",
    "df_final = df_final.withColumn('lineage_rank', F.row_number().over(window_spec))\n",
    "\n",
    "print(\"\u2713 Flights ranked\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Get Previous Flight Data Using LAG\n",
    "print(\"\\nStep 4: Getting previous flight data using LAG...\")\n",
    "\n",
    "# Core Previous Flight Information (Required for Feature Engineering)\n",
    "df_final = df_final.withColumn('prev_flight_origin', F.lag('origin', 1).over(window_spec))\n",
    "df_final = df_final.withColumn('prev_flight_dest', F.lag('dest', 1).over(window_spec))\n",
    "df_final = df_final.withColumn('prev_flight_actual_dep_time', F.lag('dep_time', 1).over(window_spec))\n",
    "df_final = df_final.withColumn('prev_flight_actual_arr_time', F.lag('arr_time', 1).over(window_spec))\n",
    "df_final = df_final.withColumn('prev_flight_dep_delay', F.lag('dep_delay', 1).over(window_spec))\n",
    "df_final = df_final.withColumn('prev_flight_arr_delay', F.lag('arr_delay', 1).over(window_spec))\n",
    "df_final = df_final.withColumn('prev_flight_air_time', F.lag('air_time', 1).over(window_spec))\n",
    "\n",
    "# Scheduled Times (for fallback when actual unavailable)\n",
    "df_final = df_final.withColumn('prev_flight_crs_dep_time', F.lag('crs_dep_time', 1).over(window_spec))\n",
    "df_final = df_final.withColumn('prev_flight_crs_arr_time', F.lag('crs_arr_time', 1).over(window_spec))\n",
    "df_final = df_final.withColumn('prev_flight_crs_elapsed_time', F.lag('crs_elapsed_time', 1).over(window_spec))\n",
    "\n",
    "# Time Components (for turn time and taxi time calculations)\n",
    "df_final = df_final.withColumn('prev_flight_taxi_in', F.lag('taxi_in', 1).over(window_spec))\n",
    "df_final = df_final.withColumn('prev_flight_taxi_out', F.lag('taxi_out', 1).over(window_spec))\n",
    "df_final = df_final.withColumn('prev_flight_wheels_off', F.lag('wheels_off', 1).over(window_spec))\n",
    "df_final = df_final.withColumn('prev_flight_wheels_on', F.lag('wheels_on', 1).over(window_spec))\n",
    "df_final = df_final.withColumn('prev_flight_actual_elapsed_time', F.lag('actual_elapsed_time', 1).over(window_spec))\n",
    "\n",
    "# Route and Flight Information\n",
    "df_final = df_final.withColumn('prev_flight_distance', F.lag('distance', 1).over(window_spec))\n",
    "df_final = df_final.withColumn('prev_flight_op_carrier', F.lag('op_carrier', 1).over(window_spec))\n",
    "df_final = df_final.withColumn('prev_flight_op_carrier_fl_num', F.lag('op_carrier_fl_num', 1).over(window_spec))\n",
    "df_final = df_final.withColumn('prev_flight_fl_date', F.lag('fl_date', 1).over(window_spec))\n",
    "\n",
    "# Status Flags (for understanding previous flight context)\n",
    "df_final = df_final.withColumn('prev_flight_cancelled', F.lag('cancelled', 1).over(window_spec))\n",
    "df_final = df_final.withColumn('prev_flight_diverted', F.lag('diverted', 1).over(window_spec))\n",
    "\n",
    "print(\"\u2713 Previous flight data retrieved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SANITY CHECK: After Weather Join\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"SANITY CHECK: After Weather Join\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Row count\n",
    "try:\n",
    "    row_count = df_joined.count()\n",
    "    print(f\"\\n\u2713 Row count: {row_count:,}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n\u2717 Error counting rows: {e}\")\n",
    "    row_count = None\n",
    "\n",
    "# Check for NULLs in key columns\n",
    "print(\"\\n--- NULL Analysis ---\")\n",
    "key_columns = []\n",
    "if 'origin' in df_joined.columns:\n",
    "    key_columns.append('origin')\n",
    "if 'dest' in df_joined.columns:\n",
    "    key_columns.append('dest')\n",
    "if 'origin_station_id' in df_joined.columns:\n",
    "    key_columns.append('origin_station_id')\n",
    "if 'origin_latitude' in df_joined.columns:\n",
    "    key_columns.append('origin_latitude')\n",
    "if 'origin_longitude' in df_joined.columns:\n",
    "    key_columns.append('origin_longitude')\n",
    "\n",
    "for col_name in key_columns:\n",
    "    try:\n",
    "        null_count = df_joined.filter(F.col(col_name).isNull()).count()\n",
    "        null_pct = (null_count / row_count * 100) if row_count else 0\n",
    "        print(f\"  {col_name}: {null_count:,} NULLs ({null_pct:.2f}%)\")\n",
    "    except Exception as e:\n",
    "        print(f\"  {col_name}: Error - {e}\")\n",
    "\n",
    "# Identify dropped airports (if applicable)\n",
    "if 'origin_station_id' in df_joined.columns:\n",
    "    try:\n",
    "        dropped_airports = df_joined.filter(\n",
    "            F.col('origin_station_id').isNull()\n",
    "        ).select('origin').distinct()\n",
    "        dropped_count = dropped_airports.count()\n",
    "        if dropped_count > 0:\n",
    "            print(f\"\\n--- Airports Without Stations: {dropped_count} ---\")\n",
    "            print(\"Sample airports without stations:\")\n",
    "            dropped_airports.show(20, truncate=False)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError analyzing dropped airports: {e}\")\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Compute Actual Turnover Time (with data leakage check)\n",
    "\n",
    "# Convert crs_dep_time to minutes (needed for data leakage checks and calculations)\n",
    "df_final = df_final.withColumn(\n",
    "    'crs_dep_time_minutes',\n",
    "    F.when(\n",
    "        col('crs_dep_time').isNotNull(),\n",
    "        (F.floor(col('crs_dep_time') / 100) * 60 + (col('crs_dep_time') % 100))\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "print(\"\\nStep 6: Computing actual turnover time (with data leakage check)...\")\n",
    "\n",
    "# Convert crs_dep_time to minutes (needed for data leakage checks and calculations)\n",
    "df_final = df_final.withColumn(\n",
    "    'crs_dep_time_minutes',\n",
    "    F.when(\n",
    "        col('crs_dep_time').isNotNull(),\n",
    "        (F.floor(col('crs_dep_time') / 100) * 60 + (col('crs_dep_time') % 100))\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "\n",
    "# Convert actual times to minutes\n",
    "df_final = df_final.withColumn(\n",
    "    'prev_flight_actual_arr_time_minutes',\n",
    "    F.when(\n",
    "        col('prev_flight_actual_arr_time').isNotNull(),\n",
    "        (F.floor(col('prev_flight_actual_arr_time') / 100) * 60 + (col('prev_flight_actual_arr_time') % 100))\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "df_final = df_final.withColumn(\n",
    "    'actual_dep_time_minutes',\n",
    "    F.when(\n",
    "        col('dep_time').isNotNull(),\n",
    "        (F.floor(col('dep_time') / 100) * 60 + (col('dep_time') % 100))\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "# Check data leakage: prev_arr_time must be <= crs_dep_time - 2 hours (120 minutes)\n",
    "df_final = df_final.withColumn(\n",
    "    'prev_arr_time_safe_to_use',\n",
    "    F.when(\n",
    "        (col('prev_flight_actual_arr_time_minutes').isNotNull()) &\n",
    "        (col('crs_dep_time_minutes').isNotNull()),\n",
    "        col('prev_flight_actual_arr_time_minutes') <= (col('crs_dep_time_minutes') - 120)\n",
    "    ).otherwise(False)\n",
    ")\n",
    "\n",
    "# Actual turnover time (only compute if safe)\n",
    "df_final = df_final.withColumn(\n",
    "    'lineage_actual_turnover_time_minutes',\n",
    "    F.when(\n",
    "        (col('prev_arr_time_safe_to_use') == True) &\n",
    "        (col('actual_dep_time_minutes').isNotNull()) &\n",
    "        (col('prev_flight_actual_arr_time_minutes').isNotNull()),\n",
    "        F.when(\n",
    "            col('actual_dep_time_minutes') >= col('prev_flight_actual_arr_time_minutes'),\n",
    "            col('actual_dep_time_minutes') - col('prev_flight_actual_arr_time_minutes')\n",
    "        ).otherwise(\n",
    "            col('actual_dep_time_minutes') + 1440 - col('prev_flight_actual_arr_time_minutes')\n",
    "        )\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "# Create aliases\n",
    "df_final = df_final.withColumn('lineage_actual_taxi_time_minutes', col('lineage_actual_turnover_time_minutes'))\n",
    "df_final = df_final.withColumn('lineage_actual_turn_time_minutes', col('lineage_actual_turnover_time_minutes'))\n",
    "\n",
    "print(\"\u2713 Actual turnover time computed with data leakage check\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Compute Expected Flight Time and Cumulative Features\n",
    "print(\"\\nStep 7: Computing expected flight time and cumulative features...\")\n",
    "\n",
    "# Expected flight time = scheduled arrival - scheduled departure\n",
    "df_final = df_final.withColumn(\n",
    "    'crs_arr_time_minutes',\n",
    "    F.when(\n",
    "        col('crs_arr_time').isNotNull(),\n",
    "        (F.floor(col('crs_arr_time') / 100) * 60 + (col('crs_arr_time') % 100))\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "df_final = df_final.withColumn(\n",
    "    'lineage_expected_flight_time_minutes',\n",
    "    F.when(\n",
    "        (col('crs_arr_time_minutes').isNotNull()) &\n",
    "        (col('crs_dep_time_minutes').isNotNull()),\n",
    "        F.when(\n",
    "            col('crs_arr_time_minutes') >= col('crs_dep_time_minutes'),\n",
    "            col('crs_arr_time_minutes') - col('crs_dep_time_minutes')\n",
    "        ).otherwise(\n",
    "            col('crs_arr_time_minutes') + 1440 - col('crs_dep_time_minutes')  # Day rollover\n",
    "        )\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "# Cumulative delay (exclude current row)\n",
    "window_spec_cumulative = Window.partitionBy(tail_num_col).orderBy(F.col('arrival_timestamp').asc_nulls_last()).rowsBetween(Window.unboundedPreceding, -1)\n",
    "df_final = df_final.withColumn('lineage_cumulative_delay', F.sum('dep_delay').over(window_spec_cumulative))\n",
    "\n",
    "# Number of previous flights\n",
    "df_final = df_final.withColumn('lineage_num_previous_flights', F.count('*').over(window_spec_cumulative))\n",
    "\n",
    "# Average and max delay from previous flights\n",
    "df_final = df_final.withColumn('lineage_avg_delay_previous_flights', F.avg('dep_delay').over(window_spec_cumulative))\n",
    "df_final = df_final.withColumn('lineage_max_delay_previous_flights', F.max('dep_delay').over(window_spec_cumulative))\n",
    "\n",
    "print(\"\u2713 Cumulative features computed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Jump Detection\n",
    "print(\"\\nStep 8: Detecting jumps (aircraft repositioning)...\")\n",
    "\n",
    "# Jump = previous flight didn't arrive at current origin\n",
    "# After imputation, prev_flight_dest will never be NULL (imputed to origin for first flight)\n",
    "df_final = df_final.withColumn(\n",
    "    'lineage_is_jump',\n",
    "    F.when(col('lineage_rank') == 1, F.lit(False))  # First flight is not a jump\n",
    "    .when(col('prev_flight_dest') != col('origin'), F.lit(True))  # Route mismatch = jump\n",
    "    .otherwise(F.lit(False))\n",
    ")\n",
    "\n",
    "print(\"\u2713 Jump detection complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Check Data Leakage for All Risky Columns and Create columns_with_data_leakage Array\n",
    "print(\"\\nStep 9: Checking data leakage for all risky columns and creating columns_with_data_leakage array...\")\n",
    "\n",
    "# Create prediction_cutoff column: scheduled departure time - 2 hours (in minutes since midnight)\n",
    "# This is the cutoff time for data leakage - any actual event after this time has data leakage\n",
    "df_final = df_final.withColumn(\n",
    "    'prediction_cutoff_minutes',\n",
    "    F.when(\n",
    "        col('crs_dep_time_minutes').isNotNull(),\n",
    "        col('crs_dep_time_minutes') - 120\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "# Also create as timestamp for easier comparison\n",
    "df_final = df_final.withColumn(\n",
    "    'prediction_cutoff_timestamp',\n",
    "    F.when(\n",
    "        col('sched_depart_date_time').isNotNull(),\n",
    "        F.expr(\"sched_depart_date_time - INTERVAL 2 HOURS\")\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "# Data leakage = timestamp is AFTER prediction_cutoff, i.e., timestamp > prediction_cutoff_minutes\n",
    "\n",
    "# Convert all actual time columns to minutes for checking\n",
    "df_final = df_final.withColumn(\n",
    "    'prev_flight_actual_dep_time_minutes',\n",
    "    F.when(\n",
    "        col('prev_flight_actual_dep_time').isNotNull(),\n",
    "        (F.floor(col('prev_flight_actual_dep_time') / 100) * 60 + (col('prev_flight_actual_dep_time') % 100))\n",
    "    ).otherwise(None)\n",
    ")\n",
    "# prev_flight_actual_arr_time_minutes already exists from Step 6\n",
    "\n",
    "df_final = df_final.withColumn(\n",
    "    'prev_flight_wheels_off_minutes',\n",
    "    F.when(\n",
    "        col('prev_flight_wheels_off').isNotNull(),\n",
    "        (F.floor(col('prev_flight_wheels_off') / 100) * 60 + (col('prev_flight_wheels_off') % 100))\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "df_final = df_final.withColumn(\n",
    "    'prev_flight_wheels_on_minutes',\n",
    "    F.when(\n",
    "        col('prev_flight_wheels_on').isNotNull(),\n",
    "        (F.floor(col('prev_flight_wheels_on') / 100) * 60 + (col('prev_flight_wheels_on') % 100))\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "# Check each risky timestamp column for data leakage\n",
    "# Data leakage = timestamp is AFTER prediction_cutoff, i.e., timestamp > prediction_cutoff_minutes\n",
    "df_final = df_final.withColumn(\n",
    "    'prev_flight_actual_dep_time_has_leakage',\n",
    "    F.when(\n",
    "        (col('prev_flight_actual_dep_time_minutes').isNotNull()) &\n",
    "        (col('prediction_cutoff_minutes').isNotNull()),\n",
    "        col('prev_flight_actual_dep_time_minutes') > col('prediction_cutoff_minutes')\n",
    "    ).otherwise(False)\n",
    ")\n",
    "\n",
    "df_final = df_final.withColumn(\n",
    "    'prev_flight_actual_arr_time_has_leakage',\n",
    "    F.when(\n",
    "        (col('prev_flight_actual_arr_time_minutes').isNotNull()) &\n",
    "        (col('prediction_cutoff_minutes').isNotNull()),\n",
    "        col('prev_flight_actual_arr_time_minutes') > col('prediction_cutoff_minutes')\n",
    "    ).otherwise(False)\n",
    ")\n",
    "\n",
    "df_final = df_final.withColumn(\n",
    "    'prev_flight_wheels_off_has_leakage',\n",
    "    F.when(\n",
    "        (col('prev_flight_wheels_off_minutes').isNotNull()) &\n",
    "        (col('prediction_cutoff_minutes').isNotNull()),\n",
    "        col('prev_flight_wheels_off_minutes') > col('prediction_cutoff_minutes')\n",
    "    ).otherwise(False)\n",
    ")\n",
    "\n",
    "df_final = df_final.withColumn(\n",
    "    'prev_flight_wheels_on_has_leakage',\n",
    "    F.when(\n",
    "        (col('prev_flight_wheels_on_minutes').isNotNull()) &\n",
    "        (col('prediction_cutoff_minutes').isNotNull()),\n",
    "        col('prev_flight_wheels_on_minutes') > col('prediction_cutoff_minutes')\n",
    "    ).otherwise(False)\n",
    ")\n",
    "\n",
    "# For duration fields, check if their source columns have data leakage\n",
    "# dep_delay is derived from actual_dep_time\n",
    "df_final = df_final.withColumn(\n",
    "    'prev_flight_dep_delay_has_leakage',\n",
    "    col('prev_flight_actual_dep_time_has_leakage')\n",
    ")\n",
    "\n",
    "# arr_delay is derived from actual_arr_time\n",
    "df_final = df_final.withColumn(\n",
    "    'prev_flight_arr_delay_has_leakage',\n",
    "    col('prev_flight_actual_arr_time_has_leakage')\n",
    ")\n",
    "\n",
    "# taxi_in is derived from wheels_on (arrival) - check if wheels_on has leakage\n",
    "df_final = df_final.withColumn(\n",
    "    'prev_flight_taxi_in_has_leakage',\n",
    "    col('prev_flight_wheels_on_has_leakage')\n",
    ")\n",
    "\n",
    "# taxi_out is derived from wheels_off (departure) - check if wheels_off has leakage\n",
    "df_final = df_final.withColumn(\n",
    "    'prev_flight_taxi_out_has_leakage',\n",
    "    col('prev_flight_wheels_off_has_leakage')\n",
    ")\n",
    "\n",
    "# air_time and actual_elapsed_time are derived from actual times - check if any source has leakage\n",
    "df_final = df_final.withColumn(\n",
    "    'prev_flight_air_time_has_leakage',\n",
    "    (col('prev_flight_actual_dep_time_has_leakage') | col('prev_flight_actual_arr_time_has_leakage'))\n",
    ")\n",
    "\n",
    "df_final = df_final.withColumn(\n",
    "    'prev_flight_actual_elapsed_time_has_leakage',\n",
    "    (col('prev_flight_actual_dep_time_has_leakage') | col('prev_flight_actual_arr_time_has_leakage'))\n",
    ")\n",
    "\n",
    "# cancelled and diverted - assume they may be known late (always risky, but check if we have actual times)\n",
    "# If we have actual times that have leakage, these are also risky\n",
    "df_final = df_final.withColumn(\n",
    "    'prev_flight_cancelled_has_leakage',\n",
    "    F.when(\n",
    "        col('prev_flight_cancelled').isNotNull(),\n",
    "        (col('prev_flight_actual_dep_time_has_leakage') | col('prev_flight_actual_arr_time_has_leakage'))\n",
    "    ).otherwise(False)\n",
    ")\n",
    "\n",
    "df_final = df_final.withColumn(\n",
    "    'prev_flight_diverted_has_leakage',\n",
    "    F.when(\n",
    "        col('prev_flight_diverted').isNotNull(),\n",
    "        (col('prev_flight_actual_dep_time_has_leakage') | col('prev_flight_actual_arr_time_has_leakage'))\n",
    "    ).otherwise(False)\n",
    ")\n",
    "\n",
    "# Engineered features using actual times\n",
    "# lineage_actual_turnover_time_minutes uses actual_arr_time and actual_dep_time\n",
    "df_final = df_final.withColumn(\n",
    "    'lineage_actual_turnover_time_minutes_has_leakage',\n",
    "    (col('prev_flight_actual_arr_time_has_leakage') | col('prev_flight_actual_dep_time_has_leakage'))\n",
    ")\n",
    "df_final = df_final.withColumn(\n",
    "    'lineage_actual_taxi_time_minutes_has_leakage',\n",
    "    col('lineage_actual_turnover_time_minutes_has_leakage')\n",
    ")\n",
    "df_final = df_final.withColumn(\n",
    "    'lineage_actual_turn_time_minutes_has_leakage',\n",
    "    col('lineage_actual_turnover_time_minutes_has_leakage')\n",
    ")\n",
    "\n",
    "# Cumulative features derived from actual delays\n",
    "df_final = df_final.withColumn(\n",
    "    'lineage_cumulative_delay_has_leakage',\n",
    "    col('prev_flight_dep_delay_has_leakage')\n",
    ")\n",
    "df_final = df_final.withColumn(\n",
    "    'lineage_avg_delay_previous_flights_has_leakage',\n",
    "    col('prev_flight_dep_delay_has_leakage')\n",
    ")\n",
    "df_final = df_final.withColumn(\n",
    "    'lineage_max_delay_previous_flights_has_leakage',\n",
    "    col('prev_flight_dep_delay_has_leakage')\n",
    ")\n",
    "\n",
    "# Create an array column listing all columns that have data leakage for this row\n",
    "# This is per-row, so each row knows which specific columns have leakage\n",
    "df_final = df_final.withColumn(\n",
    "    'columns_with_data_leakage',\n",
    "    F.array_remove(\n",
    "        F.array([\n",
    "            F.when(col('prev_flight_actual_dep_time_has_leakage'), F.lit('prev_flight_actual_dep_time')).otherwise(None),\n",
    "            F.when(col('prev_flight_actual_arr_time_has_leakage'), F.lit('prev_flight_actual_arr_time')).otherwise(None),\n",
    "            F.when(col('prev_flight_wheels_off_has_leakage'), F.lit('prev_flight_wheels_off')).otherwise(None),\n",
    "            F.when(col('prev_flight_wheels_on_has_leakage'), F.lit('prev_flight_wheels_on')).otherwise(None),\n",
    "            F.when(col('prev_flight_dep_delay_has_leakage'), F.lit('prev_flight_dep_delay')).otherwise(None),\n",
    "            F.when(col('prev_flight_arr_delay_has_leakage'), F.lit('prev_flight_arr_delay')).otherwise(None),\n",
    "            F.when(col('prev_flight_air_time_has_leakage'), F.lit('prev_flight_air_time')).otherwise(None),\n",
    "            F.when(col('prev_flight_taxi_in_has_leakage'), F.lit('prev_flight_taxi_in')).otherwise(None),\n",
    "            F.when(col('prev_flight_taxi_out_has_leakage'), F.lit('prev_flight_taxi_out')).otherwise(None),\n",
    "            F.when(col('prev_flight_actual_elapsed_time_has_leakage'), F.lit('prev_flight_actual_elapsed_time')).otherwise(None),\n",
    "            F.when(col('prev_flight_cancelled_has_leakage'), F.lit('prev_flight_cancelled')).otherwise(None),\n",
    "            F.when(col('prev_flight_diverted_has_leakage'), F.lit('prev_flight_diverted')).otherwise(None),\n",
    "            F.when(col('lineage_actual_turnover_time_minutes_has_leakage'), F.lit('lineage_actual_turnover_time_minutes')).otherwise(None),\n",
    "            F.when(col('lineage_actual_taxi_time_minutes_has_leakage'), F.lit('lineage_actual_taxi_time_minutes')).otherwise(None),\n",
    "            F.when(col('lineage_actual_turn_time_minutes_has_leakage'), F.lit('lineage_actual_turn_time_minutes')).otherwise(None),\n",
    "            F.when(col('lineage_cumulative_delay_has_leakage'), F.lit('lineage_cumulative_delay')).otherwise(None),\n",
    "            F.when(col('lineage_avg_delay_previous_flights_has_leakage'), F.lit('lineage_avg_delay_previous_flights')).otherwise(None),\n",
    "            F.when(col('lineage_max_delay_previous_flights_has_leakage'), F.lit('lineage_max_delay_previous_flights')).otherwise(None)\n",
    "        ]),\n",
    "        None\n",
    "    )\n",
    ")\n",
    "\n",
    "# Also keep backward-compatible safety flags\n",
    "df_final = df_final.withColumn(\n",
    "    'prev_arr_time_safe_to_use',\n",
    "    ~col('prev_flight_actual_arr_time_has_leakage')\n",
    ")\n",
    "df_final = df_final.withColumn(\n",
    "    'prev_dep_time_safe_to_use',\n",
    "    ~col('prev_flight_actual_dep_time_has_leakage')\n",
    ")\n",
    "\n",
    "print(\"\u2713 Data leakage checks complete\")\n",
    "print(f\"   Created prediction_cutoff_minutes and prediction_cutoff_timestamp columns\")\n",
    "print(f\"   Created columns_with_data_leakage array (per-row list of columns with leakage)\")\n",
    "print(f\"   Each row knows which specific columns have data leakage\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Apply Imputation for NULL Values (First Flight Handling)\n",
    "print(\"\\nStep 10: Applying imputation for NULL values (first flight handling)...\\n\")\n",
    "\n",
    "# Imputation Strategy: For first flights (lineage_rank == 1), assume aircraft was at airport\n",
    "# 4 hours before scheduled departure, with no prior delays (anti-delay: -10 minutes)\n",
    "\n",
    "# STEP 1: Calculate scheduled times backwards from current scheduled departure - 4 hours\n",
    "# Use crs_dep_time_minutes if available (from Step 5), otherwise calculate it\n",
    "# prev_flight_crs_dep_time = current_crs_dep_time - 4 hours (240 minutes)\n",
    "df_final = df_final.withColumn(\n",
    "    'prev_flight_crs_dep_time_minutes',\n",
    "    F.coalesce(\n",
    "        # If prev_flight_crs_dep_time exists, convert to minutes\n",
    "        F.when(\n",
    "            col('prev_flight_crs_dep_time').isNotNull(),\n",
    "            (F.floor(col('prev_flight_crs_dep_time') / 100) * 60 + (col('prev_flight_crs_dep_time') % 100))\n",
    "        ),\n",
    "        # Otherwise, calculate backwards: current_crs_dep_time_minutes - 240\n",
    "        F.when(\n",
    "            col('crs_dep_time_minutes').isNotNull(),\n",
    "            col('crs_dep_time_minutes') - 240\n",
    "        ).otherwise(\n",
    "            # Fallback: calculate from crs_dep_time if minutes not available\n",
    "            F.when(\n",
    "                col('crs_dep_time').isNotNull(),\n",
    "                (F.floor(col('crs_dep_time') / 100) * 60 + (col('crs_dep_time') % 100)) - 240\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Convert back to HHMM format (handle day rollover: if negative, add 1440)\n",
    "df_final = df_final.withColumn(\n",
    "    'prev_flight_crs_dep_time',\n",
    "    F.coalesce(\n",
    "        col('prev_flight_crs_dep_time'),\n",
    "        F.when(\n",
    "            col('prev_flight_crs_dep_time_minutes').isNotNull(),\n",
    "            F.when(\n",
    "                col('prev_flight_crs_dep_time_minutes') >= 0,\n",
    "                # Same day: convert minutes back to HHMM\n",
    "                (F.floor(col('prev_flight_crs_dep_time_minutes') / 60) * 100) + (col('prev_flight_crs_dep_time_minutes') % 60)\n",
    "            ).otherwise(\n",
    "                # Previous day: add 1440 minutes (24 hours)\n",
    "                (F.floor((col('prev_flight_crs_dep_time_minutes') + 1440) / 60) * 100) + ((col('prev_flight_crs_dep_time_minutes') + 1440) % 60)\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# prev_flight_crs_arr_time = same as dep time for first flight (arrived 4 hours before current dep)\n",
    "df_final = df_final.withColumn(\n",
    "    'prev_flight_crs_arr_time',\n",
    "    F.coalesce(col('prev_flight_crs_arr_time'), col('prev_flight_crs_dep_time'))\n",
    ")\n",
    "\n",
    "# prev_flight_crs_elapsed_time = current_crs_elapsed_time\n",
    "df_final = df_final.withColumn(\n",
    "    'prev_flight_crs_elapsed_time',\n",
    "    F.coalesce(col('prev_flight_crs_elapsed_time'), col('crs_elapsed_time'))\n",
    ")\n",
    "\n",
    "# STEP 2: Impute delays (anti-delay: -10 minutes)\n",
    "df_final = df_final.withColumn('prev_flight_dep_delay', F.coalesce(col('prev_flight_dep_delay'), F.lit(-10.0)))\n",
    "df_final = df_final.withColumn('prev_flight_arr_delay', F.coalesce(col('prev_flight_arr_delay'), F.lit(-10.0)))\n",
    "\n",
    "# STEP 3: Impute actual times using scheduled times (predetermined, safe)\n",
    "df_final = df_final.withColumn('prev_flight_actual_dep_time', F.coalesce(col('prev_flight_actual_dep_time'), col('prev_flight_crs_dep_time')))\n",
    "df_final = df_final.withColumn('prev_flight_actual_arr_time', F.coalesce(col('prev_flight_actual_arr_time'), col('prev_flight_crs_arr_time')))\n",
    "\n",
    "# STEP 4: Impute route information\n",
    "df_final = df_final.withColumn('prev_flight_dest', F.coalesce(col('prev_flight_dest'), col('origin')))\n",
    "df_final = df_final.withColumn('prev_flight_origin', F.coalesce(col('prev_flight_origin'), col('origin')))\n",
    "\n",
    "# STEP 5: Impute time components\n",
    "df_final = df_final.withColumn('prev_flight_air_time', F.coalesce(col('prev_flight_air_time'), col('prev_flight_crs_elapsed_time')))\n",
    "df_final = df_final.withColumn('prev_flight_taxi_in', F.coalesce(col('prev_flight_taxi_in'), F.lit(10.0)))\n",
    "df_final = df_final.withColumn('prev_flight_taxi_out', F.coalesce(col('prev_flight_taxi_out'), F.lit(15.0)))\n",
    "df_final = df_final.withColumn('prev_flight_actual_elapsed_time', F.coalesce(col('prev_flight_actual_elapsed_time'), col('prev_flight_crs_elapsed_time')))\n",
    "\n",
    "# STEP 6: Impute wheels off/on (use scheduled times)\n",
    "df_final = df_final.withColumn('prev_flight_wheels_off', F.coalesce(col('prev_flight_wheels_off'), col('prev_flight_crs_dep_time')))\n",
    "df_final = df_final.withColumn('prev_flight_wheels_on', F.coalesce(col('prev_flight_wheels_on'), col('prev_flight_crs_arr_time')))\n",
    "\n",
    "# STEP 7: Impute route distance\n",
    "df_final = df_final.withColumn('prev_flight_distance', F.coalesce(col('prev_flight_distance'), F.lit(0.0)))\n",
    "\n",
    "# STEP 8: Impute status flags\n",
    "df_final = df_final.withColumn('prev_flight_cancelled', F.coalesce(col('prev_flight_cancelled'), F.lit(0)))\n",
    "df_final = df_final.withColumn('prev_flight_diverted', F.coalesce(col('prev_flight_diverted'), F.lit(0)))\n",
    "\n",
    "# STEP 9: Impute flight metadata (use current flight values)\n",
    "df_final = df_final.withColumn('prev_flight_fl_date', F.coalesce(col('prev_flight_fl_date'), col('fl_date')))\n",
    "df_final = df_final.withColumn('prev_flight_op_carrier', F.coalesce(col('prev_flight_op_carrier'), col('op_carrier')))\n",
    "df_final = df_final.withColumn('prev_flight_op_carrier_fl_num', F.coalesce(col('prev_flight_op_carrier_fl_num'), col('op_carrier_fl_num')))\n",
    "\n",
    "# STEP 10: Impute engineered lineage features\n",
    "# Turnover Time: 240 minutes (4 hours) - represents overnight/maintenance gap\n",
    "df_final = df_final.withColumn('lineage_turnover_time_minutes', F.coalesce(col('lineage_turnover_time_minutes'), F.lit(240.0)))\n",
    "df_final = df_final.withColumn('lineage_taxi_time_minutes', F.coalesce(col('lineage_taxi_time_minutes'), col('lineage_turnover_time_minutes')))\n",
    "df_final = df_final.withColumn('lineage_turn_time_minutes', F.coalesce(col('lineage_turn_time_minutes'), col('lineage_turnover_time_minutes')))\n",
    "df_final = df_final.withColumn('lineage_actual_turnover_time_minutes', F.coalesce(col('lineage_actual_turnover_time_minutes'), F.lit(240.0)))\n",
    "df_final = df_final.withColumn('lineage_actual_taxi_time_minutes', F.coalesce(col('lineage_actual_taxi_time_minutes'), col('lineage_actual_turnover_time_minutes')))\n",
    "df_final = df_final.withColumn('lineage_actual_turn_time_minutes', F.coalesce(col('lineage_actual_turn_time_minutes'), col('lineage_actual_turnover_time_minutes')))\n",
    "\n",
    "# Cumulative delays: 0 (no previous flights)\n",
    "df_final = df_final.withColumn('lineage_cumulative_delay', F.coalesce(col('lineage_cumulative_delay'), F.lit(0.0)))\n",
    "df_final = df_final.withColumn('lineage_avg_delay_previous_flights', F.coalesce(col('lineage_avg_delay_previous_flights'), F.lit(-10.0)))\n",
    "df_final = df_final.withColumn('lineage_max_delay_previous_flights', F.coalesce(col('lineage_max_delay_previous_flights'), F.lit(-10.0)))\n",
    "df_final = df_final.withColumn('lineage_num_previous_flights', F.coalesce(col('lineage_num_previous_flights'), F.lit(0)))\n",
    "df_final = df_final.withColumn('lineage_expected_flight_time_minutes', F.coalesce(col('lineage_expected_flight_time_minutes'), col('crs_elapsed_time')))\n",
    "\n",
    "# Clean up temporary column\n",
    "df_final = df_final.drop('prev_flight_crs_dep_time_minutes')\n",
    "\n",
    "print(\"\u2713 Imputation complete - all NULLs replaced with design doc values\")\n",
    "print(\"  Scheduled times calculated backwards: current_crs_dep_time - 4 hours\")\n",
    "print(\"  Delays: -10 minutes (anti-delay)\")\n",
    "print(\"  Turnover time: 240 minutes (4 hours)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dictionary - Flight Lineage Features\n",
    "\n",
    "This section documents all new columns added by the Flight Lineage Join.\n",
    "\n",
    "### Prediction Cutoff\n",
    "- **`prediction_cutoff_minutes`**: double (nullable) - Scheduled departure time minus 2 hours, in minutes since midnight. Used to determine data leakage cutoff.\n",
    "- **`prediction_cutoff_timestamp`**: timestamp (nullable) - Scheduled departure time minus 2 hours as timestamp. Alternative format for comparison.\n",
    "\n",
    "### Previous Flight Raw Data (prev_flight_*)\n",
    "- **`lineage_rank`**: int - Rank of flight in aircraft's sequence (1 = earliest, higher = more recent). Always non-null.\n",
    "- **`prev_flight_origin`**: string (nullable) - Previous flight's origin airport code\n",
    "- **`prev_flight_dest`**: string (nullable) - Previous flight's destination airport code. Key for jump detection.\n",
    "- **`prev_flight_fl_date`**: date (nullable) - Previous flight's date\n",
    "- **`prev_flight_op_carrier`**: string (nullable) - Previous flight's operating carrier code\n",
    "- **`prev_flight_op_carrier_fl_num`**: string (nullable) - Previous flight's flight number\n",
    "- **`prev_flight_crs_dep_time`**: int (nullable) - Previous flight's scheduled departure time (HHMM format) [SAFE]\n",
    "- **`prev_flight_crs_arr_time`**: int (nullable) - Previous flight's scheduled arrival time (HHMM format) [SAFE]\n",
    "- **`prev_flight_crs_elapsed_time`**: double (nullable) - Previous flight's scheduled elapsed time (minutes) [SAFE]\n",
    "- **`prev_flight_distance`**: double (nullable) - Previous flight's distance (miles) [SAFE]\n",
    "- **`prev_flight_actual_dep_time`**: int (nullable) - Previous flight's actual departure time (HHMM format) [DATA LEAKAGE RISK]\n",
    "- **`prev_flight_actual_arr_time`**: int (nullable) - Previous flight's actual arrival time (HHMM format) [DATA LEAKAGE RISK]\n",
    "- **`prev_flight_dep_delay`**: double (nullable) - Previous flight's departure delay (minutes) [DATA LEAKAGE RISK] (derived from actual_dep_time)\n",
    "- **`prev_flight_arr_delay`**: double (nullable) - Previous flight's arrival delay (minutes) [DATA LEAKAGE RISK] (derived from actual_arr_time)\n",
    "- **`prev_flight_air_time`**: double (nullable) - Previous flight's air time (minutes) [DATA LEAKAGE RISK] (derived from actual times)\n",
    "- **`prev_flight_taxi_in`**: double (nullable) - Previous flight's taxi-in time (minutes) [DATA LEAKAGE RISK] (derived from wheels_on)\n",
    "- **`prev_flight_taxi_out`**: double (nullable) - Previous flight's taxi-out time (minutes) [DATA LEAKAGE RISK] (derived from wheels_off)\n",
    "- **`prev_flight_wheels_off`**: int (nullable) - Previous flight's wheels off time (HHMM format) [DATA LEAKAGE RISK]\n",
    "- **`prev_flight_wheels_on`**: int (nullable) - Previous flight's wheels on time (HHMM format) [DATA LEAKAGE RISK]\n",
    "- **`prev_flight_actual_elapsed_time`**: double (nullable) - Previous flight's actual elapsed time (minutes) [DATA LEAKAGE RISK] (derived from actual times)\n",
    "- **`prev_flight_cancelled`**: int (nullable) - Previous flight's cancellation status (0/1) [DATA LEAKAGE RISK] (may be known late)\n",
    "- **`prev_flight_diverted`**: int (nullable) - Previous flight's diversion status (0/1) [DATA LEAKAGE RISK] (may be known late)\n",
    "\n",
    "### Lineage Engineered Features (lineage_*)\n",
    "- **`lineage_is_jump`**: boolean - Flag indicating if this is a jump (aircraft repositioning) or data gap. True if prev_flight_dest != origin.\n",
    "- **`lineage_turnover_time_minutes`**: double (nullable) - Expected turnover time: time between previous flight's scheduled arrival and current flight's scheduled departure (minutes) [SAFE]\n",
    "- **`lineage_taxi_time_minutes`**: double (nullable) - Alias for `lineage_turnover_time_minutes` [SAFE]\n",
    "- **`lineage_turn_time_minutes`**: double (nullable) - Alias for `lineage_turnover_time_minutes` [SAFE]\n",
    "- **`lineage_actual_turnover_time_minutes`**: double (nullable) - Actual turnover time: time between previous flight's actual arrival and current flight's actual departure (minutes) [DATA LEAKAGE RISK]\n",
    "- **`lineage_actual_taxi_time_minutes`**: double (nullable) - Alias for `lineage_actual_turnover_time_minutes` [DATA LEAKAGE RISK]\n",
    "- **`lineage_actual_turn_time_minutes`**: double (nullable) - Alias for `lineage_actual_turnover_time_minutes` [DATA LEAKAGE RISK]\n",
    "- **`lineage_expected_flight_time_minutes`**: double (nullable) - Expected flight time: scheduled arrival - scheduled departure (minutes) [SAFE]\n",
    "- **`lineage_cumulative_delay`**: double (nullable) - Total delay accumulated by previous flights (minutes) [DATA LEAKAGE RISK] (derived from actual delays)\n",
    "- **`lineage_num_previous_flights`**: long (nullable) - Number of flights the aircraft has already completed [SAFE] (count only)\n",
    "- **`lineage_avg_delay_previous_flights`**: double (nullable) - Average delay across previous flights (minutes) [DATA LEAKAGE RISK] (derived from actual delays)\n",
    "- **`lineage_max_delay_previous_flights`**: double (nullable) - Maximum delay in previous flights (minutes) [DATA LEAKAGE RISK] (derived from actual delays)\n",
    "\n",
    "### Data Leakage Flags\n",
    "- **`prev_arr_time_safe_to_use`**: boolean - Flag indicating if previous arrival time is safe to use (no data leakage)\n",
    "- **`prev_dep_time_safe_to_use`**: boolean - Flag indicating if previous departure time is safe to use (no data leakage)\n",
    "- **`columns_with_data_leakage`**: array<string> - **Per-row array** listing column names that have data leakage for this specific flight. Empty array `[]` means no leakage. Example: `['prev_flight_actual_dep_time', 'prev_flight_dep_delay', ...]`\n",
    "\n",
    "### Data Leakage Rules\n",
    "- **Cutoff**: `prediction_cutoff_minutes` = scheduled departure time - 2 hours\n",
    "- **Timestamp columns**: Have leakage if timestamp > prediction_cutoff_minutes\n",
    "- **Duration columns**: Have leakage if their source timestamp columns have leakage\n",
    "- **Status flags**: Have leakage if any related actual times have leakage\n",
    "\n",
    "### Imputation Values (First Flight)\n",
    "- Delays: -10 minutes (anti-delay, early departure/arrival)\n",
    "- Turnover time: 240 minutes (4 hours, overnight/maintenance gap)\n",
    "- Cumulative delays: 0.0\n",
    "- Number of previous flights: 0\n",
    "\n",
    "**See `FLIGHT_LINEAGE_JOIN_DESIGN.md` for complete documentation.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\u2713 FLIGHT LINEAGE JOIN COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nNew columns added: ~38 lineage features\")\n",
    "print(f\"Data leakage flags: prev_arr_time_safe_to_use, prev_dep_time_safe_to_use\")\n",
    "print(f\"Risky columns documented in: columns_with_data_leakage (18 risky columns)\")\n",
    "print(f\"\\nAll flights preserved - no rows dropped\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f864398c-2ba9-44e9-b027-3e41fdb162ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Write\n",
    "df_final.repartition(200).write.mode(\"overwrite\").parquet(flights_weather_joined_path)\n",
    "print(f\"\u2713 Data saved to: {flights_weather_joined_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f57075e4-17bb-4a92-8a87-a7a3696de5e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e5ae01d-8050-4693-8589-67b89235ee97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def check_memory_usage():\n",
    "    \"\"\"Check current memory usage\"\"\"\n",
    "    \n",
    "    print(\"Memory Usage Report\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Check cached data\n",
    "    try:\n",
    "        cached_tables = spark.sql(\"SHOW TABLES\").filter(col(\"isTemporary\") == True)\n",
    "        print(f\"\\n1. Cached Tables: {cached_tables.count()}\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # 2. Check RDD cache\n",
    "    cached_rdds = spark.sparkContext._jsc.getPersistentRDDs()\n",
    "    print(f\"\\n2. Cached RDDs: {len(cached_rdds)}\")\n",
    "    \n",
    "    # 3. Check broadcast variables\n",
    "    try:\n",
    "        broadcast_count = len(spark.sparkContext._jsc.sc().getBroadcastVariables())\n",
    "        print(f\"\\n3. Broadcast Variables: {broadcast_count}\")\n",
    "    except:\n",
    "        print(f\"\\n3. Broadcast Variables: Unknown\")\n",
    "    \n",
    "    # 4. Storage memory\n",
    "    try:\n",
    "        storage_status = spark.sparkContext._jsc.sc().getExecutorStorageStatus()\n",
    "        total_memory = sum([s.maxMem() for s in storage_status])\n",
    "        used_memory = sum([s.memUsed() for s in storage_status])\n",
    "        \n",
    "        print(f\"\\n4. Storage Memory:\")\n",
    "        print(f\"   Used: {used_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"   Total: {total_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"   Usage: {used_memory / total_memory * 100:.1f}%\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n4. Storage Memory: Unable to retrieve\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Check before and after cleanup\n",
    "check_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "530a9432-8918-4009-a25a-70b07a944f53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check for duplicate columns\n",
    "print(\"Checking for duplicate columns...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "columns = df_final.columns\n",
    "duplicates = [col for col in set(columns) if columns.count(col) > 1]\n",
    "\n",
    "if duplicates:\n",
    "    print(f\"\u274c Found duplicate columns: {duplicates}\")\n",
    "    print(f\"\\nAll columns ({len(columns)}):\")\n",
    "    for i, col in enumerate(columns):\n",
    "        print(f\"  {i+1}. {col}\")\n",
    "else:\n",
    "    print(\"\u2713 No duplicates found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8b89bcf-82f9-4fff-9685-87ccc5de2d84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "year_columns = [x for x in df_final.columns if 'weather_year' in x] \n",
    "print(year_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc32921d-555e-4d7b-a294-e3b6481444ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Quick validation (optional - comment out if too slow)\n",
    "print(f\"Full join path: {flights_weather_joined_path}\")\n",
    "df_verification = spark.read.parquet(flights_weather_joined_path)\n",
    "print(f\"Verification count: {df_verification.count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a33ad331-88c7-4fd7-b29b-e09e7a60dfd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# OTPW Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47d78422-6f79-440f-bcff-31d985554117",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"dbfs:/mnt/mids-w261/OTPW_60M_Backup/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb9620a5-ee6b-4a99-8499-4a8acf7076b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_otpw_path = \"dbfs:/mnt/mids-w261/OTPW_60M_Backup/\"\n",
    "df_otpw = spark.read.parquet(df_otpw_path)\n",
    "\n",
    "df_otpw = df_otpw.toDF(*[c.lower() for c in df_otpw.columns])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7166434-7cef-440d-b89c-9c3ba91d4be7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_otpw.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54525191-9e97-403f-bd77-6a6a08766596",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Compare OTPW and Custom Join - full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a121ca3-e2f5-414e-b04c-12ce2e0816ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, sum as _sum\n",
    "\n",
    "def compare_dataframes(df1: DataFrame, df2: DataFrame, name1: str = \"df1\", name2: str = \"df2\"):\n",
    "    print(f\"===== \ud83d\udcca Comparing {name1} and {name2} =====\\n\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 1. Row Count\n",
    "    # -----------------------------\n",
    "    print(\"\u27a1 Row Counts:\")\n",
    "    print(f\"{name1}: {df1.count()}\")\n",
    "    print(f\"{name2}: {df2.count()}\\n\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2. Column Names\n",
    "    # -----------------------------\n",
    "    cols1 = set(df1.columns)\n",
    "    cols2 = set(df2.columns)\n",
    "\n",
    "    print(\"\u27a1 Columns Present in Both:\")\n",
    "    print(sorted(cols1 & cols2), \"\\n\")\n",
    "\n",
    "    print(\"\u27a1 Columns Only in\", name1)\n",
    "    print(sorted(cols1 - cols2), \"\\n\")\n",
    "\n",
    "    print(\"\u27a1 Columns Only in\", name2)\n",
    "    print(sorted(cols2 - cols1), \"\\n\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3. Schema Comparison\n",
    "    # -----------------------------\n",
    "    print(\"\u27a1 Schema Differences:\")\n",
    "    schema1 = {f.name: f.dataType for f in df1.schema.fields}\n",
    "    schema2 = {f.name: f.dataType for f in df2.schema.fields}\n",
    "\n",
    "    common = cols1 & cols2\n",
    "\n",
    "    diffs = {c: (schema1[c], schema2[c]) for c in common if schema1[c] != schema2[c]}\n",
    "\n",
    "    if diffs:\n",
    "        for col_name, (t1, t2) in diffs.items():\n",
    "            print(f\" - Column '{col_name}': {name1}={t1}, {name2}={t2}\")\n",
    "    else:\n",
    "        print(\"No schema differences.\\n\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4. Null Count Comparison\n",
    "    # -----------------------------\n",
    "    print(\"\\n\u27a1 Null Counts Per Column:\")\n",
    "\n",
    "    print(f\"\\n{name1} Null Counts:\")\n",
    "    nulls1 = df1.select([_sum(col(c).isNull().cast(\"int\")).alias(c) for c in df1.columns])\n",
    "    nulls1.show(truncate=False)\n",
    "\n",
    "    print(f\"\\n{name2} Null Counts:\")\n",
    "    nulls2 = df2.select([_sum(col(c).isNull().cast(\"int\")).alias(c) for c in df2.columns])\n",
    "    nulls2.show(truncate=False)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 5. Numeric Summary Comparison (optional)\n",
    "    # -----------------------------\n",
    "    numeric_cols1 = [f.name for f in df1.schema.fields if \"int\" in str(f.dataType) or \"double\" in str(f.dataType)]\n",
    "    numeric_cols2 = [f.name for f in df2.schema.fields if \"int\" in str(f.dataType) or \"double\" in str(f.dataType)]\n",
    "    numeric_common = list(set(numeric_cols1) & set(numeric_cols2))\n",
    "\n",
    "    if numeric_common:\n",
    "        print(\"\\n\u27a1 Summary Statistics (common numeric columns):\")\n",
    "        print(f\"Common numeric columns: {numeric_common}\\n\")\n",
    "\n",
    "        print(f\"{name1} summary:\")\n",
    "        df1.select(numeric_common).summary().show(truncate=False)\n",
    "\n",
    "        print(f\"{name2} summary:\")\n",
    "        df2.select(numeric_common).summary().show(truncate=False)\n",
    "    else:\n",
    "        print(\"\\nNo common numeric columns to summarize.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37fe654a-5751-490d-b8f9-8f947c2562d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "compare_dataframes(df_otpw, df_final, \"df_otpw\", \"df_final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dd537f1-3989-48e1-9567-10ac0d40e0a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_otpw.groupBy(\n",
    "    \"fl_date\", \"dep_time\", \"op_carrier\", \"origin\", \"dest\"\n",
    ").count().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "063ae3e8-f477-41d2-9c11-3a4795f94074",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final.groupBy(\n",
    "    \"fl_date\", \"dep_time\", \"op_carrier\", \"origin\", \"dest\"\n",
    ").count().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "160f1c68-dfa7-4770-95fe-876194640b4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Make Custom Join DF match OTPW Schema\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92c01589-ee59-4fe9-9592-3b3f786e8e62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# mismatched columns extracted from schema diff\n",
    "cols_to_string = [\n",
    "    'carrier_delay', 'wheels_on', 'first_dep_time', 'actual_elapsed_time', 'wheels_off',\n",
    "    'dest_airport_seq_id', 'dep_delay', 'sched_depart_date_time', 'day_of_week',\n",
    "    'origin_city_market_id', 'taxi_out', 'arr_time', 'crs_arr_time', 'late_aircraft_delay',\n",
    "    'month', 'longest_add_gtime', 'origin_airport_seq_id', 'dep_time', 'origin_state_fips',\n",
    "    'arr_delay_new', 'flights', 'air_time', 'dest_city_market_id', 'arr_delay_group',\n",
    "    'dest_airport_id', 'dep_del15', 'security_delay', 'crs_dep_time', 'crs_elapsed_time',\n",
    "    'arr_del15', 'op_carrier_fl_num', 'total_add_gtime', 'diverted', 'day_of_month', 'taxi_in',\n",
    "    'op_carrier_airline_id', 'distance_group', 'arr_delay', 'origin_wac', 'dep_delay_new',\n",
    "    'quarter', 'dest_wac', 'origin_airport_id', 'weather_delay', 'nas_delay', 'distance',\n",
    "    'cancelled', 'dest_state_fips', 'dep_delay_group'\n",
    "]\n",
    "\n",
    "df_final_casted = df_final\n",
    "for c in cols_to_string:\n",
    "    if c in df_final.columns:\n",
    "        df_final_casted = df_final_casted.withColumn(c, col(c).cast(\"string\"))\n",
    "\n",
    "# rename column \n",
    "rename_map = {\n",
    "    'destination_airport_name': 'dest_airport_name',\n",
    "    'destination_latitude': 'dest_airport_lat',\n",
    "    'destination_longitude': 'dest_airport_lon',\n",
    "    'destination_country': 'dest_region',\n",
    "    'destination_timezone': 'dest_type',\n",
    "    \n",
    "    'origin_latitude': 'origin_airport_lat',\n",
    "    'origin_longitude': 'origin_airport_lon',\n",
    "    'origin_country': 'origin_region',\n",
    "    'origin_timezone': 'origin_type',\n",
    "    \n",
    "    'sched_depart_date_time_UTC': 'sched_depart_date_time_utc',\n",
    "    'two_hours_prior_depart_UTC': 'two_hours_prior_depart_utc',\n",
    "    'four_hours_prior_depart_UTC': 'four_hours_prior_depart_utc',\n",
    "\n",
    "    'date_timestamp': 'date',     # if appropriate\n",
    "    'fl_date_timestamp': 'fl_date'\n",
    "}\n",
    "\n",
    "df_final_renamed = df_final_casted\n",
    "for old, new in rename_map.items():\n",
    "    if old in df_final_renamed.columns:\n",
    "        df_final_renamed = df_final_renamed.withColumnRenamed(old, new)\n",
    "\n",
    "# Drop extra columns\n",
    "cols_to_drop = [\n",
    "    'div1_airport','div1_airport_id','div1_airport_seq_id','div1_longest_gtime',\n",
    "    'div1_tail_num','div1_total_gtime','div1_wheels_off','div1_wheels_on',\n",
    "    'div2_airport','div2_airport_id','div2_airport_seq_id','div2_longest_gtime',\n",
    "    'div2_tail_num','div2_total_gtime','div2_wheels_off','div2_wheels_on',\n",
    "    'div3_airport', 'div4_airport', 'div5_airport',\n",
    "    'div_airport_landings','div_reached_dest','div_actual_elapsed_time','div_arr_delay',\n",
    "    'div_distance','station_distance_km','date_timestamp'\n",
    "]\n",
    "\n",
    "df_final_clean = df_final_renamed.drop(*[c for c in cols_to_drop if c in df_final_renamed.columns])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "918926fc-c52f-4a6f-a62e-f3e6bdcdf3ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SUMMARY: Row Count Comparison Across Pipeline Steps\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ROW COUNT SUMMARY - Tracking Flight Losses Through Pipeline\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Store counts (will be populated as cells run)\n",
    "counts = {}\n",
    "\n",
    "try:\n",
    "    counts['Initial Load'] = df_flights.count()\n",
    "except:\n",
    "    counts['Initial Load'] = 'N/A'\n",
    "\n",
    "try:\n",
    "    counts['After dropDuplicates'] = df_flights.count()\n",
    "except:\n",
    "    counts['After dropDuplicates'] = 'N/A'\n",
    "\n",
    "try:\n",
    "    counts['After Airport Join'] = df_flights_with_airports.count()\n",
    "except:\n",
    "    counts['After Airport Join'] = 'N/A'\n",
    "\n",
    "try:\n",
    "    counts['After Station Join (Left)'] = df_flights_with_station.count()\n",
    "except:\n",
    "    counts['After Station Join (Left)'] = 'N/A'\n",
    "\n",
    "try:\n",
    "    counts['After Station Filter'] = df_flights_with_station_clean.count()\n",
    "except:\n",
    "    counts['After Station Filter'] = 'N/A'\n",
    "\n",
    "try:\n",
    "    counts['After Weather Join'] = df_joined.count()\n",
    "except:\n",
    "    counts['After Weather Join'] = 'N/A'\n",
    "\n",
    "try:\n",
    "    counts['Final Before Save'] = df_final.count()\n",
    "except:\n",
    "    counts['Final Before Save'] = 'N/A'\n",
    "\n",
    "# Print summary table\n",
    "print(\"\\nStep-by-Step Row Counts:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Step':<40} {'Row Count':>20} {'Change':>15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "prev_count = None\n",
    "for step, count in counts.items():\n",
    "    if isinstance(count, int):\n",
    "        change = ''\n",
    "        if prev_count is not None:\n",
    "            diff = count - prev_count\n",
    "            pct = (diff / prev_count * 100) if prev_count > 0 else 0\n",
    "            change = f'{diff:+,} ({pct:+.2f}%)'\n",
    "        print(f'{step:<40} {count:>20,} {change:>15}')\n",
    "        prev_count = count\n",
    "    else:\n",
    "        print(f'{step:<40} {count:>20}')\n",
    "\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Calculate total loss\n",
    "if isinstance(counts.get('Initial Load'), int) and isinstance(counts.get('Final Before Save'), int):\n",
    "    total_loss = counts['Initial Load'] - counts['Final Before Save']\n",
    "    loss_pct = (total_loss / counts['Initial Load'] * 100) if counts['Initial Load'] > 0 else 0\n",
    "    print(f'\\nTotal Flights Lost: {total_loss:,} ({loss_pct:.2f}%)')\n",
    "    print(f'Final Retention Rate: {100 - loss_pct:.2f}%')\n",
    "\n",
    "# Identify biggest drop\n",
    "if isinstance(counts.get('Initial Load'), int):\n",
    "    steps_with_counts = [(k, v) for k, v in counts.items() if isinstance(v, int)]\n",
    "    if len(steps_with_counts) > 1:\n",
    "        max_drop = 0\n",
    "        max_drop_step = None\n",
    "        for i in range(1, len(steps_with_counts)):\n",
    "            prev_step, prev_count = steps_with_counts[i-1]\n",
    "            curr_step, curr_count = steps_with_counts[i]\n",
    "            drop = prev_count - curr_count\n",
    "            if drop > max_drop:\n",
    "                max_drop = drop\n",
    "                max_drop_step = curr_step\n",
    "        if max_drop > 0:\n",
    "            print(f'\\n\u26a0 Largest Drop: {max_drop:,} flights at \\'{max_drop_step}\\'')\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "defce679-3575-49bf-a0a8-2f2956fd44cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Save Final Result\n",
    "\n",
    "# COMMAND ----------\n",
    "print(\"=\"*60)\n",
    "print(\"SAVING FINAL RESULT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check current partitioning\n",
    "num_partitions = df_final.rdd.getNumPartitions()\n",
    "print(f\"Current partitions: {num_partitions}\")\n",
    "\n",
    "# Only repartition if necessary\n",
    "if num_partitions > 500:\n",
    "    print(f\"\u26a0 Too many partitions, coalescing to 200\")\n",
    "    df_final = df_final.coalesce(200)\n",
    "elif num_partitions < 10:\n",
    "    print(f\"\u26a0 Too few partitions, repartitioning to 50\")\n",
    "    df_final = df_final.repartition(50)\n",
    "else:\n",
    "    print(f\"\u2713 Partition count looks good\")\n",
    "\n",
    "# Write\n",
    "df_final.write.mode(\"overwrite\").parquet(flights_weather_joined_path)\n",
    "print(f\"\u2713 Data saved to: {flights_weather_joined_path}\")\n",
    "\n",
    "# Quick validation (optional - comment out if too slow)\n",
    "# df_verification = spark.read.parquet(flights_weather_joined_path)\n",
    "# print(f\"Verification count: {df_verification.count():,}\")\n",
    "\n",
    "# Cleanup\n",
    "df_airports_clean.unpersist()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ALL JOINS COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SANITY CHECK: Final Before Save\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"SANITY CHECK: Final Before Save\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Row count\n",
    "try:\n",
    "    row_count = df_final.count()\n",
    "    print(f\"\\n\u2713 Row count: {row_count:,}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n\u2717 Error counting rows: {e}\")\n",
    "    row_count = None\n",
    "\n",
    "# Check for NULLs in key columns\n",
    "print(\"\\n--- NULL Analysis ---\")\n",
    "key_columns = []\n",
    "if 'origin' in df_final.columns:\n",
    "    key_columns.append('origin')\n",
    "if 'dest' in df_final.columns:\n",
    "    key_columns.append('dest')\n",
    "if 'origin_station_id' in df_final.columns:\n",
    "    key_columns.append('origin_station_id')\n",
    "if 'origin_latitude' in df_final.columns:\n",
    "    key_columns.append('origin_latitude')\n",
    "if 'origin_longitude' in df_final.columns:\n",
    "    key_columns.append('origin_longitude')\n",
    "\n",
    "for col_name in key_columns:\n",
    "    try:\n",
    "        null_count = df_final.filter(F.col(col_name).isNull()).count()\n",
    "        null_pct = (null_count / row_count * 100) if row_count else 0\n",
    "        print(f\"  {col_name}: {null_count:,} NULLs ({null_pct:.2f}%)\")\n",
    "    except Exception as e:\n",
    "        print(f\"  {col_name}: Error - {e}\")\n",
    "\n",
    "# Identify dropped airports (if applicable)\n",
    "if 'origin_station_id' in df_final.columns:\n",
    "    try:\n",
    "        dropped_airports = df_final.filter(\n",
    "            F.col('origin_station_id').isNull()\n",
    "        ).select('origin').distinct()\n",
    "        dropped_count = dropped_airports.count()\n",
    "        if dropped_count > 0:\n",
    "            print(f\"\\n--- Airports Without Stations: {dropped_count} ---\")\n",
    "            print(\"Sample airports without stations:\")\n",
    "            dropped_airports.show(20, truncate=False)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError analyzing dropped airports: {e}\")\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [
    {
     "elements": [],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "848a23e1-4a38-4b97-b84c-de53113ea481",
     "origId": 4093368127199966,
     "title": "Untitled",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Custom Join (all)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}