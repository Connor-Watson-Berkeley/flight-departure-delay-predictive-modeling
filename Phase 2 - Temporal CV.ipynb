{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6556294f-6963-4ba8-825e-362d33bf9dcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark\n",
    "from pyspark.sql.functions import col, count, countDistinct\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# time\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pandas\n",
    "import pandas as pd\n",
    "\n",
    "# sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "207be85a-bedb-4a3b-bc9f-ea5da4f5c241",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create dir for group\n",
    "SECTION = \"4\"\n",
    "NUMBER = \"2\"\n",
    "GROUP_FOLDER_PATH = f\"dbfs:/student-groups/Group_{SECTION}_{NUMBER}\"\n",
    "DATA_BASE_DIR = \"dbfs:/mnt/mids-w261\"\n",
    "\n",
    "if not dbutils.fs.ls(GROUP_FOLDER_PATH):\n",
    "    created = dbutils.fs.mkdirs(GROUP_FOLDER_PATH)\n",
    "    print(f\"Directory created:\")\n",
    "else:\n",
    "    print(\"Directory already exists:\")\n",
    "\n",
    "print(FOLDER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb237b5f-264e-44cc-9f6d-190b611b807e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Utils for checkpointing data\n",
    "def checkpoint_data(df, name, folder_path=GROUP_FOLDER_PATH):\n",
    "    df.write.parquet(f\"{folder_path}/{name}.parquet\")\n",
    "\n",
    "def load_checkpointed_data(name, folder_path=GROUP_FOLDER_PATH):\n",
    "    return spark.read.parquet(f\"{folder_path}/{name}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1bf69db-de56-47ea-8aab-138f57c98ef1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load or create data\n",
    "try:\n",
    "    train_otpw = load_checkpointed_data(\"OTPW_3M_TRAIN\")\n",
    "    test_otpw = load_checkpointed_data(\"OTPW_3M_TEST\")\n",
    "except:\n",
    "    # Load OTPW data\n",
    "    df_otpw = spark.read.format(\"csv\").option(\"header\",\"true\").load(f\"{DATA_BASE_DIR}/OTPW_3M_2015.csv\")\n",
    "\n",
    "    # Filter \n",
    "    df_otpw = df_otpw.filter(col(\"DEP_DELAY\").isNotNull())\n",
    "\n",
    "    # Get non null column names\n",
    "    def get_non_null_cols(df, threshold=10):\n",
    "        \"\"\"\n",
    "        Calculate and return column names with null percentages at or below the threshold.\n",
    "\n",
    "        Args:\n",
    "            df (DataFrame): Spark DataFrame to analyze.\n",
    "            threshold (float, optional): Maximum percentage of nulls to include a column in the output. Defaults to 10%.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: List of column names with null percentage <= threshold, sorted by percentage descending.\n",
    "        \"\"\"\n",
    "        total_count = df.count()\n",
    "        null_counts = df.select([\n",
    "            (F.sum(F.col(c).isNull().cast(\"int\")) / total_count * 100).alias(c) \n",
    "            for c in df.columns\n",
    "        ])\n",
    "        null_dict = null_counts.collect()[0].asDict()\n",
    "        low_null_cols = {col: pct for col, pct in null_dict.items() if pct <= threshold}\n",
    "        sorted_cols = sorted(low_null_cols.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [col for col, _ in sorted_cols]\n",
    "\n",
    "    non_null_cols = get_non_null_cols(train_otpw, threshold=10)\n",
    "\n",
    "    # only non-null columns\n",
    "    train_otpw = train_otpw.select(non_null_cols)\n",
    "    test_otpw = train_otpw.select(non_null_cols)\n",
    "\n",
    "    # checkpoint data\n",
    "    checkpoint_data(train_otpw, \"OTPW_3M_TRAIN\")\n",
    "    checkpoint_data(test_otpw, \"OTPW_3M_TEST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43716861-25f4-4715-acde-a0f77fae4fc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(train_otpw.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db92bd0d-bf90-4da0-9a69-84edd808a03a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_expanding_window_folds(\n",
    "    df, \n",
    "    start_date: str | None = None, \n",
    "    end_date: str | None = None, \n",
    "    date_col: str = \"FL_DATE\",\n",
    "    n_folds: int = 4\n",
    "):\n",
    "    \"\"\"\n",
    "    Create expanding window time-series cross-validation folds\n",
    "    Automatically divides the time range into equal periods\n",
    "    \n",
    "    Args:\n",
    "        df: PySpark DataFrame\n",
    "        start_date: Starting date as string 'YYYY-MM-DD' or None for auto-detection\n",
    "        end_date: Ending date as string 'YYYY-MM-DD' or None for auto-detection\n",
    "        date_col: Name of the date column\n",
    "        n_folds: Number of folds to create\n",
    "    \n",
    "    Returns: List of tuples (train_df, val_df) for each fold\n",
    "    \"\"\"\n",
    "    # Auto-detect dates if not provided\n",
    "    if start_date is None or end_date is None:\n",
    "        date_range = df.select(\n",
    "            F.min(F.col(date_col)).alias(\"start_date\"),\n",
    "            F.max(F.col(date_col)).alias(\"end_date\")\n",
    "        ).first()\n",
    "        \n",
    "        start_date = start_date or str(date_range[\"start_date\"])\n",
    "        end_date = end_date or str(date_range[\"end_date\"])\n",
    "    \n",
    "    # Convert to datetime\n",
    "    start_dt = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_dt = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "    \n",
    "    # Calculate total days and period size\n",
    "    total_days = (end_dt - start_dt).days\n",
    "    period_days = total_days // (n_folds + 1)  # +1 because first period is just training\n",
    "    \n",
    "    if period_days == 0:\n",
    "        raise ValueError(f\"Dataset too small for {n_folds} folds. Total days: {total_days}\")\n",
    "    \n",
    "    # Print header\n",
    "    print(f\"Detected date range: {start_date} to {end_date}\")\n",
    "    print(f\"Total days: {total_days}, Period size: {period_days} days (~{period_days/7:.1f} weeks)\")\n",
    "    print()\n",
    "    \n",
    "    # Create visual timeline\n",
    "    _print_timeline(start_dt, end_dt, period_days, n_folds)\n",
    "    print()\n",
    "    \n",
    "    # Create folds\n",
    "    folds = []\n",
    "    \n",
    "    for fold_num in range(n_folds):\n",
    "        # Calculate validation period\n",
    "        val_start_days = period_days * (fold_num + 1)\n",
    "        val_end_days = period_days * (fold_num + 2)\n",
    "        \n",
    "        val_start = start_dt + timedelta(days=val_start_days)\n",
    "        val_end = start_dt + timedelta(days=val_end_days)\n",
    "        \n",
    "        # Make sure we don't exceed end date\n",
    "        if val_end > end_dt:\n",
    "            val_end = end_dt\n",
    "        \n",
    "        # Training: all data from start to val_start\n",
    "        train_df = df.filter(\n",
    "            (F.col(date_col) >= F.lit(start_date)) & \n",
    "            (F.col(date_col) < F.lit(val_start.strftime('%Y-%m-%d')))\n",
    "        )\n",
    "        \n",
    "        # Validation: data in the validation window\n",
    "        val_df = df.filter(\n",
    "            (F.col(date_col) >= F.lit(val_start.strftime('%Y-%m-%d'))) & \n",
    "            (F.col(date_col) < F.lit(val_end.strftime('%Y-%m-%d')))\n",
    "        )\n",
    "        \n",
    "        folds.append((train_df, val_df))\n",
    "        \n",
    "        train_days = (val_start - start_dt).days\n",
    "        val_days = (val_end - val_start).days\n",
    "        \n",
    "        print(f\"Fold {fold_num + 1}: Train [{start_date} to {val_start.strftime('%Y-%m-%d')}), \"\n",
    "              f\"Val [{val_start.strftime('%Y-%m-%d')} to {val_end.strftime('%Y-%m-%d')})\")\n",
    "        print(f\"  Train: {train_days} days, Val: {val_days} days\")\n",
    "        print(f\"  Train size: {train_df.count()}, Val size: {val_df.count()}\")\n",
    "        print()\n",
    "    \n",
    "    return folds\n",
    "\n",
    "\n",
    "def _print_timeline(start_dt, end_dt, period_days, n_folds):\n",
    "    \"\"\"Print visual timeline of the cross-validation strategy\"\"\"\n",
    "    \n",
    "    # Create period labels\n",
    "    periods = []\n",
    "    current = start_dt\n",
    "    for i in range(n_folds + 1):\n",
    "        period_end = current + timedelta(days=period_days)\n",
    "        if period_end > end_dt:\n",
    "            period_end = end_dt\n",
    "        periods.append({\n",
    "            'start': current,\n",
    "            'end': period_end,\n",
    "            'label': current.strftime('%Y-%m-%d')\n",
    "        })\n",
    "        current = period_end\n",
    "    \n",
    "    # Fixed width for each period segment\n",
    "    segment_width = 17\n",
    "    \n",
    "    # Print timeline header\n",
    "    print(\"Dataset Timeline: \", end=\"\")\n",
    "    for i, period in enumerate(periods):\n",
    "        if i == 0:\n",
    "            print(f\"{period['label']}\", end=\"\")\n",
    "        else:\n",
    "            # Calculate separator to maintain fixed width\n",
    "            separator_len = segment_width - len(period['label'])\n",
    "            print(f\" {'─' * separator_len} {period['label']}\", end=\"\")\n",
    "    print()\n",
    "    \n",
    "    # Print timeline bars\n",
    "    print(\" \" * 18, end=\"\")\n",
    "    for i in range(len(periods)):\n",
    "        print(f\"|{'═' * (segment_width - 1)}\", end=\"\")\n",
    "    print(\"|\")\n",
    "    print()\n",
    "    \n",
    "    # Print each fold\n",
    "    for fold_num in range(n_folds):\n",
    "        train_periods = fold_num + 1\n",
    "        val_period = fold_num + 1\n",
    "        \n",
    "        # Calculate positions with fixed width\n",
    "        fold_label = f\"Fold {fold_num + 1}:\"\n",
    "        fold_indent = 18  # Fixed indent for all fold labels\n",
    "        \n",
    "        # Build train label\n",
    "        if train_periods == 1:\n",
    "            train_label = f\"[Train: {periods[0]['label']}]\"\n",
    "        else:\n",
    "            train_label = f\"[Train: {periods[0]['label']}─────{periods[train_periods-1]['label']}]\"\n",
    "        \n",
    "        # Build val label\n",
    "        val_label = f\"[Val: {periods[val_period]['label']}]\"\n",
    "        \n",
    "        # Calculate spacing to align val label with its period\n",
    "        # Position of val period start\n",
    "        val_position = fold_indent + (segment_width * val_period)\n",
    "        \n",
    "        # Current position after fold label and train label\n",
    "        current_pos = len(fold_label) + len(train_label) + fold_indent\n",
    "        \n",
    "        # Calculate spacing needed\n",
    "        spacing = val_position - current_pos\n",
    "        \n",
    "        # Print the fold description line\n",
    "        print(f\"{fold_label:<18}{train_label}\", end=\"\")\n",
    "        print(\" \" * max(0, spacing), end=\"\")\n",
    "        print(val_label)\n",
    "        \n",
    "        # Print visual representation\n",
    "        print(\" \" * fold_indent, end=\"\")\n",
    "        \n",
    "        # Train arrow\n",
    "        train_width = segment_width * train_periods\n",
    "        print(\"|\", end=\"\")\n",
    "        print(\"═\" * (train_width - 1), end=\"\")\n",
    "        print(\">\", end=\"\")\n",
    "        \n",
    "        # Val bar\n",
    "        print(\"|\", end=\"\")\n",
    "        val_width = segment_width - 2\n",
    "        print(\"─\" * val_width, end=\"\")\n",
    "        print(\"|\")\n",
    "        print()\n",
    "    \n",
    "    print(\"Legend:  [═══] = Training Data    [───] = Validation Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e16975a-9793-4f73-b82b-64ee74d369f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "folds = create_expanding_window_folds(train_otpw, n_folds=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5a5bacf-2878-4874-8708-586879cee51a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a175093-06ef-43e5-a8fc-703729850790",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def sanity_check_folds(folds, date_col=\"FL_DATE\"):\n",
    "    \"\"\"\n",
    "    Perform sanity checks on cross-validation folds\n",
    "    \n",
    "    Args:\n",
    "        folds: List of tuples (train_df, val_df)\n",
    "        date_col: Name of the date column\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"SANITY CHECKS FOR FOLDS\")\n",
    "    print(\"=\"*70)\n",
    "    print()\n",
    "    \n",
    "    all_checks_passed = True\n",
    "    \n",
    "    for fold_num, (train_df, val_df) in enumerate(folds, 1):\n",
    "        print(f\"Fold {fold_num}:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Get date ranges\n",
    "        train_stats = train_df.select(\n",
    "            F.min(F.col(date_col)).alias(\"train_min\"),\n",
    "            F.max(F.col(date_col)).alias(\"train_max\"),\n",
    "            F.count(\"*\").alias(\"train_count\")\n",
    "        ).first()\n",
    "        \n",
    "        val_stats = val_df.select(\n",
    "            F.min(F.col(date_col)).alias(\"val_min\"),\n",
    "            F.max(F.col(date_col)).alias(\"val_max\"),\n",
    "            F.count(\"*\").alias(\"val_count\")\n",
    "        ).first()\n",
    "        \n",
    "        train_min = str(train_stats[\"train_min\"])\n",
    "        train_max = str(train_stats[\"train_max\"])\n",
    "        val_min = str(val_stats[\"val_min\"])\n",
    "        val_max = str(val_stats[\"val_max\"])\n",
    "        train_count = train_stats[\"train_count\"]\n",
    "        val_count = val_stats[\"val_count\"]\n",
    "        \n",
    "        print(f\"  Train date range: {train_min} to {train_max} ({train_count:,} rows)\")\n",
    "        print(f\"  Val date range:   {val_min} to {val_max} ({val_count:,} rows)\")\n",
    "        \n",
    "        # Check 1: Train max < Val min (no temporal overlap)\n",
    "        if train_max >= val_min:\n",
    "            print(f\"  ❌ FAIL: Train max ({train_max}) >= Val min ({val_min}) - TEMPORAL OVERLAP!\")\n",
    "            all_checks_passed = False\n",
    "        else:\n",
    "            print(f\"  ✓ PASS: No temporal overlap (train ends before val starts)\")\n",
    "        \n",
    "        # Check 2: Val comes immediately after train (no gap)\n",
    "        train_max_dt = datetime.strptime(train_max, '%Y-%m-%d')\n",
    "        val_min_dt = datetime.strptime(val_min, '%Y-%m-%d')\n",
    "        gap_days = (val_min_dt - train_max_dt).days\n",
    "        \n",
    "        if gap_days == 1:\n",
    "            print(f\"  ✓ PASS: Val starts immediately after train (1 day gap as expected)\")\n",
    "        elif gap_days > 1:\n",
    "            print(f\"  ⚠️  WARNING: {gap_days} day gap between train and val\")\n",
    "        else:\n",
    "            print(f\"  ❌ FAIL: Val starts before train ends (gap = {gap_days} days)\")\n",
    "            all_checks_passed = False\n",
    "        \n",
    "        # Check 3: No empty datasets\n",
    "        if train_count == 0:\n",
    "            print(f\"  ❌ FAIL: Train set is empty!\")\n",
    "            all_checks_passed = False\n",
    "        else:\n",
    "            print(f\"  ✓ PASS: Train set has {train_count:,} rows\")\n",
    "        \n",
    "        if val_count == 0:\n",
    "            print(f\"  ❌ FAIL: Validation set is empty!\")\n",
    "            all_checks_passed = False\n",
    "        else:\n",
    "            print(f\"  ✓ PASS: Validation set has {val_count:,} rows\")\n",
    "        \n",
    "        # Check 4: Check for data leakage (any val dates in train)\n",
    "        leakage_count = train_df.join(\n",
    "            val_df.select(F.col(date_col).alias(\"val_date\")),\n",
    "            F.col(date_col) == F.col(\"val_date\"),\n",
    "            \"inner\"\n",
    "        ).count()\n",
    "        \n",
    "        if leakage_count > 0:\n",
    "            print(f\"  ❌ FAIL: {leakage_count} rows with same dates in both train and val - DATA LEAKAGE!\")\n",
    "            all_checks_passed = False\n",
    "        else:\n",
    "            print(f\"  ✓ PASS: No data leakage (no overlapping dates)\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # Cross-fold checks\n",
    "    print(\"=\"*70)\n",
    "    print(\"CROSS-FOLD CHECKS\")\n",
    "    print(\"=\"*70)\n",
    "    print()\n",
    "    \n",
    "    # Check that training sets are expanding\n",
    "    for i in range(1, len(folds)):\n",
    "        train_prev = folds[i-1][0]\n",
    "        train_curr = folds[i][0]\n",
    "        \n",
    "        prev_count = train_prev.count()\n",
    "        curr_count = train_curr.count()\n",
    "        \n",
    "        print(f\"Fold {i} → Fold {i+1}: Train size {prev_count:,} → {curr_count:,}\", end=\"\")\n",
    "        \n",
    "        if curr_count > prev_count:\n",
    "            print(\" ✓ (expanding)\")\n",
    "        elif curr_count == prev_count:\n",
    "            print(\" ⚠️  WARNING: Train set not growing\")\n",
    "        else:\n",
    "            print(\" ❌ FAIL: Train set shrinking!\")\n",
    "            all_checks_passed = False\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Check validation set sizes are similar\n",
    "    val_sizes = [val_df.count() for _, val_df in folds]\n",
    "    avg_val_size = sum(val_sizes) / len(val_sizes)\n",
    "    max_deviation = max(abs(size - avg_val_size) / avg_val_size for size in val_sizes)\n",
    "    \n",
    "    print(f\"Validation set sizes: {val_sizes}\")\n",
    "    print(f\"Average: {avg_val_size:,.0f}, Max deviation: {max_deviation*100:.1f}%\", end=\"\")\n",
    "    \n",
    "    if max_deviation < 0.1:\n",
    "        print(\" ✓ (consistent sizes)\")\n",
    "    else:\n",
    "        print(\" ⚠️  WARNING: Validation sets have inconsistent sizes\")\n",
    "    \n",
    "    print()\n",
    "    print(\"=\"*70)\n",
    "    if all_checks_passed:\n",
    "        print(\"✓ ALL CHECKS PASSED!\")\n",
    "    else:\n",
    "        print(\"❌ SOME CHECKS FAILED - REVIEW FOLD CREATION!\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "\n",
    "# Run sanity checks\n",
    "sanity_check_folds(folds, date_col=\"FL_DATE\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Phase 2 - Temporal CV",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
