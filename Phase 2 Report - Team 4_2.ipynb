{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d705b59-fb45-465d-a020-01df3daf3304",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Final Project Phase 2: Team 4_2\n",
    "==============================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a88fe336-3155-4729-9e5d-7f17b7e79df8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Flight Delay Prediction with Weather-Aware ML Models: Phase 2\n",
    "**Group #:**  4_2\n",
    "## Team and Project Meta Information\n",
    "\n",
    "### Phase Leader\n",
    "- **Name:**  Emily Lieske\n",
    "- **Role:**  Phase Leader\n",
    "- **Email:**  emily-lieske@berkeley.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7a7f13a-7888-4751-ab38-ee9ab63b63fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Team Members\n",
    "<table>\n",
    "  <tr>\n",
    "    <td align=\"center\" style=\"padding:12px;\">\n",
    "      <img src=\"https://github.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/blob/main/report-images/emily_lieske.jpeg?raw=true\" width=\"110\" style=\"border-radius:12px; object-fit:cover;\">\n",
    "      <div style=\"font-weight:600; margin-top:8px;\">Emily Lieske</div>\n",
    "      <a href=\"mailto:emily-lieske@berkeley.edu\">emily-lieske@berkeley.edu</a>\n",
    "    </td>\n",
    "    <td align=\"center\" style=\"padding:12px;\">\n",
    "      <img src=\"https://github.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/blob/main/report-images/siddharth_manu.jpg?raw=true\" width=\"110\" style=\"border-radius:12px; object-fit:cover;\">\n",
    "      <div style=\"font-weight:600; margin-top:8px;\">Siddharth Manu</div>\n",
    "      <a href=\"mailto:siddharthmanu@berkeley.edu\">siddharthmanu@berkeley.edu</a>\n",
    "    </td>\n",
    "      </td>\n",
    "    <td align=\"center\" style=\"padding:12px;\">\n",
    "      <img src=\"https://github.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/blob/main/report-images/indri_Adisoemarta.jpg?raw=true\" width=\"110\" style=\"border-radius:12px; object-fit:cover;\">\n",
    "      <div style=\"font-weight:600; margin-top:8px;\">Indri Adisoemarta</div>\n",
    "      <a href=\"mailto:indri.a@berkeley.edu\">indri.a@berkeley.edu</a>\n",
    "    </td>\n",
    "        </td>\n",
    "    <td align=\"center\" style=\"padding:12px;\">\n",
    "      <img src=\"https://i.imgur.com/onHwe6w.jpeg\" width=\"110\" style=\"border-radius:12px; object-fit:cover;\">\n",
    "      <div style=\"font-weight:600; margin-top:8px;\">Paul Lin</div>\n",
    "      <a href=\"mailto:paul.lin@berkeley.edu\">paul.lin@berkeley.edu</a>\n",
    "    </td>\n",
    "        </td>\n",
    "    <td align=\"center\" style=\"padding:12px;\">\n",
    "      <img src=\"https://github.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/blob/main/report-images/conner-watson.jpg?raw=true\" width=\"110\" style=\"border-radius:12px; object-fit:cover;\">\n",
    "      <div style=\"font-weight:600; margin-top:8px;\">Connor Watson</div>\n",
    "      <a href=\"mailto:connorwatson@berkeley.edu\">connorwatson@berkeley.edu</a>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3388e16a-51c6-435d-9736-84a5a21ee01b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Credit Assignment Plan for Phase 2\n",
    "\n",
    "| Team Member | Tasks Completed | Start Date | End Date | Hours Spent | % Contribution |\n",
    "|--------------|----------------|-------------|-----------|--------------|----------------|\n",
    "| **Emily Lieske** | Linear regression baseline and experiments, slide preparation and presentation, writing of Modeling, Results and Discussion sections, Phase Leader, Conclusion | 11/10 | 11/22| 50| 20|\n",
    "| **Indri Adisoemarta** | Exploratory data analysis of 3 month/1 year/5 year data, joining 3 month/1 year/5 year raw data tables, data dictionary, feature identification, null/missing values strategy, validating custom join data, visualizations for slides | 11/10 | 11/23 | 50 | 20|\n",
    "| **Siddharth Manu** | Custom join strategy, temporal & spatial joins for 3m data, Spark optimization to do bucketized joins for full dataset, filtering & saving processed data, checkpointing | 11/10 | 11/22| 50| 20|\n",
    "| **Connor Watson** | Homework 5, Exploring External Datasources (FAA, Gov Shutdown, etc), slide preparation and presentation | 11/10 | 11/22 | 50 |20 |\n",
    "| **Paul Lin** | Implement time series splitting (expanding window and rolling window), cross validation pipeline , lasso regression and feature selection, random forest experimentation | 11/10| 11/22|50 | 20|\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f87514b-011f-4bea-8e4a-b2a8a45a3c27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Abstract\n",
    "\n",
    "Our project addresses the critical business problem of predicting domestic flight departure delays to enable proactive airline operations and improve passenger experience. We leverage comprehensive\n",
    "data sources spanning 2015-2021: DOT Bureau of Transportation Statistics On-Time Performance records (70M+ flights), NOAA Integrated Surface Database weather observations, and airport metadata. Phase\n",
    "1 exploratory data analysis revealed that while most flights operate on-time (80% within 15 minutes of schedule), delays exhibit high variability (mean 10 minutes, σ=38) with notable temporal\n",
    "patterns including elevated delays on Mondays and Sundays, and seasonal peaks during summer travel periods.\n",
    "\n",
    "For Phase 2, we established a baseline linear regression model using a robust 6-stage feature engineering pipeline encompassing data preparation, median imputation, categorical encoding, feature\n",
    "assembly, standardized scaling, and model training. We selected linear regression as our baseline for its interpretability, computational efficiency on distributed systems, and ability to establish\n",
    "clear performance benchmarks for subsequent model comparisons. Using expanding window cross-validation across five temporal folds, our baseline model achieved a test set RMSE of 35.96 minutes and MAE\n",
    "of 18.29 minutes on the held-out test set, with an R² near zero indicating substantial room for improvement. The most important features driving predictions include temporal attributes (day of week,\n",
    "month, scheduled departure time), weather conditions at origin and destination airports (visibility, wind speed, precipitation), flight-specific characteristics (distance, carrier), and airport\n",
    "operational indicators (congestion proxies derived from scheduled traffic volume).\n",
    "\n",
    "Moving forward, we plan to significantly enhance model performance through three parallel tracks: (1) advanced feature engineering incorporating temporal dynamics (Prophet time-series components,\n",
    "rolling window delay aggregations over 2hr/6hr/24hr windows with backward-looking constraints), flight network graph features (PageRank, degree centrality, delay propagation patterns), and external event data (government shutdown indicators, FAA OPSNET operational metrics including ATC delays and ground stops); (2) non-linear modeling approaches including XGBoost for capturing feature interactions, deep neural networks (MLP architecture) for complex pattern recognition, and ensemble methods (weighted averaging and stacking) to combine predictions from multiple models; and (3) rigorous data leakage controls with phased evaluation to ensure our offline metrics reliably predict production performance. This systematic approach balances model sophistication with operational feasibility, targeting substantial RMSE reduction while maintaining interpretability for airline decision-makers.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb65d719-433c-42fd-ac74-028ffd00ce50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Project Description (Data and Tasks)\n",
    "\n",
    "### Data Description\n",
    "\n",
    "Our analysis leverages three primary data sources integrated through Spark-based distributed processing:\n",
    "\n",
    "**DOT Bureau of Transportation Statistics (TranStats)** serves as our core dataset, providing comprehensive flight operations records from 2015-2021 in CSV format. Key features include temporal attributes (flight date, scheduled/actual departure times, day of week, month), flight identifiers (carrier, flight number, tail number), route information (origin/destination airports, distance), and outcome variables (departure delay in minutes, cancellation indicators). The dataset encompasses over 70 million domestic flight records across 346 airports and 18 carriers.\n",
    "\n",
    "**NOAA Integrated Surface Database (ISD)** contributes meteorological observations matched to airport locations and flight timestamps. Weather features include visibility (miles), wind speed (knots), precipitation indicators (rain, snow, fog), temperature (°F), and atmospheric pressure. Data is provided in CSV format with hourly granularity, requiring temporal alignment to flight schedules.\n",
    "\n",
    "**Airport Metadata** supplements the primary datasets with geographic coordinates (latitude/longitude) and operational classifications (hub status, airport size categories), enabling spatial analysis and network feature construction.\n",
    "\n",
    "Our feature engineering pipeline processes these raw data sources into a standardized feature set of 50+ attributes. The temporal dataset structure maintains chronological ordering with expanding window splits: training sets grow incrementally from months 1-8 through months 1-12, with each subsequent month serving as the validation fold. The final held-out test set consists of data from a separate time period to evaluate generalization performance. For detailed exploratory data analysis including distribution statistics, correlation matrices, and temporal visualizations, please refer to **Section 4: Exploratory Data Analysis**.\n",
    "\n",
    "**Table 1: Representative Data Dictionary (Selected Features)**\n",
    "\n",
    "| Feature Name | Description | Data Type | Example Values |\n",
    "|--------------|-------------|-----------|----------------|\n",
    "| `FL_DATE` | Flight date | Date | 2018-06-15 |\n",
    "| `OP_CARRIER` | Operating airline carrier code | Categorical | AA, DL, UA, WN |\n",
    "| `ORIGIN` | Origin airport code | Categorical | ATL, ORD, DFW |\n",
    "| `DEST` | Destination airport code | Categorical | LAX, JFK, SFO |\n",
    "| `CRS_DEP_TIME` | Scheduled departure time (local) | Integer | 1430 (2:30 PM) |\n",
    "| `DEP_DELAY` | Departure delay in minutes (**target**) | Continuous | -5, 0, 23, 147 |\n",
    "| `DISTANCE` | Flight distance in miles | Continuous | 337, 1846, 2475 |\n",
    "| `DAY_OF_WEEK` | Day of week | Integer | 1 (Mon) - 7 (Sun) |\n",
    "| `MONTH` | Month of year | Integer | 1 (Jan) - 12 (Dec) |\n",
    "| `origin_visibility` | Visibility at origin (miles) | Continuous | 10.0, 5.0, 0.5 |\n",
    "| `origin_wind_speed` | Wind speed at origin (knots) | Continuous | 8.0, 15.0, 25.0 |\n",
    "| `dest_temperature` | Temperature at destination (°F) | Continuous | 72.0, 45.0, 95.0 |\n",
    "\n",
    "*Note: This table shows key features only. Full data dictionary with 50+ features available in project repository.*\n",
    "\n",
    "### Task Description\n",
    "\n",
    "**Prediction Task:** We formulate flight delay prediction as a supervised regression problem, where the objective is to predict continuous departure delay time in minutes (`DEP_DELAY`) for a given flight two hours before its scheduled departure. Delays are measured as the difference between actual and scheduled departure times, with negative values indicating early departures, zero indicating on-time performance, and positive values indicating delays.\n",
    "\n",
    "**Business Relevance:** Accurate delay predictions enable multiple operational improvements across the aviation ecosystem. Airlines can optimize crew scheduling, gate assignments, and aircraft turnaround operations when delays are anticipated. Passengers benefit from proactive rebooking options and realistic departure estimates, reducing uncertainty and airport congestion. Air traffic control can better allocate resources and manage runway capacity during periods of expected disruption. The 2-hour prediction window balances actionability (sufficient time for operational adjustments) with forecast reliability (limiting exposure to unpredictable last-minute events).\n",
    "\n",
    "**Modeling Approach:** Our workflow follows a systematic pipeline architecture illustrated in Figure 1 below. The process begins with temporal data partitioning into expanding window folds, followed by a 6-stage feature engineering pipeline: (1) data preparation and cleaning, (2) median imputation for missing values, (3) categorical encoding (one-hot encoding for carriers/airports, ordinal encoding for temporal features), (4) feature assembly into unified vector representations, (5) standardized scaling for numeric features, and (6) model training using linear regression with L2 regularization. Cross-validation employs an expanding window strategy respecting temporal ordering, where training sets grow incrementally (months 1-8, 1-9, 1-10, 1-11, 1-12) and each subsequent month serves as the validation fold. Final model evaluation occurs on a held-out test set from a separate time period. Performance is assessed using regression metrics (RMSE, MAE, R²) and domain-specific operational metrics (On-Time Prediction Accuracy, Severe Delay Detection Rate). This baseline establishes a foundation for subsequent experiments with advanced feature engineering (temporal dynamics, network graph features, external events) and non-linear models (XGBoost, deep neural networks, ensemble methods)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6347370-7926-48c7-bbbb-10dadc06f0d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "### Dataset Summary\n",
    "\n",
    "The following is a table summary of the raw data tables used as our data sources. Some of the data sources have duplicated data, such as the flights data, so both the duplicated and deduplicated row counts have been included for completeness.\n",
    "\n",
    "| Name | Source | Format | Months | Date Range | Rows | Columns |  Parquet Size (GB) | Description |\n",
    "|------|--------|--------|--------|------------|------|---------|--------------------|-------------|\n",
    "| Flights | U.S. DOT TranStats – On-Time Performance | Parquet | 3 | 2015-01-01 - 2015-03-31 | 2,806,942 | 109 | 0.09 | records of U.S. passenger flights, with details such as departure and arrival times, delays |\n",
    "| Flights | U.S. DOT TranStats – On-Time Performance | Parquet | 12 | 2015-01-01 - 2015-12-31 | 11,638,158 | 108 | 0.42 | records of U.S. passenger flights, with details such as departure and arrival times, delays |\n",
    "| Flights | U.S. DOT TranStats – On-Time Performance | Parquet | 60 | 2015-01-01 - 2019-12-31 | 63,493,682 | 108 | 2.41 | records of U.S. passenger flights, with details such as departure and arrival times, delays |\n",
    "| Deduplicated Flights | U.S. DOT TranStats – On-Time Performance | Parquet | 3 | 2015-01-01 - 2015-03-31 | 1,403,471 | 109 | 0.045 | records of U.S. passenger flights, with details such as departure and arrival times, delays |\n",
    "| Deduplicated Flights | U.S. DOT TranStats – On-Time Performance | Parquet | 12 | 2015-01-01 - 2015-12-31 | 5,819,079 | 108 | 0.21 | records of U.S. passenger flights, with details such as departure and arrival times, delays |\n",
    "| Deduplicated Flights | U.S. DOT TranStats – On-Time Performance | Parquet | 60 | 2015-01-01 - 2019-12-31 | 31,746,841 | 108 | 1.205 | records of U.S. passenger flights, with details such as departure and arrival times, delays |\n",
    "| Weather | NOAA Global Hourly Weather Repository | Parquet | 3 | 2015-01-01 - 2015-03-31 | 30,528,602 | 124 | 1.09 | contains temperature, wind, visibility, and precipitation data from global weather stations |\n",
    "| Weather | NOAA Global Hourly Weather Repository | Parquet | 12 | 2015-01-01 - 2015-12-31 | 123,856,083 | 123 | 4.66 | contains temperature, wind, visibility, and precipitation data from global weather stations |\n",
    "| Weather | NOAA Global Hourly Weather Repository | Parquet | 60 | 2015-01-01 - 2019-12-31 | 639,726,637 | 123 | 23.35 | contains temperature, wind, visibility, and precipitation data from global weather stations |\n",
    "| Stations with neighbors | U.S. Department of Transportation | Parquet |  |  | 5,004,169 | 12 | 0.05 | airport-level metadata to enhance the flight dataset with geographic and administrative context |\n",
    "| Airport Codes | DataHub – Global Airport Codes | CSV |  |  | 20,822 | 6 | 0.0005 | airport-code reference table that contains both IATA (3-letter) and ICAO (4-letter) codes |\n",
    "| Airport Timezones | Open Travel Data | CSV |  |  | 82,808 | 13 | 0.004 | used to unify time zones |\n",
    "\n",
    "##### 1. Flights Data\n",
    "\n",
    "We plan to use the U.S. Department of Transportation (DOT) TranStats On-Time Performance (OTP) dataset, which contains detailed records of U.S. passenger flights from 2015 to 2021.\n",
    "The dataset includes operational details such as departure and arrival times, carrier information, delays, and airport identifiers.\n",
    "\n",
    "Source: U.S. DOT TranStats – On-Time Performance\n",
    "\n",
    "Deduplicated Dimensions (2015–2019): 31,746,841 rows × 109 columns\n",
    "\n",
    "##### 2. Weather Data\n",
    "\n",
    "Weather conditions are a critical factor influencing flight delays.\n",
    "We plan to use hourly weather observations from the National Oceanic and Atmospheric Administration (NOAA), covering January 2015 – December 2021.\n",
    "This dataset contains temperature, wind, visibility, and precipitation data from global weather stations.\n",
    "\n",
    "Source: NOAA Global Hourly Weather Repository\n",
    "\n",
    "Deduplicated Dimensions (2015–2019): 630,904,436 rows × 177 columns\n",
    "\n",
    "##### 3. Airport Metadata\n",
    "\n",
    "We plan to incorporate airport-level metadata to enhance the flight dataset with geographic and administrative context. This table is joined with its nearest neighbors to create the \"Stations with neighbors\" table described in the table above.\n",
    "\n",
    "Source: U.S. Department of Transportation\n",
    "\n",
    "Dimensions: 18,097 rows × 10 columns\n",
    "\n",
    "##### 4. Airport Codes\n",
    "\n",
    "To link airports between datasets, we plan to use an external airport-code reference table containing both IATA (3-letter) and ICAO (4-letter) codes.\n",
    "This mapping will allow us to join flight records (which use IATA codes) with weather and airport datasets (which may use ICAO codes).\n",
    "\n",
    "Source: DataHub – Global Airport Codes\n",
    "\n",
    "Purpose: IATA/ICAO code harmonization for join operations\n",
    "\n",
    "##### 5. Timezones\n",
    "\n",
    "Used to link airport codes to appropriate time zones.\n",
    "\n",
    "Source: Open Travel Data (https://github.com/opentraveldata/opentraveldata/)\n",
    "\n",
    "### Data Dictionary, Samples, and Summary Statistics\n",
    "\n",
    "Our modeling and EDA will be done with 3 sets of samples of data: \n",
    "\n",
    "* **3-month** data from 2015-01-01 to 2015-03-31, using both the provided OTPW data and our custom join data\n",
    "* **12-month** data from 2015-01-01 to 2015-12-31, using both the provided OTPW data and our custom join data\n",
    "* **60-month** data from 2015-01-01 to 2019-12-31, using both the provided OTPW data and our custom join data\n",
    "\n",
    "**Training, Test, and Validation**\n",
    "\n",
    "The division of our samples of data into training, test and validation is described in the **Cross Validation** section below. \n",
    "\n",
    "We divided our features of interest into the following feature families:\n",
    "\n",
    "| Feature Family | Description | Columns |\n",
    "|----------------|-------------|---------|\n",
    "| Target Delay Variables | These variables constitute our outcome variable - the amount of time for a delay. Arrival time is included in this category as it occurs after departure. | WEATHER_DELAY, LATE_AIRCRAFT_DELAY, NAS_DELAY, SECURITY_DELAY, CARRIER_DELAY, ARR_DELAY, ARR_DELAY_NEW, ARR_DEL15, ARR_DELAY_GROUP, ARR_TIME, DEP_DELAY, DEP_DELAY_NEW, DEP_DEL15, DEP_DELAY_GROUP, CANCELLED, DIVERTED |\n",
    "| Flight Features | These features are related to the aircraft for each flight. | OP_CARRIER, OP_UNIQUE_CARRIER, OP_CARRIER_AIRLINE_ID, OP_CARRIER_FL_NUM, TAIL_NUM |\n",
    "| Temporal Features | These features are all related to timing of flights - there are actual timestamps, like DEP_TIME, and scheduled time stamps, like CRS_DEP_TIME. Some are derived timestamps, like sched_depart_date_time. | FL_DATE, YEAR, QUARTER, MONTH, DAY_OF_MONTH, DAY_OF_WEEK, CRS_DEP_TIME, CRS_ARR_TIME, DEP_TIME_BLK, ARR_TIME_BLK, CRS_ELAPSED_TIME, sched_depart_date_time, sched_depart_date_time_UTC, two_hours_prior_depart_UTC, four_hours_prior_depart_UTC, DATE |\n",
    "| Weather Features | These features are all related to weather, like temperature, pressure, and wind speed. | HourlyDryBulbTemperature, HourlyWetBulbTemperature, HourlyDewPointTemperature, HourlyStationPressure, HourlySeaLevelPressure, HourlyAltimeterSetting, HourlyPressureChange, HourlyRelativeHumidity, HourlyWindSpeed, HourlyWindDirection, HourlyVisibility, HourlyPrecipitation, HourlySkyConditions, REPORT_TYPE, SOURCE, REM|\n",
    "| Geographic Features | These features are related to locations of airports and weather stations. | ORIGIN, ORIGIN_AIRPORT_ID, ORIGIN_AIRPORT_SEQ_ID, origin_iata_code, origin_icao, ORIGIN_CITY_NAME, ORIGIN_CITY_MARKET_ID, ORIGIN_STATE_NM, ORIGIN_STATE_ABR, ORIGIN_STATE_FIPS, ORIGIN_WAC, origin_region, origin_airport_lat, origin_airport_lon, origin_type, origin_airport_name, DEST, DEST_AIRPORT_ID, DEST_AIRPORT_SEQ_ID, dest_iata_code, dest_icao, DEST_CITY_NAME, DEST_CITY_MARKET_ID, DEST_STATE_NM, DEST_STATE_ABR, DEST_STATE_FIPS, DEST_WAC, dest_region, dest_type, dest_airport_name, ELEVATION |\n",
    "\n",
    "You can see the detailed table with the column descriptions and null percentages in the appendix. We selected 88 columns from the original 216 columns based on three key criteria: data completeness, predictive relevance, and operational availability.\n",
    "\n",
    "##### Data Completeness\n",
    "\n",
    "We excluded 106 columns with excessive missing data (>50% null values):\n",
    "\n",
    "- **Monthly aggregates** (100% null): All 34 `Monthly*` columns are entirely empty, providing zero predictive value\n",
    "- **Daily summaries** (~99.8% null): The 17 `Daily*` features are nearly complete missing, making them unreliable\n",
    "- **Infrequent weather phenomena** (100% null): Short-duration precipitation metrics and extreme temperature trackers captured no meaningful events in our dataset\n",
    "- **Backup/metadata fields** (60-74% null): Equipment backup information and alternative measurements are sparsely populated\n",
    "\n",
    "**Columns we kept despite moderate missingness:**\n",
    "- `HourlyPrecipitation` (11% null): Manageable with \"T\" (trace) conversion to 0.001\n",
    "- `HourlySeaLevelPressure` (9.8% null): Critical for aviation, can be imputed from station pressure\n",
    "- `HourlySkyConditions` (2% null): Important for visibility, low missingness\n",
    "- Delay attribution columns (81.73% null): Only populated when delays occur, will be filled with 0\n",
    "\n",
    "##### Predictive Relevance\n",
    "\n",
    "Our selected features directly influence or measure flight delays:\n",
    "\n",
    "1. **Weather Features (16 columns)**: Temperature, pressure, wind, visibility, and precipitation are the primary environmental factors affecting flight operations. We excluded `HourlyPressureChange` (65.6% null) as it's a derived metric that adds limited value when we have actual pressure readings.\n",
    "\n",
    "2. **Temporal Features (16 columns)**: Time-of-day, seasonality, and scheduling patterns are strong predictors of congestion and delays. We kept both raw timestamps and derived features for modeling flexibility.\n",
    "\n",
    "3. **Geographic Features (29 columns)**: Airport characteristics, regional patterns, and location data capture systematic differences in delay patterns across the national airspace system.\n",
    "\n",
    "4. **Flight Identifiers (5 columns)**: Carrier and aircraft information reveal operational patterns and reliability differences between airlines and specific aircraft.\n",
    "\n",
    "5. **Target Variables (16 columns)**: Multiple delay measurements provide flexibility in defining prediction targets (binary vs. continuous, arrival vs. departure).\n",
    "\n",
    "##### Operational Availability\n",
    "\n",
    "All selected features are available at or before scheduled departure time, enabling practical delay prediction systems. We excluded:\n",
    "\n",
    "- Future-looking variables like `ARR_TIME`, `WHEELS_ON`, `TAXI_IN` (these are outcomes, not predictors)\n",
    "- Post-flight metadata like `CANCELLATION_CODE`\n",
    "- Fields requiring manual reporting that may not be timely\n",
    "\n",
    "The following is a summary of the columns we kept.\n",
    "\n",
    "| Category | Original Columns | Selected Columns | Exclusion Reason |\n",
    "|----------|-----------------|------------------|------------------|\n",
    "| **Kept** | - | **88** | High quality, predictive, available |\n",
    "| **100% Null** | 66 | 0 | No information |\n",
    "| **>90% Null** | 40 | 0 | Unreliable |\n",
    "| **Non-predictive** | 22 | 0 | Metadata, IDs, post-flight data |\n",
    "\n",
    "### Summary Statistics and EDA of Departure Delays of 1-Year Dataset (2015)\n",
    "\n",
    "The following EDA was done on the **1-year dataset**, from **2015-01-01 to 2015-12-31**.\n",
    "\n",
    "Here are the summary statistics of our target variable, departure delay. This variable is coded as DEP_DELAY in the dataset.\n",
    "\n",
    "| mean | stddev | min | 25% | median | 75% | max  |\n",
    "|------|--------|-----|-----|--------|-----|------|\n",
    "| 9.36 | 37.07  | -82 | -5  | -2     | 7   | 1988 |\n",
    "\n",
    "The variable is highly skewed. On average, flight are delayed by 9 minutes, with a large standard deviation of 37 minutes. Interestingly, the median delay is -2 minutes, indicating that many flights depart slightly early. This means half the flights in fact depart on time or early. Even at the 75th percentile, the delay is at 7 minutes -- still on time! Indeed, 81.02% have a delay within 15 minutes, considered on-time. So if framed as a classification problem (which we are not), there is a serious class imbalance. \n",
    "\n",
    "There is also a more normal distribution of the delayed flights centered around 40 minutes, but with a very long right tail and a large range of extreme values (the more extreme values >= 300 are not shown here in the histogram). This is expected as departure time is censored on the left -- a flight simply cannot depart hours before scheduled time. \n",
    "\n",
    "![Histogram: Departure Delay](https://raw.githubusercontent.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/refs/heads/main/report_2/histogram_dep_delay.png)\n",
    "\n",
    "#### Seasonality\n",
    "\n",
    "Is there any seasonality in the departure delays? Over the year, the average delay does not appear homogeneous across time. Whereas the first week of the year averaged 20 minutes of delay, only two weeks later this dropped significantly to under 5 minutes. It's likely that the beginning-of-the-year delay is a spillover from the holiday season. In mid-February, there is another spike in delay minutes, then it goes down again, likely due to the winter storm that February. \n",
    "\n",
    "There is another spike during the summer, possibly due to increased traffic congestion from increased summer travel. Towards the winter months at the end of the year, we see the average departure delay spike significantly to greater than 25 minutes, likely due to the holiday season and increased travel as well as winter storms.\n",
    "\n",
    "In the chart below, the winter months are highlighted in blue, and the summer months are highlighted in yellow.\n",
    "\n",
    "![Departure Delays by Week](https://raw.githubusercontent.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/refs/heads/main/report_2/departure_delay_by_week.png)\n",
    "\n",
    "Is there any difference by day of the week? It seems so. Delays on Monday and Sunday are higher than delays on other days of the week. This possibly reflects the weekend travel back to work, though there is not a comparable spike on Friday and Saturday. Interestingly, with the 3-month dataset it was Wednesday that had the lowest average delay of all days, but with the 1-year/12-month dataset Saturday has the lowest average departure delay of all days.\n",
    "\n",
    "![Departure Delays by Day of Week](https://raw.githubusercontent.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/refs/heads/main/report_2/departure_delay_day.png)\n",
    "\n",
    "#### Seasonality X Carriers\n",
    "\n",
    "What if we break down departure delays by day for each carrier? Here, we can see the worst carriers are generally worse on all days of the week. Ultra low-cost carriers such as NK(Spirit) and F9 (Frontier) have generally worse delays, along with UA (United Airlines). Interestingly, the large differences between Saturday and Monday seem to disappear when looked at by each carrier. This could indicate that the cases of Monday/Friday peaks are external and apply to all carriers; however, the consequence (the delay) is modified by each carrier's strategy for handling delays. This indicates both day of week and carrier could be predictors of delays.\n",
    "\n",
    "![Departure Delays by Day of Week and Carrier](https://raw.githubusercontent.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/refs/heads/main/report_2/delays_by_day_carrier.png)\n",
    "\n",
    "#### Delays by Hour by Carrier\n",
    "\n",
    "What if we break down departure delays by hour for each carrier? Delays noticeably get worse late at night, from 21:00 to 2:00. This makes sense due to the compounding nature of airline departure delays, as delays earlier in the day can compound to longer delays late at night. Ultra low-cost carriers such as NK (Spirit) and F9 (Frontier) once again have longer than average departure delay times. UA (United Airlines) does as well. This indicates that hour of day can also be a predictor of departure delays.\n",
    "\n",
    "![Departure Delays by Hour and Carrier](https://raw.githubusercontent.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/refs/heads/main/report_2/delays_by_carrier_hour.png)\n",
    "\n",
    "#### Geography\n",
    "\n",
    "Departure delays differ by airports. Here we plot the delay on a map by departure airport of their average delay. Flight volume is encoded by the bubble size, and delay is encoded by color, with warmer colors being longer delays. The 10 airports with longest delays are annotated on the map in yellow bubbles. You can see the wide variety of airports included in our dataset, from the American Samoa to Puerto Rico. Not pictured as an inset here is Guam.\n",
    "\n",
    "It is interesting to observe that airports with large volumes (large bubbles) do not necessarily have worse delays than airports with less traffic. For example, hubs like Los Angeles CA, San Francisco CA, Austin TX have shorter delays than many smaller airports on the map.\n",
    "\n",
    "Regionally, a number of clustered airports around Massachusetts though do seem to be a regional hotspot of delays. Therefore, in model fitting, the geographical region should likely play an important role.\n",
    "\n",
    "![Departure Delays Geographically](https://raw.githubusercontent.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/refs/heads/main/report_2/geo_delays.png)\n",
    "\n",
    "#### Carrier Delays: Average Delay vs. Percent Flights Delayed\n",
    "\n",
    "Below we can see the relationship between the average departure delay, the percentage of flights delayed, with the cancellation rate of a particular airline. The size of the bubble indicates the flight volume of the carrier, and this chart only includes carriers with greater than 1000 flights in the dataset. \n",
    "\n",
    "Ultra low-cost carriers such as NK (Spirit) and F9 (Frontier) have noticeably higher average delay times, as well as UA (United). Cancellation rates do not seem directly related to average departure delay time, or percentage of flights delayed. There does seem to be a relationship between the average departure delay time and the percentage of flights delayed, however. Flight volume does not show an obvious relation relationship with average departure delay, percentage of flights delayed, or cancellation rate either.\n",
    "\n",
    "![Departure Delays by Carrier Volume](https://raw.githubusercontent.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/refs/heads/main/report_2/bubble_delays.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbded7bd-58f8-4901-adf5-0fc398ce5e8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Missing Values and Other Handling in 1-Year Dataset (2015)\n",
    "\n",
    "The feature family with the largest percentage of missing values is the Target Delay Variables as seen below - specifically the NAS_DELAY (National Airspace System Delay), LATE_AIRCRAFT_DELAY (Late Aircraft Delay), CARRIER_DELAY (Carrier Delay), WEATHER_DELAY (Weather Delay), and SECURITY_DELAY (Security Delay) features. This is because for these columns, if the delay is not associated with the delay in the description, or if there is no departure delay, the value is left blank.\n",
    "\n",
    "![Missingness Values](https://raw.githubusercontent.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/refs/heads/main/report_2/missingness_chart_readable.png)\n",
    "\n",
    "To handle these missing values, we will replace the null values with 0 in these columns. This way we can identify the flights that do not have delays related to the delay described in the column name (such as a weather delay or security delay.) The null values in this column are associated with flights that do not have a delay, and we can use the DEP_DELAY (Departure Delays) variable as the identifier for whether or not the particular flight was delayed.\n",
    "\n",
    "The second feature family with a large percentage of missing values is Weather Features - specifically HourlyPressureChange. It's less clear why this feature has such a high percentage of nulls, but possible reasons include that this metric is just not reported by a lot of weather stations, or equipment outages at the weather stations in question. It's also unclear exactly for what timespan this change is being reported. To handle these missing values, we will drop this column altogether, as the missingness is such a high percentage (greater than 50%), and the feature itself does not provide much insight into departure delays.\n",
    "\n",
    "For the other missing values in Weather Features, such as HourlyPrecipitation, we will replace the nulls with 0 if they are numerical as the existing data is useful for us and the null percentage is around 10% or less. HourlySkyConditions is a string value and will require more careful handling and transformation to fully utilize its potential.\n",
    "\n",
    "#### Cancelled Flights Handling\n",
    "\n",
    "In the 1-year dataset (2015), about **1.54%** of flights are cancelled. Since this is such a small proportion of our total dataset, and the departure delay time of a cancelled flight is uncertain, we will drop these flights.\n",
    "\n",
    "#### Negative Flight Departure Times Handling\n",
    "\n",
    "Another interesting category in our 1-year dataset is early departure flights, which have negative departure delay times. This is a large percentage of our flights; about **56.3%** have early departures. We intend to keep these negative values unchanged for the following reasons. \n",
    "\n",
    "First, these negative values contain important information for our models - they indicate whether a carrier is more operationally efficient, or perhaps better weather conditions, less congestion, or more efficient ground operations.\n",
    "\n",
    "Second, most machine learning models handle negative values fine - linear regression, random forests, gradient boosting, and neural networks all work perfectly well with negative numbers. This way the model can learn that negative values indicate good performance.\n",
    "\n",
    "Third, keeping the negative values preserves the full distribution of departure delay values. Excluding early arrivals would bias the model toward predicting only late outcomes. \n",
    "\n",
    "### Correlations and Distributions of 1-Year Dataset (2015)\n",
    "\n",
    "We ran a Pearson correlation on the continuous, numeric features in our dataset, dropping columns such as DEP_DELAY15 which are categorical.\n",
    "\n",
    "The most correlated features in our dataset are departure time and arrival time - as we would expect, delays in a flight's departure are correlated with a flight's arrival delay. Dew Point, Dry Bulb, and Wet Bulb Hourly Temperature are also highly correlated. HourlyStation Pressure and Elevation are inversely correlated - the higher elevation a station is at, the lower the pressure.\n",
    "\n",
    "Based on this information, we plan to remove features that are highly correlated with each other such as HourlyDewPointTemperature and HourlyWetBulbTemperature. We will also remove features that occur after departure, such as the arrival features, to avoid data leakage. \n",
    "\n",
    "![Pearson Correlation](https://raw.githubusercontent.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/refs/heads/main/report_2/pearson_correlation.png)\n",
    "\n",
    "The time variables (departures, arrivals) seem to have large outliers which need to be removed. The temperature variables show some normality in their histogram shapes. The wind variables are less normally distributed. Departure delay, as mentioned earlier, is highly skewed. This indicates a possible need for a feature transformation for our target variable of departure delays, such as a log transform.\n",
    "\n",
    "![Histograms One-Year](https://raw.githubusercontent.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/refs/heads/main/report_2/histograms_1y_readable.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cca67964-8f7a-4355-9d3c-056cfb985d06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Data Pipeline - Custom Joins\n",
    "#### 1. Flight–Airport Integration (IATA/ICAO Joins + Timezone Normalization)\n",
    "The first join enriches each flight record with origin and destination airport metadata.\n",
    "\n",
    "- **Airport Metadata Construction:**  \n",
    "  Airport codes and airport-timezone tables are merged using both **IATA and ICAO identifiers**, ensuring maximum coverage across large and small airports.\n",
    "\n",
    "- **Flight Enrichment:**  \n",
    "  Cleaned airport attributes—including latitude, longitude, country, and timezone—are joined to the flights dataset twice (origin and destination).  \n",
    "  A **broadcast hint** is applied so the airport table is held fully in memory, reducing shuffle and improving join performance at scale.\n",
    "\n",
    "- **UTC Time Construction:**  \n",
    "  Local departure timestamps are derived from the flight date and CRS departure time, then converted to UTC using the **origin airport timezone**.  \n",
    "  Additional features required by the weather join—such as **two-hours-prior** and **four-hours-prior** UTC timestamps—are computed directly from these normalized fields.\n",
    "\n",
    "#### 2. Spatial Mapping: Linking Airports to Nearest NOAA Weather Station\n",
    "Each origin airport is mapped to its nearest NOAA station using a two-stage spatial filter.\n",
    "\n",
    "  <p align=\"center\">\n",
    "  <img \n",
    "    src=\"https://github.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/raw/main/report-images/Sid-finding-nearest-weather-station.png\"\n",
    "    width=\"60%\"\n",
    "    style=\"border:3px solid black;\"\n",
    "  >\n",
    "</p>\n",
    "\n",
    "- **Bounding Box Pre-Filter:**  \n",
    "  A 0.5-degree latitude/longitude bounding box limits candidate weather stations.\n",
    "\n",
    "- **Precise Distance Calculation:**  \n",
    "  Candidates undergo a Haversine great-circle distance computation, restricted to stations within **50 km**.\n",
    "\n",
    "- **Nearest-Station Selection:**  \n",
    "  Window-ranking selects the closest station for each airport; mappings are cached to avoid repeated spatial joins.\n",
    "\n",
    "#### 3. Temporal Integration: Time-Window Weather Matching (Bucketed Join + Ranking)\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src='https://github.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/raw/main/report-images/Sid-time-weather-match.png' width=\"60%\"\n",
    "    style=\"border:3px solid black;\">\n",
    "</p>\n",
    "\n",
    "This join aligns flights with weather observations using co-partitioning by (station, bucket) and includes both current and previous bucket weather. This ensures each flight can see all relevant weather observations.\n",
    "\n",
    "## Overview of Logic\n",
    "1. Create 30-minute time buckets for both flights and weather.\n",
    "2. Shift weather buckets forward so each bucket contains its own and the previous bucket’s observations.\n",
    "3. Repartition flights and weather by (station, bucket).\n",
    "4. Join flights with weather on exact (station, bucket).\n",
    "5. Keep only weather before the flight’s two-hours-prior time.\n",
    "6. Rank by closeness and choose the nearest weather observation.\n",
    "7. Retain all flights, even those with no matching weather.\n",
    "\n",
    "## Bucket Construction\n",
    "Both datasets receive a Unix-based 30-minute bucket.\n",
    "\n",
    "## Bucket Shifting\n",
    "Weather is duplicated with bucket shifted by +30 min, then unioned with original.\n",
    "\n",
    "## Co-Partitioned Repartitioning\n",
    "Weather: repartition(\"station\", \"bucket\")  \n",
    "Flights: repartition(\"origin_station_id\", \"bucket\")\n",
    "\n",
    "## Join Logic\n",
    "Left join:\n",
    "```\n",
    "(origin_station_id == station) AND (flight.bucket == weather.bucket)\n",
    "```\n",
    "\n",
    "## Weather Filtering & Ranking\n",
    "- Keep weather only if weather_ts <= flight_ts.\n",
    "- Compute time difference (flight_ts - weather_ts).\n",
    "- Window partition on natural flight keys.\n",
    "- Choose rank = 1 (closest weather).\n",
    "- Retain weather-null flights.\n",
    "\n",
    "\n",
    "#### 4. Entity Relationship Diagram\n",
    "\n",
    "<p align=\"center\">\n",
    "\n",
    "<img src='https://github.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/raw/main/report-images/Sid-Phase2-ERD-diag-2.png' width=\"60%\"\n",
    "    style=\"border:3px solid black;\">\n",
    "</p>\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3d8084e-8ce1-4388-be1f-268fb3e99404",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Pipeline - Modeling\n",
    "\n",
    "<img src='https://github.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/raw/main/report-images/Pipeline-Diagram.png' style=\"width:100%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b950cd26-f16f-45c7-9d14-60972e1ec84e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Cross Validation\n",
    "\n",
    "To prevent data leakage and ensure robustness, we implemented cross validation for the times series data. We started with **expanding window** cross validation, i.e., each new fold containing both the training and validation data of the previous fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a88e24c-68af-4bea-b944-392e82b1bedd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Expanding Window CV\n",
    "![expanding window cv](https://github.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/blob/main/report_2/expanding-window-cv-5y.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39b21666-8381-48b3-8c96-cd14be4ae0f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In this method, we divide the total time span into 5 equal periods. For example, for the 5-year data, each period is 1 year. The first fold uses the first period as the training data and the second period as the validation data. The second fold uses both the training and validation data of the previous fold as the new training data, and the next period as the validation data, and so on. The last period is reserved as the test data that is held out for testing, not used in cross validation.\n",
    "\n",
    "This method is relatively easy to implement and provides ample training data for each fold. However, it is not very scalable to a large dataset, as the amount of training grows quadratically with respect to the number of rows.\n",
    "\n",
    "Moreover, it also assumes that older data is as predictive of the future as more recent data, which may not be correct. Therefore, we have decided to transition to a rolling window approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23d5b899-2e79-4e5a-a4ce-2ceb94b6f6a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Rolling Window CV\n",
    "\n",
    "![rolling window cv](https://github.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/blob/main/report_2/rolling-window-cv-5y.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f32bb81d-91d5-448c-9971-16a9b5a5207d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In the rolling window approach, we instead only consider a rolling window for each fold instead of all data thus far. The first fold is the same as before, but the second fold now only uses the validation data in the first fold as the new training data, and so on. This way, each fold has constant size, which significantly cuts down the training runtime. Similar to the expanding window approach, we still reserve the last fold as the test set, which makes sure that the model is still eventually evaluated on held-out data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7213c3fe-937f-49f9-8324-ff4b47ba8edd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Linear Regression Modeling Pipeline\n",
    "The modeling pipeline for our linear regression models begins by preparing and loading the necessary data folds. It applies a consistent feature engineering process across all runs: cleaning labels and numeric values, imputing missing data where applicable, encoding categorical features, assembling all inputs, and standardizing features before training. Numerical fields such as day of week, month and hour departure block components are transformed into categorical variables to capture cyclical patterns.\n",
    "\n",
    "Different models may use varied validation strategies, including fixed train/validation/test splits, k‑fold cross‑validation, or expanding‑window validation schemes that progressively increase the training period. Model selection is typically guided by validation performance, often using metrics such as RMSE, MAE, R², or MSE for regression tasks and accuracy‑based measures for classification evaluations. The final model is then assessed on a held‑out test set to report comprehensive performance across all splits.\n",
    "\n",
    "<img src='https://github.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/raw/main/report-images/Pipeline-Diagram.png' style=\"width:100%;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6c40a4b-34a3-4f6d-a8c5-fcf518b139e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Checkpointing and Versioning\n",
    "\n",
    "#### Spark Checkpointing\n",
    "A global Spark checkpoint directory is used:\n",
    "\n",
    "```\n",
    "dbfs:/mnt/mids-w261/student-groups/Group_4_2/checkpoints\n",
    "```\n",
    "\n",
    "#### Active Checkpoint\n",
    "One checkpoint is active:\n",
    "\n",
    "- **Post–bucketed weather join checkpoint**  \n",
    "  `df_weather_join_filtered = df_weather_join_filtered.checkpoint(eager=True)`\n",
    "\n",
    "This trims DAG lineage and stabilizes the pipeline.\n",
    "\n",
    "#### Not Checkpointed\n",
    "The pipeline does **not** checkpoint raw ingestion, cleaned datasets, or spatial join outputs.\n",
    "\n",
    "#### Persisted Outputs\n",
    "Two Parquet outputs serve as versioned datasets:\n",
    "\n",
    "1. Full integrated flights + weather table  \n",
    "2. Year-sliced 2015 dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8f796f0-40ea-455d-a606-6205991db740",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Baseline Model: Simple Linear Regression with Top Features\n",
    "In order to develop our baseline model we conducted a variety of different experiments primarily focusing on changes to the data set to test scalability, null data handling, the use of cross validation and the dataset itself.\n",
    "\n",
    "We conducted the following Experments:\n",
    "1. Experiment 1: Does cross-validation effect performance?\n",
    "2. Experiment 2: Does imputation strategy effect performance?\n",
    "3. Experiment 3: Does a custom data set effect performance?\n",
    "- **Number of Experiments Conducted:**  3\n",
    "\n",
    "We changed the following variables to test these questions:\n",
    "- Scalabilty - Modifying the size of the dataset.\n",
    "- Data Cleanliness - Using the provided default data vs the one with the custom joins.\n",
    "- Data Leakage - The use of data without cross-validation vs with cross-validation.\n",
    "- Data Imputation - Dropping the null values or imputing the missing values with the median.\n",
    "\n",
    "In all of the experiments conducted we kept the feature families and features the same as we are primarily testing to determine implementation details and in our next model plan to implement an improved linear regression model using Lasso regularization to perform feature selection for our more complex models. \n",
    "\n",
    "Using our findings from the following experiments we then implement a final linear regression baseline model using scikit-learn's LinearRegression class, trained only on the ten raw features that appear to have the most significance.\n",
    "\n",
    "### Feature Families\n",
    "In order to create, experiment with and assess our baseline models we chose to evaluate our model using ten features which we've categorized into four types of feature families: Temporal Features, Airport Features, Weather Features and Flight Features. Within these feature families we chose the specific features based on their percieved importance, low number of null values and because they are limited in the amount of data leakage. Some features will entail some data leakage if they don't use cross validation and even with cross validation their may be some data leakage from our imputation strategies. The features affected by data leakage through imputation of the median are the Weather Factors: HourlyWindSpeed, HourlyVisibility and HourlyPrecipitation as these are the only features with null values. Additionally, some experiments may be impacted by data leakage through the train/val/test split. Note: The specific experiments affected by any form of data leakage are clearly outlined in each experiments Model Table. \n",
    "\n",
    "| Feature Family | Number of Features | Features | Description |\n",
    "|----------------|--------------------|--------------|--------------|\n",
    "|Temporal Features | 3 | DAY_OF_WEEK, MONTH, DEP_TIME_BLK | Weather conditions can inherently increase an airport's susceptibility to operational delays.\n",
    "|Airport Features | 2 | ORIGIN, DEST | An airport's inherent characteristics can make it more or less prone to delays.\n",
    "|Weather Features | 3| HourlyWindSpeed, HourlyVisibility, HourlyPrecipitation | Factors such as time of day, day of the week, and seasonality, can inherently increase an airport's susceptibility to operational delays \n",
    "|Flight Features | 2 | OP_UNIQUE_CARRIER, DIST | Flight characteristics like route distance and carrier may amplify delays.\n",
    "\n",
    "### Input Features\n",
    "\n",
    "|Feature Family|   Temporal Features  |   Description   |  % null |  Rationale   | \n",
    "|-----------------|-----------------|---------------|----------|---------------|\n",
    "|Temporal Features| DAY_OF_WEEK  | Day of week | 0% | Temporal pattern - certain days (e.g., Mondays, Fridays) may have higher delay rates due to business travel patterns and airport congestion | \n",
    "|Temporal Features| MONTH  | Month of flight | 0%  | Seasonal patterns - weather seasons, holidays, peak travel periods (summer, winter holidays) may affect delay rates |\n",
    "|Temporal Features| DEP_TIME_BLK  | CRS departure time block in hourly intervals | 0%  | Time of day effects - rush hours (early morning, evening) have higher congestion and delay rates. Overnight operations may have different patterns. |  \n",
    "|Airport Features| ORIGIN  | Origin airport code (IATA) | 0% | Airport-specific delay patterns - some airports are more prone to delays due to congestion, infrastructure, weather patterns, and operational capacity| \n",
    "|Airport Features| DEST  | Destination airport code (IATA) | 0%  | Destination airport characteristics - congestion at destination can cause delays, and weather at destination may affect departure decisions|\n",
    "| Weather Features| HourlyWindSpeed  | Horizontal wind speed rate in meters per second (scaled by 10, 9999=missing) | 0.33% | High winds can cause delays and cancellations. Crosswinds above certain thresholds require different runway operations or may ground flights. | \n",
    "| Weather Features| HourlyVisibility | Horizontal visibility distance in meters (999999=missing, >160000 entered as 160000) |  0.29%  | Low visibility (fog, haze, precipitation) causes significant delays as it affects takeoff and landing safety requirements. |\n",
    "| Weather Features| HourlyPrecipitation  |  Liquid precipitation depth in millimeters (scaled by 10, 9999=missing) | 11.35%  |  Precipitation directly impacts operations - rain, snow, and freezing precipitation can cause delays and cancellations. |  \n",
    "|Flight Features| OP_UNIQUE_CARRIER  |  Unique carrier code - when same code used by multiple carriers, numeric suffix added (e.g., PA, PA(1)) | 0% | Carrier-specific operational efficiency and reliability - different airlines have varying on-time performance, maintenance practices, and operational strategies.| \n",
    "|Flight Features| DISTANCE  | Distance between airports in miles | 0%  | Flight distance may delays - longer flights may be exposed to more potential disruptions from weather or air traffic control.|\n",
    "\n",
    "### Metrics and Evaluation\n",
    "Our model is a regression model designed to predict flight delays (in minutes). To evaluate its performance, we use a combination of standard regression metrics and domain-specific operational metrics. For transparency we report on a number of regression and classification metrics, but will primarily be evaluating the success on our models on the RMSE for regression and OTPA and SDDR for classification. Please refer to the definitions of these additional metrics in the Appendix.\n",
    "\n",
    "###Loss Function:\n",
    "#####  Mean Squared Error (MSE)\n",
    "Our baseline linear regression model predicts continuous flight delay times in minutes using Mean Squared Error (MSE) as its loss function during training, which penalizes large prediction errors more heavily than small ones. It is useful as our loss function as it measures the average squared difference between predicted and actual delay times. MSE penalizes large errors more than small errors due to the squaring operation, making it sensitive to outliers and ensuring the model focuses on reducing substantial prediction errors.\n",
    "\n",
    "Mean Squared Error (MSE), calculated as \n",
    "\n",
    "$$MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}i)^2$$\n",
    "\n",
    "| Loss Function | Formula | Description |\n",
    "|--------|---------|-------------|\n",
    "| MSE | $\\mathrm{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2$ | Mean Squared Error - average of squared prediction errors, useful for optimization but less interpretable than RMSE |\n",
    "\n",
    "###Key Regression Metrics\n",
    "#### Root Mean Squared Error (RMSE)\n",
    "We've selected RMSE (the square root of MSE) as our key regression metric as it is expressed in the same units as the target variable (minutes), providing a more interpretable measure of average prediction error magnitude that can be directly understood by stakeholders. \n",
    "\n",
    "The Root Mean Squared Error (RMSE), computed as\n",
    "\n",
    "$$RMSE = \\sqrt{MSE} = \\sqrt{\\frac{1}{n}\\sum{i=1}^{n}(y_i - \\hat{y}i)^2}$$\n",
    "\n",
    "| Metric | Formula | Description |\n",
    "|--------|---------|-------------|\n",
    "| RMSE | $\\mathrm{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2}$ | Root Mean Squared Error - measures average prediction error magnitude in original units, penalizes large errors more heavily |\n",
    "\n",
    "###Key Classification Metrics\n",
    "Beyond standard regression metrics, we evaluate operational utility through domain-specific classification metrics to help business stakeholders understand our models prediction outcomes.\n",
    "\n",
    "#### On-Time Performance Accuracy (OTPA)\n",
    "OTPA (On-Time Performance Accuracy) measures binary classification accuracy for the critical 15-minute on-time threshold defined by the U.S. Department of Transportation.\n",
    "\n",
    "**On-Time Performance Prediction Accuracy** measures the accuracy of predicting flights within a narrow tolerance window (+/- 5 minutes), which is critical for operational planning because resource allocation decisions, such as gate assignments, crew scheduling, and passenger communication, require precise delay estimates rather than broad categorizations. \n",
    "\n",
    "OTPA converts delay predictions into a binary outcome—*on-time* (delay < 15 minutes) or *delayed* (≥ 15 minutes)—and measures accuracy or F1-score. This bridges regression and classification perspectives, aligning with how the aviation industry defines on-time performance. On-Time flights help improve the reputation of the airline and keep the airport traffic smoothly moving. \n",
    "\n",
    "We operationalize this metric by converting continuous delay predictions into binary outcomes consistent with U.S. Department of Transportation (DOT) definitions:\n",
    "\n",
    "- On-Time: delay < 15 minutes\n",
    "- Delayed: delay ≥ 15 minutes\n",
    "\n",
    "**On-Time Definition:**\n",
    "\n",
    "- Predicted On-Time = 1 if `ŷ < 15`, else 0  \n",
    "- Actual On-Time = 1 if `y < 15`, else 0  \n",
    "\n",
    "**Formula:**\n",
    "\n",
    "`On-Time Prediction Accuracy (OTPA) = (TP + TN) / (TP + TN + FP + FN)`\n",
    "\n",
    "Where:  \n",
    "- **TP** = correctly predicted on-time flights  \n",
    "- **TN** = correctly predicted delayed flights  \n",
    "- **FP** = predicted on-time but actually delayed  \n",
    "- **FN** = predicted delayed but actually on-time  \n",
    "\n",
    "| Metric | Formula | Description |\n",
    "|--------|---------|-------------|\n",
    "| **OTPA Accuracy** | $\\text{OTPA} = \\frac{TP + TN}{TP + TN + FP + FN}$ | Binary classification accuracy for on-time (<15 min) vs delayed (≥15 min) flights |\n",
    "\n",
    "This metric translates regression outputs into actionable insights for **crew scheduling**, **resource allocation**, and **passenger notifications**, emphasizing precision in predicting near-schedule flights.\n",
    "\n",
    "#### Severe Delay Detection Rate (SDDR)\n",
    "Severe Delay Detection Rate measures the recall specifically for delays >=60 minutes, which is important for identifying flights requiring major operational adjustments such as aircraft substitutions, crew reassignments, or passenger compensation, as these severe delays have the most significant impact on operations and customer satisfaction. \n",
    "\n",
    "Buckets:\n",
    "- OnTime: < 15 minutes\n",
    "- Delayed: 15-60 minutes\n",
    "- Severe: 60+ minutes\n",
    "\n",
    "***Other metrics***\n",
    "\n",
    "For transparency we do report on other metrics as they can be useful for identifying patterns or concerns in our modeling, some of thes other metrics include: \n",
    "- MAE calculates the average absolute error without squaring, making it more robust to outliers and representing typical prediction error\n",
    "- R² (coefficient of determination) to quantify the proportion of variance in flight delays explained by our model, with values closer to 1 indicating better predictive power. \n",
    "\n",
    "# Experiment 1: Does Cross validation effect performance?\n",
    "In our first experiment we started by establishing Model 1: No-Cross-Validation/3-month/Data-Imputed which is a fast, exploratory baseline on the 3‑month default dataset to validate feature choices, data types, preprocessing and pipeline performance. We then implemented Model 2: Cross-Validation/3-month/Data-Imputed with the same parameters but using the cross-validated data and then re‑estimated performance under stricter, data leakage‑resistant evaluation. These models allowed for rapid model development and allowed us to prototype and establish rudimentary performance benchmarks. We expect performance to decrease as we add stricter data leakage controls through cross validation, which is desirable because it means our training metrics more accurately reflect production performance.\n",
    "\n",
    "### Experiment 1: Model Table\n",
    "\n",
    "| Experiment ID | Pipeline | Features Used | Model Details | Data Leakage |\n",
    "|----------------|-----------|----------------|---------------|---------------|\n",
    "| Model 1 | No-Cross-Validation/3-month/Data-Imputed | Feature Families: Temporal, Airport, Weather, Flight  | Raw Data, 3 month default data set, no cross validation, imputed null | Random split of train/val/test, Weather data imputed median |\n",
    "| Model 2 | Cross-Validation/3-month/Data-Imputed | Feature Families: Temporal, Airport, Weather, Flight  | Raw Data, 3 month default data set, cross validation, imputed null | Weather data imputed median |\n",
    "\n",
    "### Model 1: No-Cross-Validation/3-month/Data-Imputed\n",
    "Model 1 does not implement any data leakage controls and these results will have data leakage as they don't utilize cross validaiton and is only to be evaluated as an exploration model to get a feel for the which data types need to be converted and if they will be assessed numerically or categorically. For our numerical features we imputed the median and for our categorical features we imputed \"UNKNOWN\". \n",
    "\n",
    "- **Cluster Size:** \n",
    "| Resource            | Value   | Calculation                                      |\n",
    "|---------------------|---------|--------------------------------------------------|\n",
    "| Total CPUs (cores)  | 12      | 3 executors × 4 cores/executor                   |\n",
    "| Total RAM           | ~6.7 GB | 3 executors × 2218 MB/executor ≈ 6654 MB ≈ 6.7 GB |\n",
    "| Executors           | 3       | Worker processes                                 |\n",
    "| Cores per executor  | 4       | Parallel tasks per executor (estimated)          |\n",
    "| RAM per executor    | 2.2 GB  | Memory available to each executor                |\n",
    "- **Build Time per Model:**  10m 55s\n",
    "            \n",
    "\n",
    "###Pipeline Overview\n",
    "The pipeline first loads the 3‑month OTPW data, selects 10 baseline features plus DEP_DELAY, cleans numeric columns, and uses median imputation to fill missing weather and distance values. It then treats six categorical variables with StringIndexer + OneHotEncoder, assembles all imputed numeric and encoded categorical features into a single vector, and standardizes them with StandardScaler. Finally, it fits an unregularized linear regression model on the scaled features and evaluates performance with regression metrics (RMSE, MAE, R²) and classification metrics (OTPA, SDDR, bucket accuracy) on train/validation/test splits.\n",
    "\n",
    "<img src='https://github.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/raw/main/report-images/Model1-Pipeline.png' style=\"width:100%; width:5000px;\">\n",
    "\n",
    "###Model 1 Results\n",
    "This model is deceptively stable across the training, validation and test sets as a result of data leakage by implementing a random split which pulls future data into the training and validation sets. We can see that regression and classification metrics are operational and the pipeline is producing results. The MAE indicates that predictions are off by about 19 minutes on average, the R² explains roughly 3-4% of the variance in departure delays and the RMSE of about 37 minutes indicates there are some larger errors pulling the average error up as large errors are penalized with a higher weight. For OTPA an accuracy of 71–72% accuracy indicates the model is reasonably good at distinguishing on-time (<15 min) vs delayed (≥15 min), largely because most flights are on time or only mildly delayed. The SDDR recall is 0 meaning the baseline never identifies severe delays which may be caused by a severe class imbalance of on time vs delayed flights and the smaller 3 month data set. Accuracy for classifying flights into the correct bucket is about 28% indicating there's room for model refinement and improvement. \n",
    "\n",
    "#### Regression and Classification Metrics\n",
    "\n",
    "<table style=\"width:100%; border-collapse: collapse;\">\n",
    "    <tr>\n",
    "        <td style=\"width:50%; text-align:center; padding:10px; vertical-align:top;\">\n",
    "            <h3>Regression Metrics</h3>\n",
    "            <img src='https://github.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/raw/main/report-images/Emily-Model1-RegressionMetrics.png' style=\"width:100%; max-width:450px;\">\n",
    "        </td>\n",
    "        <td style=\"width:50%; text-align:center; padding:10px; vertical-align:top;\">\n",
    "            <h3>Classification Metrics</h3>\n",
    "            <img src='https://github.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/raw/main/report-images/Emily-Model1-ClassificationMetrics.png' style=\"width:100%; max-width:450px;\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "### Model 2: Cross-Validation/3-month/Data-Imputed\n",
    "Model 2 introduces explicit data leakage controls by using pre‑checkpointed, time‑ordered folds and an expanding‑window cross‑validation scheme, so each fold only trains on past months and validates/tests on future months, making it our first production‑style baseline rather than a purely exploratory model. Like Model 1, it imputes the median for numerical features and replaces missing categorical values with \"UNKNOWN\", but it evaluates performance across multiple folds to provide a more reliable estimate of generalization. Note: there aren't currently any categorcial features with null values but this will allow our pipeline to scale.\n",
    "\n",
    "- **Cluster Size:** \n",
    "| Resource            | Value   | Calculation                                      |\n",
    "|---------------------|---------|--------------------------------------------------|\n",
    "| Total CPUs (cores)  | 8       | 2 executors × 4 cores/executor                   |\n",
    "| Total RAM           | ~4.4 GB | 2 executors × 2218 MB/executor ≈ 4436 MB ≈ 4.4 GB |\n",
    "| Executors           | 2       | Worker processes                                 |\n",
    "| Cores per executor  | 4       | Parallel tasks per executor (estimated)          |\n",
    "| RAM per executor    | 2.2 GB  | Memory available to each executor   \n",
    "- **Build Time per Model:**  10m 44s runtime\n",
    "            \n",
    "\n",
    "###Pipeline Overview\n",
    "The Model 2 pipeline first loads pre-checkpointed 3‑month folds and, for each fold, applies the same baseline feature pipeline: clean the label and numeric fields, perform median imputation on weather and distance, encode categorical variables with StringIndexer + OneHotEncoder, assemble all features, standardize them, and fit an unregularized linear regression model. It uses an expanding-window cross‑validation scheme where each fold trains on progressively more months of data and validates on the next month, tracking the best model by validation RMSE. Finally, it evaluates that best model on a held‑out test fold and reports comprehensive regression (RMSE, MAE, R², MSE) and classification (OTPA, SDDR) metrics across train, validation, and test splits. \n",
    "\n",
    "<img src='https://github.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/raw/main/report-images/Emily-Model2-Pipeline.png' style=\"width:100%;\">\n",
    "\n",
    "###Model 2 Results\n",
    "In this model we begin to notice a slight decrease in R² suggesting minimal explanatory gain in terms of variance captured. Regression metrics showed moderate improvement under cross-validation. Model 2 achieved a lower RMSE (34.28) and MSE (1174.85) than Model 1 (RMSE = 37.17; MSE = 1381.57), indicating more stable predictions. The MAE indicates that predictions are off by about 18 minutes, the R² explains roughly 1% of the variance in departure delays and the RMSE indicates our predictions are off by 32-42 minutes. For OTPA accuracy remained steady at about 71% accuracy is reasonably good at determining on time flights vs delayed and we don't see a considerable change in performance. The SDDR recall remained at 0 meaning the baseline never identifies severe delays and bucket accuracy dropped slightly from 28% to 27%.\n",
    "\n",
    "#### Regression and Classification Metrics\n",
    "<table style=\"width:100%; border-collapse: collapse;\">\n",
    "    <tr>\n",
    "        <td style=\"width:50%; text-align:center; padding:10px; vertical-align:top;\">\n",
    "            <h3>Regression Metrics</h3>\n",
    "            <img src='https://github.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/raw/main/report-images/Emily-Model2-RegressionMetrics.png' style=\"width:100%; max-width:450px;\">\n",
    "        </td>\n",
    "        <td style=\"width:50%; text-align:center; padding:10px; vertical-align:top;\">\n",
    "            <h3>Classification Metrics</h3>\n",
    "            <img src='https://github.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/raw/main/report-images/Emily-Model3-ClassificationMetrics.png' style=\"width:100%; max-width:450px;\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "### Experiment 1 Results\n",
    "Overall both models are similiarly weak in regression and classification performance but in order to accurately predict real world data we will need to implement the cross validation strategy. Model 1 is susceptible to data leakage and overly optimistic results that likely won't generalize well to unseen test data. Model 2 has a wider range in RMSE which reveals that performance depends on which months you’re predicting and that the model can even be worse than a mean baseline in some windows. This is the more honest view of generalization over time. Neither model was able to predict severe delays of over 60 minutes. Given the result of this experiment we plan to pursue a cross validation strategy as it respects temporal ordering by only training on past data and controls data leakage. Going forward with our models temporal factors need to respected and patterns in the cross-validation folds may be useful in our predictions.\n",
    "\n",
    "# Experiment 2: Does imputation strategy effect performance?\n",
    "In this experiment we evaluate Model 3 Cross-Validation/12-month/Data-Imputed with Model 4 Cross-Validation/12-month/Dropped-Null to assess the impact of handling missing data by imputing null values versus dropping null values. In practice we can’t just drop null values because our models will need to be able to train and make test predictions even if data isn't available,but dropping them can help evaluate our imputation strategy.\n",
    "\n",
    "Both models implement a baseline linear regression pipeline with expanding window cross-validation on 12 months of flight data without regularization for interpretability, and evaluation using both standard regression metrics and domain-specific classification metrics.\n",
    "\n",
    "To assess the impact of missing data handling, we will run two variants of this model: Model 3 which imputes missing values using median imputation to retain all available data and Model 4 which drops rows with null values in weather features to train only on complete cases.\n",
    "\n",
    "By comparing performance between these approaches, we can evaluate whether the information gained from imputed data outweighs potential noise introduced by the imputation process. This comparison will inform our data handling strategy for more complex models in subsequent experiments.\n",
    "\n",
    "| Experiment ID | Pipeline | Features Used | Model Details | Data Leakage |\n",
    "|----------------|-----------|----------------|---------------|---------------|\n",
    "| Model 3 | Cross-Validation/12-month/Data-Imputed/Default-Data | Feature Families: Temporal, Airport, Weather, Flight  | 12 month default data set, using cross-validation, imputed missing values | Weather data imputed median |\n",
    "| Model 4 | Cross-Validation/12-month/Dropped-Null/Default-Data | Feature Families: Temporal, Airport, Weather, Flight  | 12 month default data set, using cross-validation, dropped null | Weather data imputed median |\n",
    "\n",
    "## Model 3 - Cross-Validation/12-month/Data-Imputed/Default-Data\n",
    "Model 3 implements a production‑style baseline by running an expanding‑window, time‑ordered cross‑validation on 12 months of data, so each fold trains only on past months and validates/tests on future months to control temporal data leakage. It uses a linear regression pipeline with median imputation for missing weather/distance features and \"UNKNOWN\" for missing categorical values, allowing the model to retain all available data while we measure generalization across folds via both regression and domain‑specific classification metrics. As with the other models, some leakage risk remains because weather features are imputed with the median.\n",
    "\n",
    "- **Cluster Size:** \n",
    "| Resource | Value | Calculation |\n",
    "|----------|-------|-------------|\n",
    "| Total CPUs (cores) | 32 | 8 executors × 4 cores/executor |\n",
    "| Total RAM | ~17.6 GB | 8 executors × 2218 MB/executor ≈ 17.7 GB |\n",
    "| Executors | 8 | Worker processes |\n",
    "| Cores per executor | 4 | Parallel tasks per executor |\n",
    "| RAM per executor | 2.2 GB | Memory available to each executor | \n",
    "- **Build Time for Model 3:**  0h 20m 5.90s\n",
    "\n",
    "###Pipeline Overview\n",
    "<img src='https://github.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/raw/main/report-images/Emily-Model3_Pipeline-Diagram.png' style=\"width:100%;\">\n",
    "\n",
    "### Model 3 Results\n",
    "Model 3 was trained on the 12‑month flight delay dataset using expanding window cross-validation with four folds. Validation performance across folds was stable, with an average RMSE of 35.86 minutes (SD = 3.49) and MAE of 19.09 minutes. The mean R² value was -0.0037, revealing that the model captured minimal variance in actual delay outcomes. The best fold achieved an RMSE of 30.38, while performance on the held-out test set remained consistent (RMSE = 35.96; MAE = 18.29; R² = 0.0077).\n",
    "\n",
    "Classification performance indicated moderate success in identifying on-time flights but poor discrimination of severe delays. Overall on-time prediction accuracy (OTPA) reached 69.52 percent. However, the severe delay recall (SDDR) was virtually negligible at 0.03 percent, showing that the current features and model complexity were insufficient for capturing rare, high-delay events.\n",
    "\n",
    "Collectively, Model 3 serves as a functional but limited baseline. Its consistency across folds suggests reliable training behavior, yet the low explanatory power and weak severe-delay detection highlight the need for more informative features and refined temporal modeling.\n",
    "\n",
    "#### Regression and Classification Metrics\n",
    "\n",
    "<table style=\"width:100%; border-collapse: collapse;\">\n",
    "    <tr>\n",
    "        <td style=\"width:50%; text-align:center; padding:10px; vertical-align:top;\">\n",
    "            <h3>Regression Metrics</h3>\n",
    "            <img src='https://github.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/raw/main/report-images/Emily-Model3_regression-Correct.png' style=\"width:100%; max-width:450px;\">\n",
    "        </td>\n",
    "        <td style=\"width:50%; text-align:center; padding:10px; vertical-align:top;\">\n",
    "            <h3>Classification Metrics</h3>\n",
    "            <img src='https://github.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/raw/main/report-images/Emily-Model3-Classification-Correct.png' style=\"width:100%; max-width:450px;\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "## Model 4 - Cross-Validation/12-month/Dropped-Null/Default-Data\n",
    "Model 4 implements the same 12‑month, expanding‑window cross‑validation framework as Model 3, using pre‑checkpointed OTPW folds so each fold trains only on past months and validates/tests on future months, giving a leakage‑controlled baseline. The key difference is its drop‑null strategy for weather and distance features: instead of imputing missing values, Model 4 trains only on complete cases, which yields cleaner data and avoids making distributional assumptions about missingness, and slightly simplifies the pipeline (no imputation stage). The trade‑off is that we reduce the effective dataset size and can introduce bias if missingness is systematic (e.g., sensors fail more often in severe weather), so comparing Model 4 against the imputation‑based Model 3 helps us understand whether the gain in data quality outweighs the loss of coverage.\n",
    "\n",
    "- **Cluster Size:**\n",
    "| Resource            | Value   | Calculation                                      |\n",
    "|---------------------|---------|--------------------------------------------------|\n",
    "| Total CPUs (cores)  | 12      | 3 executors × 4 cores/executor                   |\n",
    "| Total RAM           | ~6.7 GB | 3 executors × 2218 MB/executor ≈ 6654 MB ≈ 6.7 GB |\n",
    "| Executors           | 3       | Worker processes                                 |\n",
    "| Cores per executor  | 4       | Parallel tasks per executor (estimated)          |\n",
    "| RAM per executor    | 2.2 GB  | Memory available to each executor     \n",
    "- **Build Time for Model 5:**  11m 16s\n",
    "\n",
    "###Pipeline Overview\n",
    "Model 4 uses a baseline linear regression pipeline with an expanding‑window, time‑ordered cross‑validation on 12 months of pre‑checkpointed OTPW data. For each fold, it cleans the label and numeric features, drops any rows with missing weather/distance values, encodes categorical variables with StringIndexer + OneHotEncoder, assembles all features, standardizes them, and fits an unregularized linear regression model. Performance is then evaluated per fold (and on a held‑out test set) using both standard regression metrics and domain‑specific classification metrics.\n",
    "\n",
    "<img src='https://github.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/raw/main/report-images/Emily-Model4-Pipeline.png' style=\"width:100%;\">\n",
    "\n",
    "### Model 4 Results\n",
    "Model 4 was trained on the 12‑month flight delay dataset using expanding window cross-validation with four folds and a dropped-null data handling strategy. The model demonstrated consistent performance across folds, with an average RMSE of 35.33 minutes (SD = 3.37) and MAE of 19.11 minutes. The mean R² was -0.0156. The best validation fold reached an RMSE of 29.94, showing stable though modest predictive accuracy. Performance on the held-out test set mirrored cross-validation trends, achieving an RMSE of 34.99 minutes, MAE of 17.74 minutes, and an R² of 0.0034.\n",
    "\n",
    "Classification metrics indicated reasonable on-time flight detection but continued difficulty distinguishing severe delays. The model achieved an on-time performance accuracy (OTPA) of 71.37 percent. Despite these results, the severe delay detection rate (SDDR) remained zero, demonstrating that the model could not identify any severe delay cases.\n",
    "\n",
    "Interestingly, when we evaluated our top ten coefficients in Model 3 when had imputed the median we can see that Hourly Visibility was our second most significant coeffiient but in Model 4 where we had dropped the null values it didn't even make it into the top ten coefficients. This indicates that missing information can be meaningful and that this information may be missing for a reason. Dropping null values may reduce noise and imputation of missing values may introduce bias, which is a tradeoff we will need to consider in our imputation strategy.\n",
    "\n",
    "Overall, Model 4 produced slightly better regression and classification performance than Model 3, suggesting a marginal benefit from excluding incomplete records. However, the total absence of severe delay recall highlight ongoing limitations in modeling high-delay events and the need for enhanced feature engineering and class balancing in future iterations. Additionally our evaluation of the coefficients shows that we will need to be straegic and consistent in how we handle missing values across all our models and consider the tradeoffs carefully.\n",
    "\n",
    "#### Significance of Coefficients\n",
    "<table style=\"width:100%; border-collapse: collapse;\">\n",
    "    <tr>\n",
    "        <td style=\"width:50%; text-align:center; padding:10px; vertical-align:top;\">\n",
    "            <h3>Model 3 Coefficients</h3>\n",
    "            <img src='https://github.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/raw/main/report-images/Model3-Coefficients.png' style=\"width:100%; max-width:450px;\">\n",
    "        </td>\n",
    "        <td style=\"width:50%; text-align:center; padding:10px; vertical-align:top;\">\n",
    "            <h3>Model 4 Coefficients</h3>\n",
    "            <img src='https://github.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/raw/main/report-images/Model4-Coefficients.png' style=\"width:100%; max-width:450px;\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "#### Regression and Classification Metrics\n",
    "<table style=\"width:100%; border-collapse: collapse;\">\n",
    "    <tr>\n",
    "        <td style=\"width:50%; text-align:center; padding:10px; vertical-align:top;\">\n",
    "            <h3>Regression Metrics</h3>\n",
    "            <img src='https://github.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/raw/main/report-images/Emily-Model4-RegressionMetrics.png' style=\"width:100%; max-width:450px;\">\n",
    "        </td>\n",
    "        <td style=\"width:50%; text-align:center; padding:10px; vertical-align:top;\">\n",
    "            <h3>Classification Metrics</h3>\n",
    "            <img src='https://github.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/raw/main/report-images/Emily-Model4-ClassificationMetrics.png' style=\"width:100%; max-width:450px;\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "# Experiment 3: Does a custom data set effect performance?\n",
    "For our third experiment we plan to reuse Model 3 from Experiment 2 which uses the 12 month data, utilizes cross-validation, imputed null data handling strategy and the provided default data set. We plan to compare this to Model 5 which also employs a 12 month, cross-validated, imputed null strategy but instead of using the default data set we will use a custom joined data set. In this experiment we plan to evaluate how well our customized data set performs against the default one although more experiments will likely be needed to assess this in future models due to the limited number of features in these baseline models. \n",
    "\n",
    "| Experiment ID | Pipeline | Features Used | Model Details | Data Leakage |\n",
    "|----------------|-----------|----------------|---------------|---------------|\n",
    "| Model 3 | Cross-Validation/12-month/Data-Imputed/Default-Data | Feature Families: Temporal, Airport, Weather, Flight  | 12 month default data set, using cross-validation, imputed null, default data | Weather data imputed median |\n",
    "| Model 5 | Cross-Validation/12-month/Data-Imputed/Custom-Data | Feature Families: Temporal, Airport, Weather, Flight  | 12 month default data set, using cross-validation, imputed null, custom data | Weather data imputed median |\n",
    "\n",
    "## Model 5 - Cross-Validation/12-month/Data-Imputed/Custom-Data\n",
    "Model 5 extends the cross‑validated, time‑aware framework by running an expanding‑window CV on the full 12‑month horizon, but now using the custom 1‑year joined dataset rather than the default OTPW folds, while still enforcing the “train on past months, validate/test on future months” constraint to limit temporal data leakage. Like Model 3, it uses an imputed‑null strategy for numerical features (imputing the median for missing weather/distance values) and treats categorical features via StringIndexer + OneHotEncoder, so differences in performance can be attributed primarily to the underlying data source rather than a different modeling pipeline. This makes Model 5 a production‑style baseline on the enriched dataset, aimed at answering whether the custom join adds signal over the default data.\n",
    "\n",
    "- **Cluster Size:**\n",
    "| Resource            | Value   | Calculation                                      |\n",
    "|---------------------|---------|--------------------------------------------------|\n",
    "| Total CPUs (cores)  | 8       | 2 executors × 4 cores/executor                   |\n",
    "| Total RAM           | ~4.4 GB | 2 executors × 2218 MB/executor ≈ 4436 MB ≈ 4.4 GB |\n",
    "| Executors           | 2       | Worker processes                                 |\n",
    "| Cores per executor  | 4       | Parallel tasks per executor (estimated)          |\n",
    "| RAM per executor    | 2.2 GB  | Memory available to each executor                |\n",
    "- **Build Time for Model 5:**  6m 42s runtime\n",
    "\n",
    "\n",
    "### Pipeline Overview\n",
    "Model 5 takes the custom 2015 joined flights–weather dataset, selects baseline temporal, airport, flight, and weather features, cleans the numeric weather and distance columns, and imputes missing numerical values with the median. It then encodes categorical features using StringIndexer + OneHotEncoder, assembles the imputed numeric and encoded categorical features into a single feature vector, standardizes them with StandardScaler, and fits an unregularized linear regression model on dep_delay. The pipeline is evaluated with time‑aware expanding‑window cross‑validation over months (training on earlier months and validating/testing on later months), producing both regression metrics and operational classification metrics (OTPA, SDDR) on validation folds and a held‑out test month.\n",
    "\n",
    "<img src='https://github.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/raw/main/report-images/Emily-Model5-Pipeline_Data_imputed.png' style=\"width:100%;\">\n",
    "\n",
    "### Model 5 Results\n",
    "Model 5 which remains a weak but fairly consistent linear predictor of departure delay. Validation RMSE ranges from about 31–38 minutes (mean ≈ 34.1, MAE ≈ 18.2) with near‑zero mean R² (≈ −0.005), and the held‑out test RMSE increases to ≈ 43 minutes (MAE ≈ 21.4, R² ≈ 0.017), indicating that the model generalizes moderately worse to the final test month. From a classification standpoint, OTPA accuracy on the held‑out test is ≈ 0.67, so the model is reasonably good at separating on‑time (<15 min) from delayed flights, but it effectively fails to detect severe delays (SDDR recall = 0) and only achieves ~0.26 overall 4‑bucket accuracy, reflecting difficulty in assigning flights to the correct delay‑severity bucket.\n",
    "\n",
    "#### Regression and Classification Metrics\n",
    "<table style=\"width:100%; border-collapse: collapse;\">\n",
    "    <tr>\n",
    "        <td style=\"width:50%; text-align:center; padding:10px; vertical-align:top;\">\n",
    "            <h3>Regression Metrics</h3>\n",
    "            <img src='https://github.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/raw/main/report-images/Emily-Model5-2015-Regression.png' style=\"width:100%; max-width:450px;\">\n",
    "        </td>\n",
    "        <td style=\"width:50%; text-align:center; padding:10px; vertical-align:top;\">\n",
    "            <h3>Classification Metrics</h3>\n",
    "            <img src='https://github.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/raw/main/report-images/Emily-Model5-2015-ClassifcationMetrics.png' style=\"width:100%; max-width:450px;\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "### Experiment 3 Results\n",
    "Model 3 (default data) achieved a mean validation RMSE of 35.86 (test RMSE 35.96) and mean MAE of 19.09 (test MAE 18.29), with an R² near zero. Model 5 (custom data) performed similarly during validation (mean RMSE 34.10, mean MAE 18.16) but degraded significantly on the held-out test set (test RMSE 43.24, test MAE 21.43, R² = 0.0171), suggesting potential issues with generalization and feature mismatch.\n",
    "\n",
    "For both models, on-time performance prediction was moderate: Model 3 achieved 0.6952 OTPA accuracy  versus Model 5’s 0.6702 OTPA accuracy on test. Neither model could reliably identify severely delayed flights (SDDR ≈ 0 for both), and overall 4-bucket classification accuracy remained low (Model 3: 0.2508, Model 5: 0.2585 test).\n",
    "\n",
    "Overall,our custom data set needs refinement to address potential class imbalances, data quality and strategic imputation strategies in order to improve our models performance. In this experiment we see that the quality of data going into our models is going to be as impoartant if not more so than the models themselves and we'll need to refine our data strategy going forward.\n",
    "\n",
    "## Results and Discussion of Results\n",
    "### Results Summary\n",
    "## Model 6 - No-Cross-Validation/3/4Train-1/4Test-Split/12-month/Imputed-Null/Default-Data\n",
    "Model 6 was trained on the first nine months of the 12‑month OTPW‑2015 flights–weather dataset and evaluated on the final three months as a blind hold‑out test set, preserving the natural temporal ordering of the data. The pipeline mirrors Model 3’s median‑imputation strategy on the 10 baseline temporal, airport, flight, and weather features, followed by categorical encoding, feature assembly, standardization, and an unregularized linear regression model on DEP_DELAY. This configuration serves as our last linear regression baseline on the OTPW 12‑month dataset without cross‑validation, providing directly comparable train vs. blind‑test performance for downstream model comparisons.\n",
    "\n",
    "| Experiment ID | Pipeline | Features Used | Model Details | Data Leakage |\n",
    "|----------------|-----------|----------------|---------------|---------------|\n",
    "| Model 6 | No-Cross-Validation/3/4Train-1/4Test-Split/12-month/Imputed-Null/Default-Data | Feature Families: Temporal, Airport, Weather, Flight  | 12 month OTPW data set (2015), train/test split (.75/.25), imputed null | Weather data imputed median |\n",
    "\n",
    "- **Cluster Size:**\n",
    "| Resource            | Value   | Calculation                                      |\n",
    "|---------------------|---------|--------------------------------------------------|\n",
    "| Total CPUs (cores)  | 8       | 2 executors × 4 cores/executor                   |\n",
    "| Total RAM           | ~4.4 GB | 2 executors × 2218 MB/executor ≈ 4436 MB ≈ 4.4 GB |\n",
    "| Executors           | 2       | Worker processes                                 |\n",
    "| Cores per executor  | 4       | Parallel tasks per executor (estimated)          |\n",
    "| RAM per executor    | 2.2 GB  | Memory available to each executor                |\n",
    "- **Build Time for Model 5:** 10m 13s  runtime\n",
    "\n",
    "###Pipeline Overview\n",
    "Model 6 uses the 12‑month OTPW‑2015 flights dataset (converted to Parquet) with a time‑based split, training on months 1–9 and blind‑testing on months 10–12 to respect temporal order. The pipeline cleans the DEP_DELAY label, selects 10 baseline temporal, airport, flight, and weather features, and converts numeric weather fields to DoubleType, then applies median imputation to HourlyWindSpeed, HourlyVisibility, HourlyPrecipitation, and DISTANCE. Categorical variables (DAY_OF_WEEK, MONTH, DEP_TIME_BLK, ORIGIN, DEST, OP_UNIQUE_CARRIER) are string‑cleaned, indexed with StringIndexer, one‑hot encoded, and combined with the imputed numeric features via VectorAssembler, then standardized with StandardScaler. A non‑regularized linear regression model is trained on this feature vector, and we report regression metrics (RMSE, MAE, R², MSE) and classification metrics (OTPA, SDDR) for both the training (months 1–9) and blind test (months 10–12) sets, along with coefficient‑based feature importance for interpretability.\n",
    "\n",
    "<img src='https://github.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/raw/main/report-images/model6-pipeline-updated.png' style=\"width:100%;\">\n",
    "\n",
    "#### Regression and Classification Metrics\n",
    "<table style=\"width:100%; border-collapse: collapse;\">\n",
    "    <tr>\n",
    "        <td style=\"width:50%; text-align:center; padding:10px; vertical-align:top;\">\n",
    "            <h3>Regression Metrics</h3>\n",
    "            <img src='https://github.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/raw/main/report-images/Emily-Model6-RegressionMetrics.png' style=\"width:100%; max-width:450px;\">\n",
    "        </td>\n",
    "        <td style=\"width:50%; text-align:center; padding:10px; vertical-align:top;\">\n",
    "            <h3>Classification Metrics</h3>\n",
    "            <img src='https://github.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/raw/main/report-images/Model6-ClassificationMetricsFinal.png' style=\"width:100%; max-width:450px;\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "#### Significance of Coefficients\n",
    "<table style=\"width:100%; border-collapse: collapse;\">\n",
    "    <tr>\n",
    "        <td style=\"width:50%; text-align:center; padding:10px; vertical-align:top;\">\n",
    "            <h3>Model 3 Coefficients</h3>\n",
    "            <img src='https://github.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/raw/main/report-images/Model6-Coefficients.png' style=\"width:100%; max-width:450px;\">\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "### Model 6 Results\n",
    "On the training data (months 1–9), the model achieved an RMSE of 36.60 minutes (MAE = 18.32, R² = 0.0381). Performance on the blind test set (months 10–12) was comparable, with RMSE = 36.00 minutes (MAE = 17.87, R² = 0.0133). The minimal degradation between training and test sets indicates relatively stable generalization, though the low R² values confirm the limited explanatory power of linear regression in capturing complex delay patterns inherent in flight operations and weather interactions.\n",
    "\n",
    "When evaluated as a binary classification problem for on‑time departure (<15 min delay), Model 6 attained an on‑time prediction accuracy (OTPA) of 0.7278 on training and 0.7231 on test data, with test precision = 0.8686 and recall = 0.7896 (F1 = 0.8272). These results suggest that the linear model effectively distinguishes broadly between on‑time and delayed flights but remains biased toward the majority class, with relatively high precision but moderate recall.The Severe Delay Detection Rate (SDDR), focused on flights delayed by ≥60 minutes, was extremely low (recall = 0.0007 train, 0.0002 test), reflecting the model’s inability to identify rare extreme events within highly imbalanced delay distributions. \n",
    "\n",
    "Coefficient magnitudes indicated that time‑of‑departure blocks and weather visibility were the strongest predictors of departure delay. Early morning departure windows (e.g., 06:00–08:00) were associated with lower delays, while evening hours (17:00–20:00) corresponded to increased delay likelihood. Seasonal indicators such as June and September also exhibited moderate influence, aligning with known temporal variability in flight and weather conditions. Overall, Model 6 serves as a stable linear baseline within the OTPW 12‑month framework. Despite limited detection of severe delays, it maintains consistent variance across training and blind test sets. \n",
    "\n",
    "## Discussion\n",
    "Evaluating these models based on the performance of their test sets, we can compare how well these models and experiments are able to generalize on unseen test data in terms of regression metics and classification metrics. Across the experiments, several consistent findings emerged regarding model stability, data handling, and dataset composition.\n",
    "\n",
    "####Regression Metrics of the Test Sets\n",
    "| Model         |  rmse  | mae | r2 | mse |\n",
    "|---------------------|---------|-----------|-----------|-----------|\n",
    "| Model 1 - No-Cross-Validation/3-month/Data-Imputed | 37.1695 | 18.7141 | .0381 | 1381.5691 |\n",
    "| Model 2 - Cross-Validation/3-month/Data-Imputed | 34.2761 | 17.619 | .014 | 1174.8532 |\n",
    "| Model 3 - Cross-Validation/12-month/Data-Imputed/Default-Data| 35.9611 | 18.293 | .0077 | 1293.2014 |\n",
    "| Model 4 - Cross-Validation/12-month/Dropped-Null/Default-Data | 34.9891 | 17.7445 | .0034  | 1224.2345 |\n",
    "| Model 5 - Cross-Validation/12-month/Data-Imputed/Custom-Data | 43.2399 | 21.4274 | .0171 | 1869.6893|\n",
    "| Model 6 - No-Cross-Validation/3/4Train-1/4Test-Split/12-month/Imputed-Null/Default-Data | 36.003 | 17.867 | .0133 | 1296.2151 |\n",
    "\n",
    "<img src='https://github.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/raw/main/report-images/RMSE.png' style=\"width:100%;\">\n",
    "\n",
    "####Classification Metrics of the Test Sets\n",
    "| Model         |  otpa_accuracy  | otpa_f1 | sddr_recall | bucket_accuracy |\n",
    "|---------------------|---------|-----------|-----------|-----------|\n",
    "| Model 1 - No-Cross-Validation/3-month/Data-Imputed | .7162 | .817 | 0 | .2824 |\n",
    "| Model 2 - Cross-Validation/3-month/Data-Imputed | .7086 | .8163 | 0 | .2736 |\n",
    "| Model 3 - Cross-Validation/12-month/Data-Imputed/Default-Data| .6952 | .8038 | 0003 | .2508 |\n",
    "| Model 4 - Cross-Validation/12-month/Dropped-Null/Default-Data | .7137 | .8202 | 0 | .2497 |\n",
    "| Model 5 - Cross-Validation/12-month/Data-Imputed/Custom-Data | .6702 | .7782 | 0 | .2585 |\n",
    "| Model 6 - No-Cross-Validation/3/4Train-1/4Test-Split/12-month/Imputed-Null/Default-Data | .7231 | .8272 | .0002 | .2578 |\n",
    "\n",
    "<table style=\"width:100%; border-collapse: collapse;\">\n",
    "    <tr>\n",
    "        <td style=\"width:50%; text-align:center; padding:10px; vertical-align:top;\">\n",
    "            <img src='https://github.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/raw/main/report-images/OTPA.png' style=\"width:100%; max-width:450px;\">\n",
    "        </td>\n",
    "        <td style=\"width:50%; text-align:center; padding:10px; vertical-align:top;\">\n",
    "            <img src='https://github.com/Connor-Watson-Berkeley/flight-departure-delay-predictive-modeling/raw/main/report-images/SDDR.png' style=\"width:100%; max-width:450px;\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "The first experiment examined whether introducing cross-validation meaningfully altered model performance by comparing two otherwise identical pipelines that differed only in their validation strategy (simple hold‑out vs. cross‑validation). Incorporating cross-validation led to a modest gain in regression performance, with slightly lower lower RMSE and MSE values and evidence of reduced data leakage, while classification outcomes, particularly for on‑time vs. delayed prediction, remained largely unchanged. Overall, this suggests that cross-validation helps stabilize regression performance rather than substantially enhancing predictive capability. Cross-validation does not, on its own, resolve the broader limitations of the baseline linear model but it does help respect temporal relationships.\n",
    "\n",
    "In Experiment 2, the comparison between imputation and dropped-null strategies revealed that removing incomplete records slightly improved both regression and classification metrics. The dropped-null models yielded a lower RMSE and higher OTPA accuracy relative to the imputed models, implying that imputed values may have introduced minor noise or bias. This supports the importance of data quality over quantity in this prediction context, especially when missingness patterns may not be random. However, both approaches continued to exhibit near‑zero recall for severe delays, highlighting that missing‑value strategy alone is insufficient to capture rare but operationally important events. This suggests dropping nulls reduced noise without substantially affecting model generalization, highlighting the need for a strategic and consistent approach to imputing missing values across our pipelines. This experiment was important in showing that missing data is meaningful and we need to incorporate a data imputation strategy that is well informed and consistent.\n",
    "\n",
    "The third experiment investigated whether augmenting the default dataset with additional joined sources would enhance performance, holding the model family, time window, and cross-validation setup fixed. Despite comparable validation error, the custom‑joined dataset produced substantially worse generalization on the held‑out test set and did not materially improve classification metrics, including severe delay detection. This experiment reminds us that in order for this new data set to be useful we will want to engineer meaningful features, clean and handle discrepancies and think carefully about what modification or imputations will need to be made to this new data set.Another critical implication is the persistent challenge in predicting rare, high-impact events (severe delays), which all models did poorly. This signals a need for deliberate focus on class imbalance, targeted feature engineering, and more complex models that can learn more non-linear features. These findings strongly emphasize the importance of thoughtful feature selection, quality data integration, and model complexity in flight delay prediction domains. Simply increasing the volume or types of data, such as adding weather records or additional context, does not guarantee better predictive performance within linear or basic ML frameworks. \n",
    "\n",
    "####Key Findings\n",
    "Among all tested models, Model 4 (Cross-Validation/12-month/Dropped-Null/Default-Data) emerged as the best-performing configuration on paper. It achieved the lowest test RMSE (34.99) and the highest OTPA accuracy (0.7137) among the evaluated models. Its consistent fold performance indicated a reliable and balanced trade-off between predictive accuracy and model stability. Despite this Model 3 (Model 3 - Cross-Validation/12-month/Data-Imputed/Default-Data) maintained the preservation of the data set through an imputation strategy that represented missing values in a meaningful way through the coefficients while maintaining performance metrics RMSE (35.961) and OTPA accuracy (0.6952) and respecting temporal ordering through cross-validation. In practice our model needs to know how to handle and evaluate missing values which is why we've selected Model 3 as our final baseline.\n",
    "\n",
    "Across all experiments, the different modeling pipelines produced broadly similar performance, indicating that changes in train–test splits, imputation strategies, datasets or model choices within this feature family did not substantially improve predictive accuracy. Collectively, the models captured some high‑level delay patterns but remained weak forecasters of exact delay magnitudes and especially of rare severe delays, which motivates a shift in focus from tuning current configurations to improving the underlying data and feature space.\n",
    "\n",
    "All models struggled with heavily imbalanced delay classes, particularly for rare but operationally critical severe delays, underscoring the need for explicit imbalance handling. Temporal variables (month, departure hour block, and related scheduling features) consistently dominated feature importance rankings, indicating that when a flight departs is more predictive than many other currently available attributes. Early‑morning departures tended to show shorter delays, while evening flights were more frequently delayed and higher visibility was associated with reduced delays.Future experiments should improve data cleaning and imputation procedures, incorporate richer operational and weather data, engineer more domain‑specific features and explicitly tackle class imbalance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbb8a9dd-5780-4a0d-a407-727a207debf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "Accurate flight delay prediction enables proactive operational decisions across the aviation ecosystem, reducing uncertainty for airlines, passengers, and air traffic control. We hypothesize that machine learning pipelines with custom feature engineering can accurately forecast departure delays at a 2-hour prediction horizon.\n",
    "\n",
    "Phase 2 established a baseline linear regression model achieving test set RMSE of 35.96 minutes and MAE of 18.29 minutes. Key contributions include a 6-stage distributed feature engineering pipeline, expanding window cross-validation respecting temporal ordering, and log transformation addressing departure delay skew in our heavy-tailed distribution. While R² near zero signals room for improvement, this baseline provides a critical benchmark.\n",
    "\n",
    "Future work targets three tracks: advanced feature engineering (temporal Prophet components, network graph metrics, external FAA OPSNET data), non-linear models (XGBoost, deep neural networks, ensembles), and rigorous data leakage controls ensuring production readiness. Building systematically on our baseline, we aim to achieve production-grade predictive accuracy for real world deployment.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a528f27c-8398-4c85-8087-f56029e7ccdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## References\n",
    "#### Code Notebooks\n",
    "Emily - Linear Baseline: https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/1047316347250763?o=4021782157704243\n",
    "\n",
    "Sid - Custom Join: https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/1792055957776737?o=4021782157704243 \n",
    "\n",
    "Paul - Cross Validation: https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/2421064614439957?o=4021782157704243#command/7250714370589247\n",
    "\n",
    "Connor - Homework 5: https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/1739154845298010?o=4021782157704243\n",
    "\n",
    "Indri - Raw Table Joins: https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/1792055957780480?o=4021782157704243\n",
    "\n",
    "Indri - EDA 1-year: https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/1792055957777159?o=4021782157704243\n",
    "\n",
    "Indri - Missing Analysis 1-year: https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/1792055957782098?o=4021782157704243"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49b6c846-7b0f-4975-972e-3e4f04fbfc90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Appendix\n",
    "Regression Metrics\n",
    "| Metric | Formula | Description |\n",
    "|--------|---------|-------------|\n",
    "| RMSE | $$\\mathrm{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2}$$ | Root Mean Squared Error - measures average prediction error magnitude in original units, penalizes large errors more heavily |\n",
    "| MAE | $$\\mathrm{MAE} = \\frac{1}{N} \\sum_{i=1}^{N} \\|y_i - \\hat{y}_i\\|$$ | Mean Absolute Error - measures average absolute prediction error, more robust to outliers than RMSE |\n",
    "| R² | $$R^2 = 1 - \\frac{\\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{N} (y_i - \\bar{y})^2}$$ | Coefficient of Determination - proportion of variance in the target variable explained by the model (0-1 scale) |\n",
    "| MSE | $$\\mathrm{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2$$ | Mean Squared Error - average of squared prediction errors, useful for optimization but less interpretable than RMSE |\n",
    "\n",
    "Classification Metrics\n",
    "| Metric | Formula | Description |\n",
    "|--------|---------|-------------|\n",
    "| **OTPA Accuracy** | $$\\text{OTPA} = \\frac{TP + TN}{TP + TN + FP + FN}$$ | Binary classification accuracy for on-time (<15 min) vs delayed (≥15 min) flights |\n",
    "| **OTPA Recall** | $$\\text{Recall} = \\frac{TP}{TP + FN}$$ | Proportion of actual on-time flights correctly identified |\n",
    "| **SDDR (Recall)** | $$\\text{SDDR} = \\frac{TP_{severe}}{TP_{severe} + FN_{severe}}$$ | Proportion of severe delays (≥60 min) correctly identified - critical for operational planning |\n",
    "| **Bucket Accuracy** | $$\\text{Accuracy} = \\frac{\\text{Correct Predictions}}{\\text{Total Predictions}}$$ | Overall accuracy across all 4 delay buckets (Early, OnTime, Delayed, Severe) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f965448-729d-4c53-9af6-d47f2a458084",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Full Data Dictionary\n",
    "\n",
    "| Column Name                 | Data Type   | Null Percentage | Description                                                                          | Data Source   | Notes            |\n",
    "| --------------------------- | ----------- | --------------- | ------------------------------------------------------------------------------------ | ------------- | ---------------- |\n",
    "| WEATHER_DELAY               | Double      | 79.58594597     | Weather delay in minutes                                                             | BTS Flight    | outcome variable |\n",
    "| LATE_AIRCRAFT_DELAY         | Double      | 79.58594597     | Late aircraft delay in minutes                                                       | BTS Flight    | outcome variable |\n",
    "| NAS_DELAY                   | Double      | 79.58594597     | National Air System delay in minutes                                                 | BTS Flight    | outcome variable |\n",
    "| SECURITY_DELAY              | Double      | 79.58594597     | Security delay in minutes                                                            | BTS Flight    | outcome variable |\n",
    "| CARRIER_DELAY               | Double      | 79.58594597     | Carrier delay in minutes (within airline's control)                                  | BTS Flight    | outcome variable |                  |\n",
    "| HourlyPressureChange        | Double      | 66.36082157     | Pressure change in hectopascals over 3 hours                                         | NOAA Weather  |                  |\n",
    "| HourlyPrecipitation         | Double      | 11.34809468     | Liquid precipitation depth in mm (scaled by 10)                                      | NOAA Weather  |                  |\n",
    "| HourlySeaLevelPressure      | Double      | 10.95048178     | Air pressure at sea level in hectopascals (scaled by 10)                             | NOAA Weather  |                  |\n",
    "| HourlyAltimeterSetting      | Double      | 4.510965396     | Altimeter setting for aviation                                                       | NOAA Weather  |                  |\n",
    "| ARR_DELAY                   | Double      | 3.325405338     | Arrival delay in minutes - negative = early arrival                                  | BTS Flight    | outcome variable |\n",
    "| AIR_TIME                    | Double      | 3.325405338     | Flight time in minutes (wheels off to wheels on)                                     | BTS Flight    | outcome variable |\n",
    "| ACTUAL_ELAPSED_TIME         | Double      | 3.325405338     | Actual elapsed time of flight in minutes                                             | BTS Flight    | outcome variable |\n",
    "| ARR_DELAY_NEW               | Double      | 3.325405338     | Arrival delay in minutes - early arrivals set to 0                                   | BTS Flight    | outcome variable |\n",
    "| ARR_DEL15                   | Integer     | 3.325405338     | Arrival delay indicator: 15+ minutes (1=Yes, 0=No)                                   | BTS Flight    | outcome variable |\n",
    "| ARR_DELAY_GROUP             | Integer     | 3.325405338     | Arrival delay intervals in 15-minute increments                                      | BTS Flight    | outcome variable |\n",
    "| WHEELS_ON                   | Integer     | 3.162349798     | Wheels on time (local time: HHMM)                                                    | BTS Flight    |                  |\n",
    "| ARR_TIME                    | Integer     | 3.162349798     | Actual arrival time (local time: HHMM)                                               | BTS Flight    | outcome variable |\n",
    "| TAXI_IN                     | Double      | 3.162349798     | Taxi in time in minutes (wheels on to gate)                                          | BTS Flight    |                  |\n",
    "| TAXI_OUT                    | Double      | 3.076932957     | Taxi out time in minutes (gate to wheels off)                                        | BTS Flight    |                  |\n",
    "| WHEELS_OFF                  | Integer     | 3.076932957     | Wheels off time (local time: HHMM)                                                   | BTS Flight    |                  |\n",
    "| DEP_DELAY_NEW               | Double      | 3.018918011     | Departure delay in minutes - early departures set to 0                               | BTS Flight    | outcome variable |\n",
    "| DEP_DELAY_GROUP             | Integer     | 3.018918011     | Departure delay intervals in 15-minute increments                                    | BTS Flight    | outcome variable |\n",
    "| DEP_DELAY                   | Double      | 3.018918011     | Departure delay in minutes - negative = early departure                              | BTS Flight    | outcome variable |\n",
    "| DEP_DEL15                   | Integer     | 3.018918011     | Departure delay indicator: 15+ minutes (1=Yes, 0=No)                                 | BTS Flight    | outcome variable |\n",
    "| DEP_TIME                    | Integer     | 3.018918011     | Actual departure time (local time: HHMM)                                             | BTS Flight    | outcome variable |\n",
    "| HourlySkyConditions         | String      | 2.346429869     | Cloud coverage and sky condition codes                                               | NOAA Weather  |                  |\n",
    "| HourlyWetBulbTemperature    | Double      | 0.73200163      | Wet bulb temperature in °C (scaled by 10)                                           | NOAA Weather  |                  |\n",
    "| HourlyStationPressure       | Double      | 0.6867599616    | Station pressure in hectopascals                                                     | NOAA Weather  |                  |\n",
    "| TAIL_NUM                    | String      | 0.5842169374    | Aircraft tail number                                                                 | BTS Flight    |                  |\n",
    "| HourlyWindDirection         | Double      | 0.4332210855    | Wind direction angle in degrees (1-360, 999=missing)                                 | NOAA Weather  |                  |\n",
    "| HourlyRelativeHumidity      | Double      | 0.3368863028    | Relative humidity percentage                                                         | NOAA Weather  |                  |\n",
    "| HourlyWindSpeed             | Double      | 0.3315343705    | Wind speed rate in m/s (scaled by 10)                                                | NOAA Weather  |                  |\n",
    "| HourlyDewPointTemperature   | Double      | 0.3263251563    | Dew point temperature in °C (scaled by 10)                                           | NOAA Weather  |                  |\n",
    "| HourlyDryBulbTemperature    | Double      | 0.3180475009    | Air temperature in °C (scaled by 10)                                                 | NOAA Weather  |                  |\n",
    "| HourlyVisibility            | Double      | 0.2916446345    | Horizontal visibility distance in meters                                             | NOAA Weather  |                  |\n",
    "| REM                         | String      | 0.040246531     | Remarks - additional plain language or coded weather information                     | NOAA Weather  |                  |\n",
    "| CRS_ELAPSED_TIME            | Double      | 0.000142718     | CRS (scheduled) elapsed time of flight in minutes                                    | BTS Flight    |                  |\n",
    "| origin_station_lon          | Double      | 0               | Longitude of weather station for origin                                              | Airport Codes |                  |\n",
    "| SOURCE                      | String      | 0               | Data source flag indicating origin                                                   | NOAA Weather  |                  |\n",
    "| CRS_DEP_TIME                | Integer     | 0               | CRS (scheduled) departure time (local time: HHMM)                                    | BTS Flight    |                  |\n",
    "| ORIGIN_WAC                  | Integer     | 0               | Origin airport World Area Code                                                       | BTS Flight    |                  |\n",
    "| CRS_ARR_TIME                | Integer     | 0               | CRS (scheduled) arrival time (local time: HHMM)                                      | BTS Flight    |                  |\n",
    "| dest_station_lon            | Double      | 0               | Longitude of weather station for destination                                         | Airport Codes |                  |\n",
    "| CANCELLED                   | Integer     | 0               | Cancelled flight indicator (1=Yes, 0=No)                                             | BTS Flight    | outcome variable |\n",
    "| origin_region               | String      | 0               | Origin airport region/state code                                                     | Airport Codes |                  |\n",
    "| DEST_WAC                    | Integer     | 0               | Destination airport World Area Code                                                  | BTS Flight    |                  |\n",
    "| LATITUDE                    | Double      | 0               | Latitude coordinate in angular degrees (scaled by 1000)                              | NOAA Weather  |                  |\n",
    "| DEP_TIME_BLK                | String      | 0               | CRS departure time block in hourly intervals                                         | BTS Flight    |                  |\n",
    "| DAY_OF_MONTH                | Integer     | 0               | Day of month                                                                         | BTS Flight    |                  |\n",
    "| DIVERTED                    | Integer     | 0               | Diverted flight indicator (1=Yes, 0=No)                                              | BTS Flight    |                  |\n",
    "| NAME                        | String      | 0               | Weather station call letter identifier                                               | NOAA Weather  |                  |\n",
    "| ARR_TIME_BLK                | String      | 0               | CRS arrival time block in hourly intervals                                           | BTS Flight    |                  |\n",
    "| dest_airport_name           | String      | 0               | Full name of destination airport                                                     | Airport Codes |                  |\n",
    "| DEST_STATE_NM               | String      | 0               | Destination airport state name                                                       | BTS Flight    |                  |\n",
    "| ELEVATION                   | Double      | 0               | Elevation relative to Mean Sea Level in meters                                       | NOAA Weather  |                  |\n",
    "| DEST                        | String      | 0               | Destination airport code (IATA)                                                      | BTS Flight    |                  |\n",
    "| LONGITUDE                   | Double      | 0               | Longitude coordinate in angular degrees (scaled by 1000)                             | NOAA Weather  |                  |\n",
    "| origin_station_lat          | Double      | 0               | Latitude of weather station for origin                                               | Airport Codes |                  |\n",
    "| dest_station_lat            | Double      | 0               | Latitude of weather station for destination                                          | Airport Codes |                  |\n",
    "| FL_DATE                     | String/Date | 0               | Flight date (YYYYMMDD)                                                               | BTS Flight    |                  |\n",
    "| sched_depart_date_time      | Timestamp   | 0               | Scheduled departure date-time in local timezone                                      | Derived       |                  |\n",
    "| dest_station_name           | String      | 0               | Weather station name nearest to destination                                          | Airport Codes |                  |\n",
    "| DATE                        | String      | 0               | Geophysical-point-observation date-time (YYYYMMDD HHMM)                              | NOAA Weather  |                  |\n",
    "| DEST_AIRPORT_ID             | Integer     | 0               | Destination airport ID - unique key assigned by DOT                                  | BTS Flight    |                  |\n",
    "| dest_airport_lat            | Double      | 0               | Latitude of destination airport                                                      | Airport Codes |                  |\n",
    "| dest_icao                   | String      | 0               | Destination airport 4-letter ICAO code                                               | Airport Codes |                  |\n",
    "| dest_type                   | String      | 0               | Destination airport type                                                             | Airport Codes |                  |\n",
    "| OP_CARRIER_FL_NUM           | Integer     | 0               | Flight number                                                                        | BTS Flight    |                  |\n",
    "| dest_airport_lon            | Double      | 0               | Longitude of destination airport                                                     | Airport Codes |                  |\n",
    "| origin_icao                 | String      | 0               | Origin airport 4-letter ICAO code                                                    | Airport Codes |                  |\n",
    "| dest_station_dis            | Double      | 0               | Distance between destination airport and weather station                             | Airport Codes |                  |\n",
    "| DEST_CITY_MARKET_ID         | Integer     | 0               | Destination city market ID                                                           | BTS Flight    |                  |\n",
    "| REPORT_TYPE                 | String      | 0               | Type of observation (METAR, SYNOP, AUTO, etc.)                                       | NOAA Weather  |                  |\n",
    "| dest_station_id             | String      | 0               | NOAA weather station identifier for destination                                      | Airport Codes |                  |\n",
    "| STATION                     | String      | 0               | Fixed-weather-station USAF master station catalog identifier                         | NOAA Weather  |                  |\n",
    "| QUARTER                     | Integer     | 0               | Quarter (1-4) of flight                                                              | BTS Flight    |                  |\n",
    "| dest_region                 | String      | 0               | Destination airport region/state code                                                | Airport Codes |                  |\n",
    "| origin_type                 | String      | 0               | Origin airport type                                                                  | Airport Codes |                  |\n",
    "| two_hours_prior_depart_UTC  | Timestamp   | 0               | Two hours prior to scheduled departure in UTC                                        | Derived       |                  |\n",
    "| OP_CARRIER                  | String      | 0               | IATA code for carrier - not always unique over time                                  | BTS Flight    |                  |\n",
    "| sched_depart_date_time_UTC  | Timestamp   | 0               | Scheduled departure date-time in UTC                                                 | Derived       |                  |\n",
    "| origin_airport_lat          | Double      | 0               | Latitude of origin airport                                                           | Airport Codes |                  |\n",
    "| four_hours_prior_depart_UTC | Timestamp   | 0               | Four hours prior to scheduled departure in UTC                                       | Derived       |                  |\n",
    "| OP_UNIQUE_CARRIER           | String      | 0               | Unique carrier code - when same code used by multiple carriers, numeric suffix added | BTS Flight    |                  |\n",
    "| origin_station_name         | String      | 0               | Weather station name nearest to origin airport                                       | Airport Codes |                  |\n",
    "| ORIGIN_CITY_NAME            | String      | 0               | Origin airport city name                                                             | BTS Flight    |                  |\n",
    "| DISTANCE_GROUP              | Integer     | 0               | Distance intervals in 250-mile increments                                            | BTS Flight    |                  |\n",
    "| MONTH                       | Integer     | 0               | Month of flight                                                                      | BTS Flight    |                  |\n",
    "| origin_station_dis          | Double      | 0               | Distance between origin airport and weather station                                  | Airport Codes |                  |\n",
    "| DEST_STATE_FIPS             | String      | 0               | Destination airport state FIPS code                                                  | BTS Flight    |                  |\n",
    "| dest_iata_code              | String      | 0               | Destination airport 3-letter IATA code                                               | Airport Codes |                  |\n",
    "| ORIGIN_AIRPORT_SEQ_ID       | Integer     | 0               | Origin airport sequence ID - unique key for time-specific airport info               | BTS Flight    |                  |\n",
    "| origin_airport_lon          | Double      | 0               | Longitude of origin airport                                                          | Airport Codes |                  |\n",
    "| ORIGIN_AIRPORT_ID           | Integer     | 0               | Origin airport ID - unique key assigned by DOT                                       | BTS Flight    |                  |\n",
    "| origin_airport_name         | String      | 0               | Full name of origin airport                                                          | Airport Codes |                  |\n",
    "| ORIGIN                      | String      | 0               | Origin airport code (IATA)                                                           | BTS Flight    |               |\n",
    "| origin_station_id           | String      | 0               | NOAA weather station identifier for origin airport                                   | Airport Codes |                  |\n",
    "| DEST_AIRPORT_SEQ_ID         | Integer     | 0               | Destination airport sequence ID                                                      | BTS Flight    |                  |\n",
    "| DISTANCE                    | Double      | 0               | Distance between airports in miles                                                   | BTS Flight    |                  |\n",
    "| ORIGIN_STATE_NM             | String      | 0               | Origin airport state name                                                            | BTS Flight    |                  |\n",
    "| origin_iata_code            | String      | 0               | Origin airport 3-letter IATA code                                                    | Airport Codes |               |\n",
    "| YEAR                        | Integer     | 0               | Year of flight                                                                       | BTS Flight    |                  |\n",
    "| DEST_CITY_NAME              | String      | 0               | Destination airport city name                                                        | BTS Flight    |                  |\n",
    "| OP_CARRIER_AIRLINE_ID       | Integer     | 0               | DOT identification number for unique airline/carrier                                 | BTS Flight    |                  |\n",
    "| ORIGIN_CITY_MARKET_ID       | Integer     | 0               | Origin city market ID - consolidates airports serving same city market               | BTS Flight    |                  |\n",
    "| ORIGIN_STATE_FIPS           | String      | 0               | Origin airport state FIPS code                                                       | BTS Flight    |                  |\n",
    "| DAY_OF_WEEK                 | Integer     | 0               | Day of week                                                                          | BTS Flight    |                  |\n",
    "| ORIGIN_STATE_ABR            | String      | 0               | Origin airport state code                                                            | BTS Flight    |                  |\n",
    "| DEST_STATE_ABR              | String      | 0               | Destination airport state code                                                       | BTS Flight    |                  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21d80bf5-d396-455a-b1e0-f3b379edb441",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Phase 2 Report - Team 4_2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
