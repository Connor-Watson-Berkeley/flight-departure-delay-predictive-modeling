{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "659f80f9-ded8-49bd-ae7b-64ddfd442f03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d44d7c44-babb-4a34-85ef-b920f0887aa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.autolog(disable=True)\n",
    "\n",
    "import importlib.util\n",
    "import sys\n",
    "\n",
    "# Load cv module directly from file path\n",
    "cv_path = \"/Workspace/Shared/Team 4_2/flight-departure-delay-predictive-modeling/notebooks/Cross Validator/cv.py\"\n",
    "spec = importlib.util.spec_from_file_location(\"cv\", cv_path)\n",
    "cv = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(cv)\n",
    "\n",
    "graph_features_path = \"/Workspace/Shared/Team 4_2/flight-departure-delay-predictive-modeling/notebooks/Feature Engineering/graph_features.py\"\n",
    "spec = importlib.util.spec_from_file_location(\"graph_features\", graph_features_path)\n",
    "graph_features = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(graph_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb776288-d50a-4686-b7cd-e22e8782681e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_loader = cv.FlightDelayDataLoader()\n",
    "data_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac521d0a-461c-4e42-b7f9-d5db97975788",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import Imputer, StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml import PipelineModel # Needed for the fitted stages\n",
    "\n",
    "# --- Feature Lists (from provided context) ---\n",
    "categorical_features = [\n",
    "    'day_of_week', 'op_carrier', 'origin', 'origin_state_abr', 'dest', \n",
    "    'dest_state_abr', 'dep_time_blk', 'arr_time_blk', 'day_of_month', 'month'\n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    'hourlyprecipitation', 'hourlysealevelpressure', 'hourlyaltimetersetting', \n",
    "    'hourlywetbulbtemperature', 'hourlystationpressure', 'hourlywinddirection', \n",
    "    'hourlyrelativehumidity', 'hourlywindspeed', 'hourlydewpointtemperature', \n",
    "    'hourlydrybulbtemperature', 'hourlyvisibility', 'crs_elapsed_time', \n",
    "    'distance', 'elevation'\n",
    "]\n",
    "\n",
    "lineage_features = [\n",
    "    'crs_elapsed_time', 'distance', 'elevation',\n",
    "]\n",
    "\n",
    "numerical_features_no_lineage = [\n",
    "    col for col in numerical_features if col not in lineage_features\n",
    "]\n",
    "\n",
    "# --- Helper Function for Feature Engineering Pipeline ---\n",
    "\n",
    "def create_feature_pipeline(categorical_cols, numerical_cols, unique_suffix):\n",
    "    \"\"\"\n",
    "    Creates a PySpark ML Pipeline for feature engineering up to VectorAssembler,\n",
    "    using a unique suffix for all intermediate columns to avoid conflicts.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use UNIQUE names for ALL intermediate columns based on the suffix\n",
    "    indexed_cols = [f\"{col}_INDEX_{unique_suffix}\" for col in categorical_cols]\n",
    "    vector_cols = [f\"{col}_VEC_{unique_suffix}\" for col in categorical_cols]\n",
    "    imputed_cols = [f\"{col}_IMPUTED_{unique_suffix}\" for col in numerical_cols]\n",
    "    \n",
    "    imputer = Imputer(\n",
    "        inputCols=numerical_cols,\n",
    "        outputCols=imputed_cols,\n",
    "        strategy=\"mean\"\n",
    "    )\n",
    "\n",
    "    indexer = StringIndexer(\n",
    "        inputCols=categorical_cols,\n",
    "        outputCols=indexed_cols,\n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "\n",
    "    encoder = OneHotEncoder(\n",
    "        inputCols=indexed_cols,\n",
    "        outputCols=vector_cols\n",
    "    )\n",
    "\n",
    "    assembler_input_cols = vector_cols + imputed_cols\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=assembler_input_cols,\n",
    "        # IMPORTANT: Output column is still 'features' because the regressor expects it\n",
    "        outputCol=\"features\", \n",
    "        handleInvalid=\"skip\"\n",
    "    )\n",
    "    \n",
    "    return Pipeline(stages=[imputer, indexer, encoder, assembler])\n",
    "\n",
    "# --- VotingEnsemble Class (The Corrected Estimator) ---\n",
    "\n",
    "class VotingEnsemble:\n",
    "    \"\"\"\n",
    "    A custom Estimator that combines predictions from two distinct \n",
    "    RandomForestRegressor models using simple averaging.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # 1. Feature Pipelines (Must be separate for different feature sets/unique column names)\n",
    "        # Suffix '_L' for Lineage, '_NL' for No Lineage\n",
    "        self.feature_pipe_lineage = create_feature_pipeline(\n",
    "            categorical_features, numerical_features, \"L\"\n",
    "        )\n",
    "        self.feature_pipe_no_lineage = create_feature_pipeline(\n",
    "            categorical_features, numerical_features_no_lineage, \"NL\"\n",
    "        )\n",
    "\n",
    "        # 2. Regressors (Define separate prediction columns)\n",
    "        self.regressor_lineage = RandomForestRegressor(\n",
    "            featuresCol=\"features\", labelCol=\"DEP_DELAY\", \n",
    "            predictionCol=\"pred_lineage\", numTrees=50, maxDepth=10\n",
    "        )\n",
    "        self.regressor_no_lineage = RandomForestRegressor(\n",
    "            featuresCol=\"features\", labelCol=\"DEP_DELAY\", \n",
    "            predictionCol=\"pred_no_lineage\", numTrees=50, maxDepth=10\n",
    "        )\n",
    "        \n",
    "        # Fitted objects\n",
    "        self.feature_model_lineage = None\n",
    "        self.feature_model_no_lineage = None\n",
    "        self.model_lineage = None\n",
    "        self.model_no_lineage = None\n",
    "\n",
    "    def fit(self, df):\n",
    "        \"\"\"\n",
    "        Fits both models using their respective feature pipelines.\n",
    "        \"\"\"\n",
    "        print(\"Fitting feature pipeline (with Lineage)...\")\n",
    "        # Fit and transform the training data using the Lineage feature set\n",
    "        self.feature_model_lineage = self.feature_pipe_lineage.fit(df)\n",
    "        df_lineage_features = self.feature_model_lineage.transform(df)\n",
    "\n",
    "        print(\"Fitting feature pipeline (without Lineage)...\")\n",
    "        # Fit and transform the training data using the No Lineage feature set\n",
    "        self.feature_model_no_lineage = self.feature_pipe_no_lineage.fit(df)\n",
    "        df_no_lineage_features = self.feature_model_no_lineage.transform(df)\n",
    "        \n",
    "        # 2. Fit Regressors on their respective feature sets\n",
    "        print(\"Fitting Regressor (with Lineage)...\")\n",
    "        self.model_lineage = self.regressor_lineage.fit(df_lineage_features)\n",
    "        \n",
    "        print(\"Fitting Regressor (without Lineage)...\")\n",
    "        self.model_no_lineage = self.regressor_no_lineage.fit(df_no_lineage_features)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        \"\"\"\n",
    "        Applies both fitted feature and model stages and computes the average \n",
    "        prediction.\n",
    "        \"\"\"\n",
    "        # Define key columns needed for joining predictions (excluding features/labels)\n",
    "        # This assumes your DataFrame has columns that uniquely identify a flight row.\n",
    "        # A common practice is to have an explicit ID column, but we use an \n",
    "        # approximate set here as a placeholder.\n",
    "        key_cols = [\n",
    "            'month', 'day_of_month', 'day_of_week', 'op_carrier', \n",
    "            'origin', 'dest', 'dep_time_blk', 'arr_time_blk'\n",
    "        ]\n",
    "\n",
    "        # --- Model 1: Lineage ---\n",
    "        # 1. Transform features\n",
    "        df_lineage_features = self.feature_model_lineage.transform(df)\n",
    "        # 2. Predict\n",
    "        df_pred_1 = self.model_lineage.transform(df_lineage_features)\n",
    "        \n",
    "        # --- Model 2: No Lineage ---\n",
    "        # 1. Transform features\n",
    "        df_no_lineage_features = self.feature_model_no_lineage.transform(df)\n",
    "        # 2. Predict\n",
    "        df_pred_2 = self.model_no_lineage.transform(df_no_lineage_features)\n",
    "        \n",
    "        # --- Ensemble ---\n",
    "        \n",
    "        # Extract required columns for joining\n",
    "        # We need the prediction column and the key columns for joining\n",
    "        df_pred_1_selected = df_pred_1.select(F.col('pred_lineage'), *key_cols)\n",
    "        df_pred_2_selected = df_pred_2.select(F.col('pred_no_lineage'), *key_cols)\n",
    "\n",
    "        # Join the two prediction columns\n",
    "        df_final = df_pred_1_selected.join(df_pred_2_selected, on=key_cols, how='inner') \\\n",
    "                                     .join(df.select(*key_cols, 'DEP_DELAY'), on=key_cols, how='inner') \n",
    "\n",
    "        # Compute the ensemble prediction (Averaging)\n",
    "        df_final = df_final.withColumn(\n",
    "            \"prediction\",\n",
    "            (F.col(\"pred_lineage\") + F.col(\"pred_no_lineage\")) / F.lit(2.0)\n",
    "        )\n",
    "        \n",
    "        # Final output for the CV framework: keep only the prediction, label, and key columns\n",
    "        return df_final.select(F.col(\"prediction\"), F.col(\"DEP_DELAY\"), *key_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f7a97cc-53b7-4cd6-b328-a145b4ff216d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import Imputer, StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml import Estimator, Transformer \n",
    "from pyspark.ml import PipelineModel # Needed for the fitted stages\n",
    "\n",
    "\n",
    "# --- Feature Lists (from provided context) ---\n",
    "# Define these outside the class to be accessible by helper functions\n",
    "categorical_features = [\n",
    "    'day_of_week', 'op_carrier', 'origin', 'origin_state_abr', 'dest', \n",
    "    'dest_state_abr', 'dep_time_blk', 'arr_time_blk', 'day_of_month', 'month'\n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    'hourlyprecipitation', 'hourlysealevelpressure', 'hourlyaltimetersetting', \n",
    "    'hourlywetbulbtemperature', 'hourlystationpressure', 'hourlywinddirection', \n",
    "    'hourlyrelativehumidity', 'hourlywindspeed', 'hourlydewpointtemperature', \n",
    "    'hourlydrybulbtemperature', 'hourlyvisibility', 'crs_elapsed_time', \n",
    "    'distance', 'elevation'\n",
    "]\n",
    "\n",
    "lineage_features = [\n",
    "    'crs_elapsed_time', 'distance', 'elevation', # Features used to define Lineage group\n",
    "]\n",
    "\n",
    "# Numerical features for the 'No Lineage' model (environmental/time features)\n",
    "numerical_features_no_lineage = [\n",
    "    col for col in numerical_features if col not in lineage_features\n",
    "]\n",
    "\n",
    "# --- Helper Function for Feature Engineering Pipeline ---\n",
    "\n",
    "def create_feature_pipeline(categorical_cols, numerical_cols, unique_suffix):\n",
    "    \"\"\"\n",
    "    Creates a PySpark ML Pipeline for feature engineering up to VectorAssembler,\n",
    "    using a unique suffix for all intermediate columns to avoid conflicts.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use UNIQUE names for ALL intermediate columns based on the suffix\n",
    "    indexed_cols = [f\"{col}_INDEX_{unique_suffix}\" for col in categorical_cols]\n",
    "    vector_cols = [f\"{col}_VEC_{unique_suffix}\" for col in categorical_cols]\n",
    "    imputed_cols = [f\"{col}_IMPUTED_{unique_suffix}\" for col in numerical_cols]\n",
    "    \n",
    "    imputer = Imputer(\n",
    "        inputCols=numerical_cols,\n",
    "        outputCols=imputed_cols,\n",
    "        strategy=\"mean\"\n",
    "    )\n",
    "\n",
    "    indexer = StringIndexer(\n",
    "        inputCols=categorical_cols,\n",
    "        outputCols=indexed_cols,\n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "\n",
    "    encoder = OneHotEncoder(\n",
    "        inputCols=indexed_cols,\n",
    "        outputCols=vector_cols\n",
    "    )\n",
    "\n",
    "    assembler_input_cols = vector_cols + imputed_cols\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=assembler_input_cols,\n",
    "        outputCol=\"features\", # Output column is still 'features' as regressor expects it\n",
    "        handleInvalid=\"skip\"\n",
    "    )\n",
    "    \n",
    "    return Pipeline(stages=[imputer, indexer, encoder, assembler])\n",
    "\n",
    "\n",
    "# --- VotingEnsemble Class (The Corrected Estimator) ---\n",
    "\n",
    "class VotingEnsemble:\n",
    "    \"\"\"\n",
    "    A custom Estimator that combines predictions from two distinct \n",
    "    RandomForestRegressor models using simple averaging.\n",
    "    \n",
    "    NOTE: The internal Pipelines are created inside fit() to ensure they are fresh\n",
    "    on every CV fold, preventing IllegalArgumentException.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # 1. Regressors (Define separate prediction columns)\n",
    "        self.regressor_lineage = RandomForestRegressor(\n",
    "            featuresCol=\"features\", labelCol=\"DEP_DELAY\", \n",
    "            predictionCol=\"pred_lineage\", numTrees=50, maxDepth=10\n",
    "        )\n",
    "        self.regressor_no_lineage = RandomForestRegressor(\n",
    "            featuresCol=\"features\", labelCol=\"DEP_DELAY\", \n",
    "            predictionCol=\"pred_no_lineage\", numTrees=50, maxDepth=10\n",
    "        )\n",
    "        \n",
    "        # Fitted objects\n",
    "        self.feature_model_lineage = None\n",
    "        self.feature_model_no_lineage = None\n",
    "        self.model_lineage = None\n",
    "        self.model_no_lineage = None\n",
    "        \n",
    "        # Define key columns for joining predictions\n",
    "        self.key_cols = [\n",
    "            'month', 'day_of_month', 'day_of_week', 'op_carrier', \n",
    "            'origin', 'dest', 'dep_time_blk', 'arr_time_blk'\n",
    "        ]\n",
    "        \n",
    "        # Define all required label columns for the CV Evaluator\n",
    "        self.required_labels = ['DEP_DELAY', 'DEP_DEL15', 'SEVERE_DEL60']\n",
    "\n",
    "    def fit(self, df):\n",
    "        \"\"\"\n",
    "        Fits both models using their respective feature pipelines.\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- 1. Instantiate FRESH Feature Pipelines (Fix for CV reuse) ---\n",
    "        feature_pipe_lineage = create_feature_pipeline(\n",
    "            categorical_features, numerical_features, \"L\"\n",
    "        )\n",
    "        feature_pipe_no_lineage = create_feature_pipeline(\n",
    "            categorical_features, numerical_features_no_lineage, \"NL\"\n",
    "        )\n",
    "        # ------------------------------------------------------------------\n",
    "\n",
    "        print(\"Fitting feature pipeline (with Lineage)...\")\n",
    "        # Fit and store the fitted feature transformer model\n",
    "        self.feature_model_lineage = feature_pipe_lineage.fit(df)\n",
    "        df_lineage_features = self.feature_model_lineage.transform(df).cache()\n",
    "\n",
    "        print(\"Fitting feature pipeline (without Lineage)...\")\n",
    "        # Fit and store the fitted feature transformer model\n",
    "        self.feature_model_no_lineage = feature_pipe_no_lineage.fit(df)\n",
    "        df_no_lineage_features = self.feature_model_no_lineage.transform(df).cache()\n",
    "        \n",
    "        # 2. Fit Regressors on their respective feature sets\n",
    "        print(\"Fitting Regressor (with Lineage)...\")\n",
    "        self.model_lineage = self.regressor_lineage.fit(df_lineage_features)\n",
    "        \n",
    "        print(\"Fitting Regressor (without Lineage)...\")\n",
    "        self.model_no_lineage = self.regressor_no_lineage.fit(df_no_lineage_features)\n",
    "        \n",
    "        # Unpersist intermediate DataFrames\n",
    "        df_lineage_features.unpersist()\n",
    "        df_no_lineage_features.unpersist()\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        \"\"\"\n",
    "        Applies both fitted feature and model stages and computes the average \n",
    "        prediction, ensuring all required label columns are retained for evaluation.\n",
    "        \"\"\"\n",
    "        # Ensure the feature models and regressors have been fitted\n",
    "        if not (self.feature_model_lineage and self.model_lineage and \n",
    "                self.feature_model_no_lineage and self.model_no_lineage):\n",
    "            raise Exception(\"VotingEnsemble must be fitted before transforming.\")\n",
    "\n",
    "        # --- Model 1: Lineage ---\n",
    "        df_lineage_features = self.feature_model_lineage.transform(df)\n",
    "        df_pred_1 = self.model_lineage.transform(df_lineage_features)\n",
    "        df_pred_1_selected = df_pred_1.select(F.col('pred_lineage'), *self.key_cols)\n",
    "        \n",
    "        # --- Model 2: No Lineage ---\n",
    "        df_no_lineage_features = self.feature_model_no_lineage.transform(df)\n",
    "        df_pred_2 = self.model_no_lineage.transform(df_no_lineage_features)\n",
    "        df_pred_2_selected = df_pred_2.select(F.col('pred_no_lineage'), *self.key_cols)\n",
    "\n",
    "        # --- Ensemble ---\n",
    "        \n",
    "        # Join predictions and the key columns\n",
    "        df_final = df_pred_1_selected.join(df_pred_2_selected, on=self.key_cols, how='inner') \n",
    "        \n",
    "        # JOIN STEP: Join with the original DataFrame (df) to get ALL required label columns (FIX)\n",
    "        df_final = df_final.join(\n",
    "            df.select(*self.key_cols, *self.required_labels), \n",
    "            on=self.key_cols, \n",
    "            how='inner'\n",
    "        ) \n",
    "\n",
    "        # Compute the ensemble prediction (Averaging)\n",
    "        df_final = df_final.withColumn(\n",
    "            \"prediction\",\n",
    "            (F.col(\"pred_lineage\") + F.col(\"pred_no_lineage\")) / F.lit(2.0)\n",
    "        )\n",
    "        \n",
    "        # Final output: select prediction, all labels, and key columns\n",
    "        return df_final.select(F.col(\"prediction\"), *self.required_labels, *self.key_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2038d6f-4bfd-475f-89b0-f6da6174269a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ensemble_estimator = VotingEnsemble()\n",
    "cv_ensemble_set = cv.FlightDelayCV(\n",
    "    estimator=ensemble_estimator,\n",
    "    dataloader=data_loader,\n",
    "    version=\"3M\" # Use the desired dataset version\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07fb3f04-f0df-4085-bdae-f4c655758bd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_df = cv_ensemble_set.evaluate()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ensemble",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
