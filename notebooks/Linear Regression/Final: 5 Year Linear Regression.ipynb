{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44ffdc26-7388-4d3f-bf9c-bb921b869e6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load local modules: Cross Validator\n",
    "import importlib.util\n",
    "cv_path = \"/Workspace/Shared/Team 4_2/flight-departure-delay-predictive-modeling/notebooks/Cross Validator/cv.py\"\n",
    "spec = importlib.util.spec_from_file_location(\"cv\", cv_path)\n",
    "cv = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(cv)\n",
    "\n",
    "import mlflow\n",
    "mlflow.autolog(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6f7fc8e-120a-4ae3-900b-cec367781e8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_loader = cv.FlightDelayDataLoader()\n",
    "data_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d2b77b2-5288-4568-8364-62011f34094c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, isnan, regexp_replace, when, length, trim\n",
    "from pyspark.sql.types import DoubleType, StringType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Imputer, StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "class BaselineEstimator:\n",
    "    \"\"\"\n",
    "    Baseline Linear Regression Estimator with Feature Engineering Pipeline.\n",
    "    \n",
    "    This class encapsulates the entire feature engineering and modeling pipeline:\n",
    "        1. Data preparation (label cleaning, feature selection)\n",
    "        2. Numerical feature cleaning (remove non-numeric chars, handle nulls)\n",
    "        3. Median imputation for numerical features\n",
    "        4. Categorical encoding (StringIndexer + OneHotEncoder)\n",
    "        5. Feature assembly and standardization\n",
    "        6. Linear regression modeling\n",
    "    \n",
    "    Feature Families:\n",
    "        - Temporal: DAY_OF_WEEK, MONTH, DEP_TIME_BLK\n",
    "        - Airport: ORIGIN, DEST\n",
    "        - Flight: OP_UNIQUE_CARRIER, DISTANCE\n",
    "        - Weather: HourlyWindSpeed, HourlyVisibility, HourlyPrecipitation\n",
    "    \n",
    "    Why This Design:\n",
    "        - Encapsulation: All preprocessing logic in one place\n",
    "        - Reusability: Same pipeline for train/val/test\n",
    "        - Spark ML compatibility: Uses Pipeline for efficient execution\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, label_col=\"DEP_DELAY\"):\n",
    "        \"\"\"\n",
    "        Initialize the estimator with feature definitions.\n",
    "        \n",
    "        Args:\n",
    "            label_col (str): Name of the target variable column\n",
    "        \"\"\"\n",
    "        self.label_col = label_col\n",
    "        self.pipeline = None\n",
    "        self.model = None\n",
    "        \n",
    "        # Categorical features: Encoded using StringIndexer + OneHotEncoder\n",
    "        # These capture temporal patterns, route characteristics, and carrier effects\n",
    "        self.categorical_features = [\n",
    "            \"DAY_OF_WEEK\",        # Day of week (1=Monday, 7=Sunday)\n",
    "            \"MONTH\",              # Month of year (1-12)\n",
    "            \"DEP_TIME_BLK\",       # Departure time block (e.g., \"0600-0659\")\n",
    "            \"ORIGIN\",             # Origin airport code\n",
    "            \"DEST\",               # Destination airport code\n",
    "            \"OP_UNIQUE_CARRIER\"   # Operating carrier code\n",
    "        ]\n",
    "        self.categorical_features = [colname.lower() for colname in self.categorical_features]\n",
    "        \n",
    "        # Numerical features: Imputed with median and standardized\n",
    "        # These capture weather conditions and flight distance\n",
    "        self.numerical_features = [\n",
    "            \"HourlyWindSpeed\",       # Wind speed at origin (mph)\n",
    "            \"HourlyVisibility\",      # Visibility at origin (miles)\n",
    "            \"HourlyPrecipitation\",   # Precipitation at origin (inches)\n",
    "            \"DISTANCE\"               # Flight distance (miles)\n",
    "        ]\n",
    "        self.numerical_features = [colname.lower() for colname in self.numerical_features]\n",
    "        \n",
    "    def _prepare(self, df):\n",
    "        \"\"\"\n",
    "        Prepare the DataFrame for modeling by cleaning the label and numerical features.\n",
    "        \n",
    "        Steps:\n",
    "            1. Cast label to DoubleType (required by Spark ML)\n",
    "            2. Filter out rows with null/NaN labels (Spark ML requirement)\n",
    "            3. Select only required features + ALL labels (including binary labels for evaluation)\n",
    "            4. Clean numerical features:\n",
    "               - Remove non-numeric characters (e.g., \"12.5mph\" -> \"12.5\")\n",
    "               - Convert empty strings to null\n",
    "               - Cast to DoubleType\n",
    "        \n",
    "        Args:\n",
    "            df (pyspark.sql.DataFrame): Input DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            pyspark.sql.DataFrame: Cleaned DataFrame\n",
    "            \n",
    "        Why This Matters:\n",
    "            - Spark ML LinearRegression requires DoubleType labels with no nulls\n",
    "            - Numerical features may contain string artifacts from source data\n",
    "            - Explicit type casting prevents downstream pipeline errors\n",
    "            - Binary labels (DEP_DEL15, SEVERE_DEL60) must be preserved for evaluation\n",
    "        \"\"\"\n",
    "        # Cast label to double and filter out null/NaN values\n",
    "        # Spark ML does not accept null labels\n",
    "        df = df.withColumn(self.label_col, col(self.label_col).cast(DoubleType()))\n",
    "        df = df.filter(~(col(self.label_col).isNull() | isnan(col(self.label_col))))\n",
    "\n",
    "        # Select only the columns we need (features + ALL labels for evaluation)\n",
    "        # This includes binary labels (DEP_DEL15, SEVERE_DEL60) needed by the evaluator\n",
    "        selected = [c for c in (self.categorical_features + \n",
    "                                self.numerical_features + \n",
    "                                [self.label_col, \"DEP_DEL15\", \"SEVERE_DEL60\"]) \n",
    "                    if c in df.columns]\n",
    "        df = df.select(*selected)\n",
    "\n",
    "        # Clean numerical features: remove non-numeric characters, handle empty strings\n",
    "        for f in self.numerical_features:\n",
    "            if f in df.columns:\n",
    "                # Step 1: Cast to string to enable regex operations\n",
    "                # Step 2: Remove all non-numeric characters except +, -, and .\n",
    "                df = df.withColumn(f, regexp_replace(col(f).cast(StringType()), r\"[^0-9+\\-\\.]\", \"\"))\n",
    "                \n",
    "                # Step 3: Convert empty strings to null (for imputation)\n",
    "                df = df.withColumn(f, when(length(trim(col(f))) == 0, None).otherwise(col(f)))\n",
    "                \n",
    "                # Step 4: Cast to DoubleType for modeling\n",
    "                df = df.withColumn(f, col(f).cast(DoubleType()))\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def _build_pipeline(self, df):\n",
    "        \"\"\"\n",
    "        Build the Spark ML Pipeline with all feature engineering stages.\n",
    "        \n",
    "        Pipeline Stages:\n",
    "            1. Imputers: Median imputation for numerical features\n",
    "            2. StringIndexers: Convert categorical strings to indices\n",
    "            3. OneHotEncoders: Convert indices to binary vectors\n",
    "            4. VectorAssembler: Combine all features into single vector\n",
    "            5. StandardScaler: Standardize features (mean=0, std=1)\n",
    "            6. LinearRegression: Train linear model\n",
    "        \n",
    "        Args:\n",
    "            df (pyspark.sql.DataFrame): Prepared DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            pyspark.sql.DataFrame: DataFrame (may have additional columns from transformations)\n",
    "            \n",
    "        Why This Design:\n",
    "            - Pipeline ensures consistent transformations across train/val/test\n",
    "            - Median imputation is robust to outliers (better than mean)\n",
    "            - OneHotEncoding with dropLast=True prevents multicollinearity\n",
    "            - StandardScaler improves convergence for gradient descent\n",
    "            - No regularization (regParam=0) for interpretable baseline\n",
    "        \"\"\"\n",
    "        stages = []\n",
    "        \n",
    "        # ========================================\n",
    "        # Stage 1: Median Imputation for Numerical Features\n",
    "        # ========================================\n",
    "        # Why median? More robust to outliers than mean\n",
    "        # Missing weather data is common in aviation datasets\n",
    "        imputers = [\n",
    "            Imputer(inputCols=[f], outputCols=[f\"{f}_imputed\"], strategy=\"median\")\n",
    "            for f in self.numerical_features if f in df.columns\n",
    "        ]\n",
    "        stages.extend(imputers)\n",
    "\n",
    "        # ========================================\n",
    "        # Stage 2-3: Categorical Encoding (StringIndexer + OneHotEncoder)\n",
    "        # ========================================\n",
    "        # StringIndexer: Converts strings to numeric indices (most frequent = 0)\n",
    "        # OneHotEncoder: Converts indices to binary vectors (prevents ordinal assumption)\n",
    "        # handleInvalid=\"keep\": Unseen categories in test set get their own index\n",
    "        # dropLast=True: Drop last category to prevent multicollinearity\n",
    "        for f in self.categorical_features:\n",
    "            if f in df.columns:\n",
    "                # For numeric-coded categoricals (DAY_OF_WEEK, MONTH, DEP_TIME_BLK),\n",
    "                # cast to string first to ensure consistent handling\n",
    "                if f in [\"day_of_week\", \"month\", \"dep_time_blk\"]:\n",
    "                    df = df.withColumn(f\"{f}_clean\", \n",
    "                                      when(col(f).isNull(), \"UNKNOWN\").otherwise(col(f).cast(StringType())))\n",
    "                else:\n",
    "                    # For string categoricals (ORIGIN, DEST, CARRIER), handle nulls only\n",
    "                    df = df.withColumn(f\"{f}_clean\", \n",
    "                                      when(col(f).isNull(), \"UNKNOWN\").otherwise(col(f)))\n",
    "                \n",
    "                # Add StringIndexer stage\n",
    "                stages.append(StringIndexer(inputCol=f\"{f}_clean\", \n",
    "                                           outputCol=f\"{f}_indexed\", \n",
    "                                           handleInvalid=\"keep\"))\n",
    "                \n",
    "                # Add OneHotEncoder stage\n",
    "                stages.append(OneHotEncoder(inputCols=[f\"{f}_indexed\"], \n",
    "                                           outputCols=[f\"{f}_encoded\"], \n",
    "                                           dropLast=True))\n",
    "\n",
    "        # ========================================\n",
    "        # Stage 4: Feature Assembly\n",
    "        # ========================================\n",
    "        # Combine all features (imputed numerical + encoded categorical) into single vector\n",
    "        # handleInvalid=\"skip\": Skip rows with invalid values (e.g., NaN after imputation)\n",
    "        feature_columns = [f\"{f}_imputed\" for f in self.numerical_features if f in df.columns] + \\\n",
    "                          [f\"{f}_encoded\" for f in self.categorical_features if f in df.columns]\n",
    "        assembler = VectorAssembler(inputCols=feature_columns, \n",
    "                                    outputCol=\"features\", \n",
    "                                    handleInvalid=\"skip\")\n",
    "        \n",
    "        # ========================================\n",
    "        # Stage 5: Feature Standardization\n",
    "        # ========================================\n",
    "        # Standardize features to mean=0, std=1\n",
    "        # withStd=True: Scale to unit variance\n",
    "        # withMean=True: Center to zero mean\n",
    "        # Why? Improves convergence and makes coefficients comparable\n",
    "        scaler = StandardScaler(inputCol=\"features\", \n",
    "                               outputCol=\"scaled_features\", \n",
    "                               withStd=True, \n",
    "                               withMean=True)\n",
    "        \n",
    "        # ========================================\n",
    "        # Stage 6: Linear Regression\n",
    "        # ========================================\n",
    "        # Baseline model: No regularization (regParam=0, elasticNetParam=0)\n",
    "        # maxIter=100: Maximum iterations for convergence\n",
    "        # Why no regularization? We want an interpretable baseline to understand\n",
    "        # feature importance before adding complexity\n",
    "        lr = LinearRegression(featuresCol=\"scaled_features\", \n",
    "                            labelCol=self.label_col, \n",
    "                            maxIter=100, \n",
    "                            regParam=0.0, \n",
    "                            elasticNetParam=0.0)\n",
    "\n",
    "        # Assemble all stages into pipeline\n",
    "        stages.extend([assembler, scaler, lr])\n",
    "        self.pipeline = Pipeline(stages=stages)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def fit(self, df):\n",
    "        \"\"\"\n",
    "        Fit the pipeline on training data.\n",
    "        \n",
    "        Args:\n",
    "            df (pyspark.sql.DataFrame): Training DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            BaselineEstimator: self (for method chaining)\n",
    "        \"\"\"\n",
    "        df_prep = self._prepare(df)\n",
    "        df_prep = self._build_pipeline(df_prep)\n",
    "        self.model = self.pipeline.fit(df_prep)\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        \"\"\"\n",
    "        Transform data using the fitted pipeline.\n",
    "        \n",
    "        Args:\n",
    "            df (pyspark.sql.DataFrame): DataFrame to transform (val/test)\n",
    "            \n",
    "        Returns:\n",
    "            pyspark.sql.DataFrame: Transformed DataFrame with predictions\n",
    "            \n",
    "        Note:\n",
    "            We rebuild the pipeline on the input DataFrame to ensure all\n",
    "            transformation columns are present, but use the fitted model\n",
    "            for predictions.\n",
    "        \"\"\"\n",
    "        df_prep = self._prepare(df)\n",
    "        df_prep = self._build_pipeline(df_prep)  # Rebuild to ensure cols present\n",
    "        return self.model.transform(df_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4da69e37-f3b0-4a0e-9ed4-ba4d66770417",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_set = cv.FlightDelayCV(\n",
    "    estimator=BaselineEstimator(),\n",
    "    dataloader=data_loader,\n",
    "    version=\"60M\"\n",
    ")\n",
    "\n",
    "cv_set.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec6a1d1f-32cd-493b-a198-a08fdbec2a71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_set = cv.FlightDelayCV(\n",
    "    estimator=BaselineEstimator(),\n",
    "    dataloader=data_loader,\n",
    "    version=\"60M\"\n",
    ")\n",
    "\n",
    "cv_set.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9764dd27-5190-4a8f-ac58-691b48aa6664",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, isnan, regexp_replace, when, length, trim\n",
    "from pyspark.sql.types import DoubleType, StringType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Imputer, StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "class BaselineEstimator:\n",
    "    \"\"\"\n",
    "    Baseline Linear Regression Estimator with Feature Engineering Pipeline.\n",
    "    \n",
    "    This class encapsulates the entire feature engineering and modeling pipeline:\n",
    "        1. Data preparation (label cleaning, feature selection)\n",
    "        2. Numerical feature cleaning (remove non-numeric chars, handle nulls)\n",
    "        3. Median imputation for numerical features\n",
    "        4. Categorical encoding (StringIndexer + OneHotEncoder)\n",
    "        5. Feature assembly and standardization\n",
    "        6. Linear regression modeling\n",
    "    \n",
    "    Feature Families:\n",
    "        - Temporal: DAY_OF_WEEK, MONTH, DEP_TIME_BLK\n",
    "        - Airport: ORIGIN, DEST\n",
    "        - Flight: OP_UNIQUE_CARRIER, DISTANCE\n",
    "        - Weather: HourlyWindSpeed, HourlyVisibility, HourlyPrecipitation\n",
    "    \n",
    "    Why This Design:\n",
    "        - Encapsulation: All preprocessing logic in one place\n",
    "        - Reusability: Same pipeline for train/val/test\n",
    "        - Spark ML compatibility: Uses Pipeline for efficient execution\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, label_col=\"DEP_DELAY\"):\n",
    "        \"\"\"\n",
    "        Initialize the estimator with feature definitions.\n",
    "        \n",
    "        Args:\n",
    "            label_col (str): Name of the target variable column\n",
    "        \"\"\"\n",
    "        self.label_col = label_col\n",
    "        self.pipeline = None\n",
    "        self.model = None\n",
    "        \n",
    "        # Categorical features: Encoded using StringIndexer + OneHotEncoder\n",
    "        # These capture temporal patterns, route characteristics, and carrier effects\n",
    "        self.categorical_features = [\n",
    "            \"DAY_OF_WEEK\",        # Day of week (1=Monday, 7=Sunday)\n",
    "            \"MONTH\",              # Month of year (1-12)\n",
    "            \"DEP_TIME_BLK\",       # Departure time block (e.g., \"0600-0659\")\n",
    "            \"ORIGIN\",             # Origin airport code\n",
    "            \"DEST\",               # Destination airport code\n",
    "            \"OP_UNIQUE_CARRIER\"   # Operating carrier code\n",
    "        ]\n",
    "        self.categorical_features = [colname.lower() for colname in self.categorical_features]\n",
    "        \n",
    "        # Numerical features: Imputed with median and standardized\n",
    "        # These capture weather conditions and flight distance\n",
    "        self.numerical_features = [\n",
    "            \"HourlyWindSpeed\",       # Wind speed at origin (mph)\n",
    "            \"HourlyVisibility\",      # Visibility at origin (miles)\n",
    "            \"HourlyPrecipitation\",   # Precipitation at origin (inches)\n",
    "            \"DISTANCE\"               # Flight distance (miles)\n",
    "        ]\n",
    "        self.numerical_features = [colname.lower() for colname in self.numerical_features]\n",
    "        \n",
    "    def _prepare(self, df):\n",
    "        \"\"\"\n",
    "        Prepare the DataFrame for modeling by cleaning the label and numerical features.\n",
    "        \n",
    "        Steps:\n",
    "            1. Cast label to DoubleType (required by Spark ML)\n",
    "            2. Filter out rows with null/NaN labels (Spark ML requirement)\n",
    "            3. Select only required features + ALL labels (including binary labels for evaluation)\n",
    "            4. Clean numerical features:\n",
    "               - Remove non-numeric characters (e.g., \"12.5mph\" -> \"12.5\")\n",
    "               - Convert empty strings to null\n",
    "               - Cast to DoubleType\n",
    "        \n",
    "        Args:\n",
    "            df (pyspark.sql.DataFrame): Input DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            pyspark.sql.DataFrame: Cleaned DataFrame\n",
    "            \n",
    "        Why This Matters:\n",
    "            - Spark ML LinearRegression requires DoubleType labels with no nulls\n",
    "            - Numerical features may contain string artifacts from source data\n",
    "            - Explicit type casting prevents downstream pipeline errors\n",
    "            - Binary labels (DEP_DEL15, SEVERE_DEL60) must be preserved for evaluation\n",
    "        \"\"\"\n",
    "        # Cast label to double and filter out null/NaN values\n",
    "        # Spark ML does not accept null labels\n",
    "        df = df.withColumn(self.label_col, col(self.label_col).cast(DoubleType()))\n",
    "        df = df.filter(~(col(self.label_col).isNull() | isnan(col(self.label_col))))\n",
    "\n",
    "        # Select only the columns we need (features + ALL labels for evaluation)\n",
    "        # This includes binary labels (DEP_DEL15, SEVERE_DEL60) needed by the evaluator\n",
    "        selected = [c for c in (self.categorical_features + \n",
    "                                self.numerical_features + \n",
    "                                [self.label_col, \"DEP_DEL15\", \"SEVERE_DEL60\"]) \n",
    "                    if c in df.columns]\n",
    "        df = df.select(*selected)\n",
    "\n",
    "        # Clean numerical features: remove non-numeric characters, handle empty strings\n",
    "        for f in self.numerical_features:\n",
    "            if f in df.columns:\n",
    "                # Step 1: Cast to string to enable regex operations\n",
    "                # Step 2: Remove all non-numeric characters except +, -, and .\n",
    "                df = df.withColumn(f, regexp_replace(col(f).cast(StringType()), r\"[^0-9+\\-\\.]\", \"\"))\n",
    "                \n",
    "                # Step 3: Convert empty strings to null (for imputation)\n",
    "                df = df.withColumn(f, when(length(trim(col(f))) == 0, None).otherwise(col(f)))\n",
    "                \n",
    "                # Step 4: Cast to DoubleType for modeling\n",
    "                df = df.withColumn(f, col(f).cast(DoubleType()))\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def _build_pipeline(self, df):\n",
    "        \"\"\"\n",
    "        Build the Spark ML Pipeline with all feature engineering stages.\n",
    "        \n",
    "        Pipeline Stages:\n",
    "            1. Imputers: Median imputation for numerical features\n",
    "            2. StringIndexers: Convert categorical strings to indices\n",
    "            3. OneHotEncoders: Convert indices to binary vectors\n",
    "            4. VectorAssembler: Combine all features into single vector\n",
    "            5. StandardScaler: Standardize features (mean=0, std=1)\n",
    "            6. LinearRegression: Train linear model\n",
    "        \n",
    "        Args:\n",
    "            df (pyspark.sql.DataFrame): Prepared DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            pyspark.sql.DataFrame: DataFrame (may have additional columns from transformations)\n",
    "            \n",
    "        Why This Design:\n",
    "            - Pipeline ensures consistent transformations across train/val/test\n",
    "            - Median imputation is robust to outliers (better than mean)\n",
    "            - OneHotEncoding with dropLast=True prevents multicollinearity\n",
    "            - StandardScaler improves convergence for gradient descent\n",
    "            - No regularization (regParam=0) for interpretable baseline\n",
    "        \"\"\"\n",
    "        stages = []\n",
    "        \n",
    "        # ========================================\n",
    "        # Stage 1: Median Imputation for Numerical Features\n",
    "        # ========================================\n",
    "        # Why median? More robust to outliers than mean\n",
    "        # Missing weather data is common in aviation datasets\n",
    "        imputers = [\n",
    "            Imputer(inputCols=[f], outputCols=[f\"{f}_imputed\"], strategy=\"median\")\n",
    "            for f in self.numerical_features if f in df.columns\n",
    "        ]\n",
    "        stages.extend(imputers)\n",
    "\n",
    "        # ========================================\n",
    "        # Stage 2-3: Categorical Encoding (StringIndexer + OneHotEncoder)\n",
    "        # ========================================\n",
    "        # StringIndexer: Converts strings to numeric indices (most frequent = 0)\n",
    "        # OneHotEncoder: Converts indices to binary vectors (prevents ordinal assumption)\n",
    "        # handleInvalid=\"keep\": Unseen categories in test set get their own index\n",
    "        # dropLast=True: Drop last category to prevent multicollinearity\n",
    "        for f in self.categorical_features:\n",
    "            if f in df.columns:\n",
    "                # For numeric-coded categoricals (DAY_OF_WEEK, MONTH, DEP_TIME_BLK),\n",
    "                # cast to string first to ensure consistent handling\n",
    "                if f in [\"day_of_week\", \"month\", \"dep_time_blk\"]:\n",
    "                    df = df.withColumn(f\"{f}_clean\", \n",
    "                                      when(col(f).isNull(), \"UNKNOWN\").otherwise(col(f).cast(StringType())))\n",
    "                else:\n",
    "                    # For string categoricals (ORIGIN, DEST, CARRIER), handle nulls only\n",
    "                    df = df.withColumn(f\"{f}_clean\", \n",
    "                                      when(col(f).isNull(), \"UNKNOWN\").otherwise(col(f)))\n",
    "                \n",
    "                # Add StringIndexer stage\n",
    "                stages.append(StringIndexer(inputCol=f\"{f}_clean\", \n",
    "                                           outputCol=f\"{f}_indexed\", \n",
    "                                           handleInvalid=\"keep\"))\n",
    "                \n",
    "                # Add OneHotEncoder stage\n",
    "                stages.append(OneHotEncoder(inputCols=[f\"{f}_indexed\"], \n",
    "                                           outputCols=[f\"{f}_encoded\"], \n",
    "                                           dropLast=True))\n",
    "\n",
    "        # ========================================\n",
    "        # Stage 4: Feature Assembly\n",
    "        # ========================================\n",
    "        # Combine all features (imputed numerical + encoded categorical) into single vector\n",
    "        # handleInvalid=\"skip\": Skip rows with invalid values (e.g., NaN after imputation)\n",
    "        feature_columns = [f\"{f}_imputed\" for f in self.numerical_features if f in df.columns] + \\\n",
    "                          [f\"{f}_encoded\" for f in self.categorical_features if f in df.columns]\n",
    "        assembler = VectorAssembler(inputCols=feature_columns, \n",
    "                                    outputCol=\"features\", \n",
    "                                    handleInvalid=\"skip\")\n",
    "        \n",
    "        # ========================================\n",
    "        # Stage 5: Feature Standardization\n",
    "        # ========================================\n",
    "        # Standardize features to mean=0, std=1\n",
    "        # withStd=True: Scale to unit variance\n",
    "        # withMean=True: Center to zero mean\n",
    "        # Why? Improves convergence and makes coefficients comparable\n",
    "        scaler = StandardScaler(inputCol=\"features\", \n",
    "                               outputCol=\"scaled_features\", \n",
    "                               withStd=True, \n",
    "                               withMean=True)\n",
    "        \n",
    "        # ========================================\n",
    "        # Stage 6: Linear Regression\n",
    "        # ========================================\n",
    "        # Baseline model: No regularization (regParam=0, elasticNetParam=0)\n",
    "        # maxIter=100: Maximum iterations for convergence\n",
    "        # Why no regularization? We want an interpretable baseline to understand\n",
    "        # feature importance before adding complexity\n",
    "        lr = LinearRegression(featuresCol=\"scaled_features\", \n",
    "                            labelCol=self.label_col, \n",
    "                            maxIter=100, \n",
    "                            regParam=0.0, \n",
    "                            elasticNetParam=0.0)\n",
    "\n",
    "        # Assemble all stages into pipeline\n",
    "        stages.extend([assembler, scaler, lr])\n",
    "        self.pipeline = Pipeline(stages=stages)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def fit(self, df):\n",
    "        \"\"\"\n",
    "        Fit the pipeline on training data.\n",
    "        \n",
    "        Args:\n",
    "            df (pyspark.sql.DataFrame): Training DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            BaselineEstimator: self (for method chaining)\n",
    "        \"\"\"\n",
    "        df_prep = self._prepare(df)\n",
    "        df_prep = self._build_pipeline(df_prep)\n",
    "        self.model = self.pipeline.fit(df_prep)\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        \"\"\"\n",
    "        Transform data using the fitted pipeline.\n",
    "        \n",
    "        Args:\n",
    "            df (pyspark.sql.DataFrame): DataFrame to transform (val/test)\n",
    "            \n",
    "        Returns:\n",
    "            pyspark.sql.DataFrame: Transformed DataFrame with predictions\n",
    "            \n",
    "        Note:\n",
    "            We rebuild the pipeline on the input DataFrame to ensure all\n",
    "            transformation columns are present, but use the fitted model\n",
    "            for predictions.\n",
    "        \"\"\"\n",
    "        df_prep = self._prepare(df)\n",
    "        df_prep = self._build_pipeline(df_prep)  # Rebuild to ensure cols present\n",
    "        return self.model.transform(df_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af26a350-896a-455d-8671-d7db34087866",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_set = cv.FlightDelayCV(\n",
    "    estimator=BaselineEstimator(),\n",
    "    dataloader=data_loader,\n",
    "    version=\"60M\"\n",
    ")\n",
    "\n",
    "cv_set.evaluate()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Final: 5 Year Linear Regression",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
