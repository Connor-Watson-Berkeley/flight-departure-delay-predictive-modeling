{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97023af5-5e99-4bc5-a86f-4d546db1f541",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "MLP Model using PyTorch + GPU Support\n",
    "====================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c7bfd43-169f-4ea5-be2a-01884b7fcea5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "- Here we build Distributed MLP models. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b24235b0-158a-46d3-ab72-ae8d5c658e97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "cv.py (simplified, CUSTOM-only, no parametrization)\n",
    "\n",
    "Assumptions:\n",
    "- Folds were created from split.py with N_FOLDS = 3 and CREATE_TEST_FOLD = True\n",
    "- Therefore total fold indices written = 4:\n",
    "    FOLD_1_VAL, FOLD_2_VAL, FOLD_3_VAL, FOLD_4_TEST\n",
    "- Files live in:\n",
    "    dbfs:/mnt/mids-w261/student-groups/Group_4_2/processed\n",
    "- File naming:\n",
    "    OTPW_CUSTOM_{VERSION}_FOLD_{i}_{TRAIN|VAL|TEST}.parquet\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import pandas as pd\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# HARD-CODED GLOBALS\n",
    "# -----------------------------\n",
    "FOLDER_PATH = \"dbfs:/mnt/mids-w261/student-groups/Group_4_2/processed\"\n",
    "SOURCE = \"CUSTOM\"\n",
    "VERSIONS = [\"3M\", \"12M\", \"60M\"]\n",
    "\n",
    "# 3 CV folds + 1 test fold = 4 total fold indices\n",
    "TOTAL_FOLDS = 4\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# DATA LOADER (CUSTOM ONLY)\n",
    "# -----------------------------\n",
    "class FlightDelayDataLoader:\n",
    "    def __init__(self):\n",
    "        self.folds = {}\n",
    "        self.numerical_features = [\n",
    "            'hourlyprecipitation',\n",
    "            'hourlysealevelpressure',\n",
    "            'hourlyaltimetersetting',\n",
    "            'hourlywetbulbtemperature',\n",
    "            'hourlystationpressure',\n",
    "            'hourlywinddirection',\n",
    "            'hourlyrelativehumidity',\n",
    "            'hourlywindspeed',\n",
    "            'hourlydewpointtemperature',\n",
    "            'hourlydrybulbtemperature',\n",
    "            'hourlyvisibility',\n",
    "            'crs_elapsed_time',\n",
    "            'distance',\n",
    "            'elevation',\n",
    "        ]\n",
    "\n",
    "    def _cast_numerics(self, df):\n",
    "        \"\"\"Safely cast all configured numeric columns to doubles.\"\"\"\n",
    "        NULL_PAT = r'^(NA|N/A|NULL|null|None|none|\\\\N|\\\\s*|\\\\.|M|T)$'\n",
    "        \n",
    "        for colname in self.numerical_features:\n",
    "            if colname in df.columns:\n",
    "                df = df.withColumn(\n",
    "                    colname,\n",
    "                    F.regexp_replace(F.col(colname).cast(\"string\"), NULL_PAT, \"\")\n",
    "                    .cast(\"double\")\n",
    "                )\n",
    "        \n",
    "        # Explicitly cast labels to expected numeric types\n",
    "        if \"DEP_DELAY\" in df.columns:\n",
    "            df = df.withColumn(\"DEP_DELAY\", col(\"DEP_DELAY\").cast(\"double\"))\n",
    "        if \"DEP_DEL15\" in df.columns:\n",
    "            df = df.withColumn(\"DEP_DEL15\", col(\"DEP_DEL15\").cast(\"int\"))\n",
    "        if \"SEVERE_DEL60\" in df.columns:\n",
    "            df = df.withColumn(\"SEVERE_DEL60\", col(\"SEVERE_DEL60\").cast(\"int\"))\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def _load_parquet(self, name):\n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "        df = spark.read.parquet(f\"{FOLDER_PATH}/{name}.parquet\")\n",
    "        df = self._cast_numerics(df)\n",
    "        return df\n",
    "\n",
    "    def _load_version(self, version):\n",
    "        folds = []\n",
    "        for fold_idx in range(1, TOTAL_FOLDS + 1):\n",
    "            train_name = f\"OTPW_{SOURCE}_{version}_FOLD_{fold_idx}_TRAIN\"\n",
    "            train_df = self._load_parquet(train_name)\n",
    "\n",
    "            if fold_idx < TOTAL_FOLDS:\n",
    "                val_name = f\"OTPW_{SOURCE}_{version}_FOLD_{fold_idx}_VAL\"\n",
    "                val_df = self._load_parquet(val_name)\n",
    "                folds.append((train_df, val_df))\n",
    "            else:\n",
    "                test_name = f\"OTPW_{SOURCE}_{version}_FOLD_{fold_idx}_TEST\"\n",
    "                test_df = self._load_parquet(test_name)\n",
    "                folds.append((train_df, test_df))\n",
    "\n",
    "        return folds\n",
    "\n",
    "    def load_version(self, version):\n",
    "        if version not in self.folds:\n",
    "            self.folds[version] = self._load_version(version)\n",
    "        return self.folds[version]\n",
    "\n",
    "    def get_version(self, version):\n",
    "        return self.load_version(version)\n",
    "    \n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# EVALUATOR (NULL-SAFE RMSE)\n",
    "# -----------------------------\n",
    "class FlightDelayEvaluator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        prediction_col=\"prediction\",\n",
    "        numeric_label_col=\"DEP_DELAY\",\n",
    "        binary_label_col=\"DEP_DEL15\",\n",
    "        severe_label_col=\"SEVERE_DEL60\",\n",
    "    ):\n",
    "        self.prediction_col = prediction_col\n",
    "        self.numeric_label_col = numeric_label_col\n",
    "        self.binary_label_col = binary_label_col\n",
    "        self.severe_label_col = severe_label_col\n",
    "\n",
    "        self.rmse_evaluator = RegressionEvaluator(\n",
    "            predictionCol=prediction_col,\n",
    "            labelCol=numeric_label_col,\n",
    "            metricName=\"rmse\"\n",
    "        )\n",
    "\n",
    "    def calculate_rmse(self, predictions_df):\n",
    "        # Drop any residual nulls before RegressionEvaluator sees them\n",
    "        clean = predictions_df.dropna(\n",
    "            subset=[self.numeric_label_col, self.prediction_col]\n",
    "        )\n",
    "        return self.rmse_evaluator.evaluate(clean)\n",
    "\n",
    "    def _calculate_classification_metrics(self, predictions_df, threshold, label_col):\n",
    "        # Null-safe for classification too\n",
    "        df = predictions_df.dropna(subset=[self.prediction_col, label_col])\n",
    "\n",
    "        pred_binary_col = f\"pred_binary_{threshold}\"\n",
    "        df = df.withColumn(\n",
    "            pred_binary_col,\n",
    "            F.when(F.col(self.prediction_col) >= threshold, 1).otherwise(0)\n",
    "        )\n",
    "\n",
    "        tp = df.filter((F.col(pred_binary_col) == 1) & (F.col(label_col) == 1)).count()\n",
    "        fp = df.filter((F.col(pred_binary_col) == 1) & (F.col(label_col) == 0)).count()\n",
    "        tn = df.filter((F.col(pred_binary_col) == 0) & (F.col(label_col) == 0)).count()\n",
    "        fn = df.filter((F.col(pred_binary_col) == 0) & (F.col(label_col) == 1)).count()\n",
    "\n",
    "        total = tp + fp + tn + fn\n",
    "        precision = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0.0\n",
    "        accuracy = (tp + tn) / total if total else 0.0\n",
    "\n",
    "        return dict(tp=tp, fp=fp, tn=tn, fn=fn,\n",
    "                    precision=precision, recall=recall, f1=f1, accuracy=accuracy)\n",
    "\n",
    "    def calculate_otpa_metrics(self, predictions_df):\n",
    "        return self._calculate_classification_metrics(\n",
    "            predictions_df, threshold=15, label_col=self.binary_label_col\n",
    "        )[\"accuracy\"]\n",
    "\n",
    "    def calculate_sddr_metrics(self, predictions_df):\n",
    "        return self._calculate_classification_metrics(\n",
    "            predictions_df, threshold=60, label_col=self.severe_label_col\n",
    "        )[\"recall\"]\n",
    "\n",
    "    def evaluate(self, predictions_df):\n",
    "\n",
    "        preds = predictions_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "        rmse = self.calculate_rmse(preds)\n",
    "        otpa = self.calculate_otpa_metrics(preds)\n",
    "        sddr = self.calculate_sddr_metrics(preds)\n",
    "        preds.unpersist()\n",
    "        return {\"rmse\": rmse, \"otpa\": otpa, \"sddr\": sddr}\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# CROSS-VALIDATOR (NO PARAMS)\n",
    "# -----------------------------\n",
    "class FlightDelayCV:\n",
    "    def __init__(self, estimator, version, dataloader = None):\n",
    "        self.estimator = estimator\n",
    "        self.version = version\n",
    "\n",
    "        self.data_loader = dataloader or FlightDelayDataLoader()\n",
    "        self.folds = self.data_loader.get_version(version)\n",
    "        \n",
    "        self.evaluator = FlightDelayEvaluator()\n",
    "        self.train_metrics = []\n",
    "        self.val_metrics = []\n",
    "        self.models = []\n",
    "        self.test_train_metric = None\n",
    "        self.test_metric = None\n",
    "        self.test_model = None\n",
    "\n",
    "    def fit(self):\n",
    "        # CV folds only (exclude last test fold)\n",
    "        for fold_idx, (train_df, val_df) in enumerate(self.folds[:-1]):\n",
    "            model = self.estimator.fit(train_df)\n",
    "            \n",
    "            # Evaluate on training data\n",
    "            train_preds = model.transform(train_df)\n",
    "            train_metric = self.evaluator.evaluate(train_preds)\n",
    "            self.train_metrics.append(train_metric)\n",
    "            \n",
    "            # Evaluate on validation data\n",
    "            val_preds = model.transform(val_df)\n",
    "            val_metric = self.evaluator.evaluate(val_preds)\n",
    "            self.val_metrics.append(val_metric)\n",
    "            \n",
    "            self.models.append(model)\n",
    "\n",
    "        # Create combined DataFrame with train and val metrics\n",
    "        train_df = pd.DataFrame(self.train_metrics)\n",
    "        val_df = pd.DataFrame(self.val_metrics)\n",
    "        \n",
    "        # Rename indices to include train/val labels\n",
    "        train_df.index = [f\"{i}-train\" for i in range(len(train_df))]\n",
    "        val_df.index = [f\"{i}-val\" for i in range(len(val_df))]\n",
    "        \n",
    "        # Interleave train and val rows\n",
    "        combined_rows = []\n",
    "        for i in range(len(self.train_metrics)):\n",
    "            combined_rows.append(train_df.iloc[i])\n",
    "            combined_rows.append(val_df.iloc[i])\n",
    "        \n",
    "        m = pd.DataFrame(combined_rows)\n",
    "        \n",
    "        # Add mean and std for train and val separately\n",
    "        train_mean = train_df.mean()\n",
    "        train_mean.name = \"mean-train\"\n",
    "        train_std = train_df.std()\n",
    "        train_std.name = \"std-train\"\n",
    "        \n",
    "        val_mean = val_df.mean()\n",
    "        val_mean.name = \"mean-val\"\n",
    "        val_std = val_df.std()\n",
    "        val_std.name = \"std-val\"\n",
    "        \n",
    "        m = pd.concat([m, train_mean.to_frame().T, train_std.to_frame().T,\n",
    "                       val_mean.to_frame().T, val_std.to_frame().T])\n",
    "        \n",
    "        return m\n",
    "\n",
    "    def evaluate(self):\n",
    "        # deprecated in favor of `test`\n",
    "        # kept for backwards compatibility\n",
    "        print(\"`evaluate()` is deprecated! Use `test()` instead\")\n",
    "        return self.test()\n",
    "    \n",
    "    def test(self):\n",
    "        # does the same thing as evaluate\n",
    "        train_df, test_df = self.folds[-1]\n",
    "        self.test_model = self.estimator.fit(train_df)\n",
    "        \n",
    "        # Evaluate on training data\n",
    "        train_preds = self.test_model.transform(train_df)\n",
    "        self.test_train_metric = self.evaluator.evaluate(train_preds)\n",
    "        \n",
    "        # Evaluate on test data\n",
    "        test_preds = self.test_model.transform(test_df)\n",
    "        self.test_metric = self.evaluator.evaluate(test_preds)\n",
    "        \n",
    "        # Return both as a DataFrame\n",
    "        results = pd.DataFrame([self.test_train_metric, self.test_metric],\n",
    "                              index=[\"train\", \"test\"])\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87c9b079-ed49-41dd-a9f5-16bc8878f3f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Spark Settings\n",
    "# to avoid OOM Error\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"16800\") \n",
    "\n",
    "# import importlib.util\n",
    "# import sys\n",
    "\n",
    "# # Load cv module directly from file path\n",
    "# cv_path = \"/Workspace/Shared/Team 4_2/flight-departure-delay-predictive-modeling/notebooks/Cross Validator/cv.py\"\n",
    "# spec = importlib.util.spec_from_file_location(\"cv\", cv_path)\n",
    "# cv = importlib.util.module_from_spec(spec)\n",
    "# spec.loader.exec_module(cv)\n",
    "\n",
    "\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pathlib import Path\n",
    "from pyspark.ml.feature import (\n",
    "    Imputer, StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    ")\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# >>> PYTORCH AND DISTRIBUTOR IMPORTS <<<\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pyspark.ml.torch.distributor import TorchDistributor \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- PYTORCH TRAIN FUNCTION (RUNS ON WORKERS) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7d86bcd-729e-46c2-aaa3-603e14cc1140",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_loader = FlightDelayDataLoader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59e57e47-b9b7-4ae6-8505-3b5085aea41f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# PYTORCH MLP REGRESSOR INTEGRATED WITH TORCHDISTRIBUTOR\n",
    "# =====================================================\n",
    "\n",
    "def train_fn(params):\n",
    "    \"\"\"\n",
    "    Run on each worker launched by TorchDistributor.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import sys\n",
    "    import traceback\n",
    "    \n",
    "    # 1. Error Handling Wrapper\n",
    "    try:\n",
    "        import glob\n",
    "        import random\n",
    "        import torch\n",
    "        import torch.nn as nn\n",
    "        import torch.optim as optim\n",
    "        import torch.distributed as dist\n",
    "        from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "        from torch.utils.data import DataLoader, IterableDataset\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "\n",
    "        # --- Local Model Definition ---\n",
    "        class PyTorchMLPRegressor_Worker(nn.Module):\n",
    "            def __init__(self, input_dim, hidden_layers, dropout_rate=0.3):\n",
    "                super().__init__()\n",
    "                layers = []\n",
    "                in_features = input_dim\n",
    "                for units in hidden_layers:\n",
    "                    layers.append(nn.Linear(in_features, units))\n",
    "                    layers.append(nn.BatchNorm1d(units))\n",
    "                    layers.append(nn.ReLU())\n",
    "                    layers.append(nn.Dropout(dropout_rate))\n",
    "                    in_features = units\n",
    "                layers.append(nn.Linear(in_features, 1))\n",
    "                self.network = nn.Sequential(*layers)\n",
    "\n",
    "            def forward(self, x):\n",
    "                return self.network(x).squeeze(1)\n",
    "\n",
    "        # --- DDP Init ---\n",
    "        backend = \"nccl\" if params[\"use_gpu\"] else \"gloo\"\n",
    "        dist.init_process_group(backend=backend)\n",
    "\n",
    "        if params[\"use_gpu\"]:\n",
    "            local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "            device = torch.device(f\"cuda:{local_rank}\")\n",
    "            device_ids = [local_rank]\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "            device_ids = None\n",
    "\n",
    "        rank = dist.get_rank()\n",
    "        world_size = dist.get_world_size()\n",
    "\n",
    "        # Convert dbfs:/ URI to /dbfs/ path for local IO\n",
    "        def dbfs_to_local(path: str) -> str:\n",
    "            if path.startswith(\"dbfs:/\"):\n",
    "                return path.replace(\"dbfs:/\", \"/dbfs/\")\n",
    "            return path\n",
    "\n",
    "        train_path_dbfs = params[\"train_path\"]\n",
    "        train_path_local = dbfs_to_local(train_path_dbfs)\n",
    "\n",
    "        # --- Dataset ---\n",
    "        class ParquetFlightIterableDataset(IterableDataset):\n",
    "            def __init__(self, path_local: str, rank: int, world_size: int):\n",
    "                if os.path.isdir(path_local):\n",
    "                    all_files = sorted(glob.glob(os.path.join(path_local, \"*.parquet\")))\n",
    "                else:\n",
    "                    all_files = [path_local]\n",
    "\n",
    "                if not all_files:\n",
    "                    raise FileNotFoundError(f\"No parquet files found at: {path_local}\")\n",
    "\n",
    "                # Shard files by rank so each process reads a disjoint subset\n",
    "                self.files = all_files[rank::world_size]\n",
    "\n",
    "            def __iter__(self):\n",
    "                # Shuffle files at the START of every epoch (every time __iter__ is called)\n",
    "                random.shuffle(self.files)\n",
    "                \n",
    "                for f in self.files:\n",
    "                    try:\n",
    "                        # Load ONE file at a time\n",
    "                        pdf = pd.read_parquet(f, columns=[\"features_arr\", \"DEP_DELAY\"])\n",
    "                        \n",
    "                        if len(pdf) == 0:\n",
    "                            continue\n",
    "\n",
    "                        X = np.stack(pdf[\"features_arr\"].to_numpy()).astype(np.float32, copy=False)\n",
    "                        y = pdf[\"DEP_DELAY\"].to_numpy(dtype=np.float32, copy=False)\n",
    "\n",
    "                        for i in range(len(y)):\n",
    "                            yield torch.from_numpy(X[i]), torch.tensor(y[i])\n",
    "                    except Exception as e:\n",
    "                        print(f\"Skipping bad file {f}: {e}\")\n",
    "                        continue\n",
    "\n",
    "        # Initialize Dataset\n",
    "        dataset = ParquetFlightIterableDataset(train_path_local, rank, world_size)\n",
    "\n",
    "        # Initialize DataLoader\n",
    "        # REMOVED: sampler=... (Incompatible with IterableDataset)\n",
    "        # REMOVED: shuffle=True (Incompatible with IterableDataset, handled inside __iter__)\n",
    "        loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=params[\"batch_size\"],\n",
    "            drop_last=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=params[\"use_gpu\"],\n",
    "        )\n",
    "\n",
    "        # --- Model / Optimizer ---\n",
    "        model = PyTorchMLPRegressor_Worker(\n",
    "            params[\"input_dim\"],\n",
    "            params[\"hidden_layers\"],\n",
    "            params[\"dropout_rate\"],\n",
    "        ).to(device)\n",
    "\n",
    "        ddp_model = DDP(model, device_ids=device_ids)\n",
    "        optimizer = optim.Adam(ddp_model.parameters(), lr=params[\"learning_rate\"])\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        # --- Training Loop ---\n",
    "        for epoch in range(params[\"epochs\"]):\n",
    "            ddp_model.train()\n",
    "            \n",
    "            # REMOVED: sampler.set_epoch(epoch) -> Not needed for IterableDataset\n",
    "            \n",
    "            total_loss = 0.0\n",
    "            num_batches = 0\n",
    "\n",
    "            for xb, yb in loader:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                out = ddp_model(xb)\n",
    "                loss = criterion(out, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "\n",
    "            # Log only on rank 0\n",
    "            if rank == 0:\n",
    "                avg_loss = total_loss / max(num_batches, 1)\n",
    "                print(f\"Epoch {epoch+1}/{params['epochs']} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # --- Save Model ---\n",
    "        dist.barrier()\n",
    "        if rank == 0:\n",
    "            torch.save(model.state_dict(), params[\"model_path\"])\n",
    "\n",
    "        dist.destroy_process_group()\n",
    "\n",
    "    except Exception:\n",
    "        # Print full traceback to driver logs if something crashes\n",
    "        print(traceback.format_exc())\n",
    "        sys.exit(1)\n",
    "\n",
    "# def train_fn(X, y, params):\n",
    "#     \"\"\"\n",
    "#     Run on each worker launched by TorchDistributor.\n",
    "#     No Spark code in here.\n",
    "#     \"\"\"\n",
    "#     import os\n",
    "#     import torch\n",
    "#     import torch.nn as nn\n",
    "#     import torch.optim as optim\n",
    "#     import torch.distributed as dist\n",
    "#     from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "#     from torch.utils.data import TensorDataset, DataLoader, DistributedSampler\n",
    "#     import numpy as np\n",
    "\n",
    "#     # --- local model definition ---\n",
    "#     class PyTorchMLPRegressor_Worker(nn.Module):\n",
    "#         def __init__(self, input_dim, hidden_layers, dropout_rate=0.3):\n",
    "#             super().__init__()\n",
    "#             layers = []\n",
    "#             in_features = input_dim\n",
    "#             for units in hidden_layers:\n",
    "#                 layers.append(nn.Linear(in_features, units))\n",
    "#                 layers.append(nn.BatchNorm1d(units))\n",
    "#                 layers.append(nn.ReLU())\n",
    "#                 layers.append(nn.Dropout(dropout_rate))\n",
    "#                 in_features = units\n",
    "#             layers.append(nn.Linear(in_features, 1))\n",
    "#             self.network = nn.Sequential(*layers)\n",
    "\n",
    "#         def forward(self, x):\n",
    "#             return self.network(x).squeeze(1)\n",
    "\n",
    "#     # --- DDP / process group init ---\n",
    "#     backend = \"nccl\" if params[\"use_gpu\"] else \"gloo\"\n",
    "#     dist.init_process_group(backend=backend)\n",
    "\n",
    "#     if params[\"use_gpu\"]:\n",
    "#         local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "#         device = torch.device(f\"cuda:{local_rank}\")\n",
    "#         device_ids = [local_rank]\n",
    "#     else:\n",
    "#         device = torch.device(\"cpu\")\n",
    "#         device_ids = None\n",
    "\n",
    "#     rank = dist.get_rank()\n",
    "#     world_size = dist.get_world_size()\n",
    "\n",
    "#     # --- dataset & sampler ---\n",
    "#     X = torch.from_numpy(np.asarray(X)).float()\n",
    "#     y = torch.from_numpy(np.asarray(y)).float()\n",
    "#     dataset = TensorDataset(X, y)\n",
    "#     sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=True)\n",
    "\n",
    "#     loader = DataLoader(\n",
    "#         dataset,\n",
    "#         batch_size=params[\"batch_size\"],\n",
    "#         sampler=sampler,\n",
    "#         drop_last=False,\n",
    "#     )\n",
    "\n",
    "#     # --- model / optimizer / loss ---\n",
    "#     model = PyTorchMLPRegressor_Worker(\n",
    "#         params[\"input_dim\"],\n",
    "#         params[\"hidden_layers\"],\n",
    "#         params[\"dropout_rate\"],\n",
    "#     ).to(device)\n",
    "\n",
    "#     ddp_model = DDP(model, device_ids=device_ids)\n",
    "#     optimizer = optim.Adam(ddp_model.parameters(), lr=params[\"learning_rate\"])\n",
    "#     criterion = nn.MSELoss()\n",
    "\n",
    "#     # --- training loop ---\n",
    "#     for epoch in range(params[\"epochs\"]):\n",
    "#         ddp_model.train()\n",
    "#         sampler.set_epoch(epoch)  # important for DistributedSampler\n",
    "#         total_loss = 0.0\n",
    "#         num_batches = 0\n",
    "\n",
    "#         for xb, yb in loader:\n",
    "#             xb = xb.to(device)\n",
    "#             yb = yb.to(device)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             out = ddp_model(xb)\n",
    "#             loss = criterion(out, yb)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             total_loss += loss.item()\n",
    "#             num_batches += 1\n",
    "\n",
    "#         # log from rank 0 only\n",
    "#         if rank == 0:\n",
    "#             avg_loss = total_loss / max(num_batches, 1)\n",
    "#             print(f\"Epoch {epoch+1}/{params['epochs']} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "#     # --- save model once on rank 0 ---\n",
    "#     dist.barrier()\n",
    "#     if rank == 0:\n",
    "#         torch.save(model.state_dict(), params[\"model_path\"])\n",
    "\n",
    "#     dist.destroy_process_group()\n",
    "\n",
    "\n",
    "# --- DRIVER-SIDE MLP MODEL DEFINITION ---\n",
    "\n",
    "class PyTorchMLPRegressor(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard PyTorch MLP model for regression.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_layers, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        in_features = input_dim\n",
    "        \n",
    "        for units in hidden_layers:\n",
    "            # Dense Layer\n",
    "            layers.append(nn.Linear(in_features, units))\n",
    "            \n",
    "            # Batch Normalization\n",
    "            layers.append(nn.BatchNorm1d(units))\n",
    "            \n",
    "            # Activation and Regularization\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            \n",
    "            in_features = units\n",
    "            \n",
    "        # Output layer (Regression: 1 unit, no activation)\n",
    "        layers.append(nn.Linear(in_features, 1))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x).squeeze(1)  # Squeeze to match target shape\n",
    "\n",
    "\n",
    "# --- PYTORCH SPARK ESTIMATOR ---\n",
    "\n",
    "# class SparkPyTorchEstimator:\n",
    "#     def __init__(self, hidden_layers=None, dropout_rate=0.3, learning_rate=0.001, \n",
    "#                  batch_size=256, epochs=30, num_processes=None, infer_batch_size=None):\n",
    "        \n",
    "#         self.hidden_layers = hidden_layers or [128, 64]\n",
    "#         self.dropout_rate = dropout_rate\n",
    "#         self.learning_rate = learning_rate\n",
    "#         self.batch_size = batch_size\n",
    "#         self.epochs = epochs\n",
    "\n",
    "#         # NEW: separate inference batch size (defaults to training batch size)\n",
    "#         self.infer_batch_size = infer_batch_size or batch_size\n",
    "        \n",
    "#         # 1. Determine GPU availability\n",
    "#         self.use_gpu = torch.cuda.is_available()\n",
    "        \n",
    "#         # 2. Determine the number of processes (Fixes NoneType error)\n",
    "#         if num_processes is None:\n",
    "#             if self.use_gpu:\n",
    "#                 # If GPU is available, use one process per GPU\n",
    "#                 self.num_processes = torch.cuda.device_count()\n",
    "#             else:\n",
    "#                 # If CPU is used, default to 1 process if not specified\n",
    "#                 self.num_processes = 1\n",
    "#         else:\n",
    "#             self.num_processes = num_processes\n",
    "            \n",
    "#         # Ensure minimum of 1 process\n",
    "#         if self.num_processes < 1:\n",
    "#             self.num_processes = 1\n",
    "\n",
    "#         self.model_path = \"/dbfs/tmp/torch_mlp_state_dict.pth\"\n",
    "#         self.input_dim = None\n",
    "        \n",
    "#     def fit(self, df):\n",
    "#         \"\"\"\n",
    "#         Uses TorchDistributor for distributed training.\n",
    "#         \"\"\"\n",
    "#         if self.input_dim is None:\n",
    "#             first_row = df.select(vector_to_array(\"scaled_features\")).head()\n",
    "#             self.input_dim = len(first_row[0])\n",
    "\n",
    "#         df_train = df.select(\n",
    "#             vector_to_array(\"scaled_features\").alias(\"features_arr\"),\n",
    "#             F.col(\"DEP_DELAY\")\n",
    "#         ).dropna()\n",
    "\n",
    "#         #  HACK HACK HACK HACK use 10% sample\n",
    "#         df_train = df_train.sample(withReplacement=False, fraction=0.1, seed=42)\n",
    " \n",
    "#         # Repartition for better shuffle balance (not used directly by TorchDistributor)\n",
    "#         num_partitions = self.num_processes * 4\n",
    "#         df_train = df_train.repartition(num_partitions)\n",
    "\n",
    "#         # Collect as pandas and build NumPy arrays\n",
    "#         pdf = df_train.toPandas()\n",
    "#         X = np.stack(pdf[\"features_arr\"].values)\n",
    "#         y = pdf[\"DEP_DELAY\"].values.astype(np.float32)\n",
    "\n",
    "#         params = {\n",
    "#             \"input_dim\": self.input_dim,\n",
    "#             \"hidden_layers\": self.hidden_layers,\n",
    "#             \"dropout_rate\": self.dropout_rate,\n",
    "#             \"learning_rate\": self.learning_rate,\n",
    "#             \"batch_size\": self.batch_size,\n",
    "#             \"epochs\": self.epochs,\n",
    "#             \"use_gpu\": self.use_gpu,\n",
    "#             \"model_path\": self.model_path,\n",
    "#         }\n",
    "\n",
    "#         distributor = TorchDistributor(\n",
    "#             num_processes=self.num_processes,\n",
    "#             local_mode=False,\n",
    "#             use_gpu=self.use_gpu,\n",
    "#         )\n",
    "\n",
    "#         # Ensure the parent directory for the model file exists\n",
    "#         model_path_obj = Path(self.model_path)\n",
    "#         model_path_obj.parent.mkdir(parents=True, exist_ok=True)\n",
    "#         print(f\"Starting distributed training. Processes: {self.num_processes}, CUDA: {self.use_gpu}\")\n",
    "\n",
    "#         # NOTE: args are exactly what train_fn expects: (X, y, params)\n",
    "#         distributor.run(train_fn, X, y, params)\n",
    "\n",
    "#         # load model on driver\n",
    "#         self.trained_model = PyTorchMLPRegressor(\n",
    "#             self.input_dim, self.hidden_layers, self.dropout_rate\n",
    "#         )\n",
    "#         self.trained_model.load_state_dict(torch.load(self.model_path)) \n",
    "#         self.trained_model.eval()\n",
    "\n",
    "#         return self\n",
    "\n",
    "class SparkPyTorchEstimator:\n",
    "    def __init__(self, hidden_layers=None, dropout_rate=0.3, learning_rate=0.001, \n",
    "                 batch_size=256, epochs=30, num_processes=None, infer_batch_size=None):\n",
    "        \n",
    "        self.hidden_layers = hidden_layers or [128, 64]\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "\n",
    "        self.infer_batch_size = infer_batch_size or batch_size\n",
    "        \n",
    "        self.use_gpu = torch.cuda.is_available()\n",
    "        \n",
    "        if num_processes is None:\n",
    "            if self.use_gpu:\n",
    "                self.num_processes = torch.cuda.device_count()\n",
    "            else:\n",
    "                self.num_processes = 1\n",
    "        else:\n",
    "            self.num_processes = max(1, num_processes)\n",
    "\n",
    "        self.model_path = \"/dbfs/tmp/torch_mlp_state_dict.pth\"\n",
    "        self.input_dim = None\n",
    "        \n",
    "    def fit(self, df):\n",
    "        \"\"\"\n",
    "        Uses TorchDistributor for distributed training, without collecting\n",
    "        the entire dataset to the driver.\n",
    "        \"\"\"\n",
    "        # Optional: clear old Spark caches to avoid leftover 60M stuff\n",
    "        df.sparkSession.catalog.clearCache()\n",
    "\n",
    "        # Determine input_dim from a single row\n",
    "        if self.input_dim is None:\n",
    "            sample = df.select(vector_to_array(\"scaled_features\").alias(\"features_arr\")).limit(1).collect()\n",
    "            if not sample:\n",
    "                raise ValueError(\"No rows in training dataframe.\")\n",
    "            self.input_dim = len(sample[0][\"features_arr\"])\n",
    "\n",
    "        # Prepare train DF (features_arr + label)\n",
    "        df_train = df.select(\n",
    "            vector_to_array(\"scaled_features\").alias(\"features_arr\"),\n",
    "            F.col(\"DEP_DELAY\").cast(DoubleType())\n",
    "        ).dropna(subset=[\"features_arr\", \"DEP_DELAY\"])\n",
    "\n",
    "\n",
    "        unique_id = str(uuid.uuid4())\n",
    "        train_path = f\"dbfs:/tmp/mlp_train_{unique_id}\"\n",
    "        \n",
    "        \n",
    "        # Write training data to sharded parquet; this is streaming and\n",
    "        # does NOT collect everything to the driver.\n",
    "    \n",
    "        num_shards = max(self.num_processes * 200, 200)\n",
    "\n",
    "        (\n",
    "            df_train\n",
    "            .repartition(num_shards)\n",
    "            .write\n",
    "            .option(\"maxRecordsPerFile\", 200_000)  # adjust lower if needed\n",
    "            .mode(\"overwrite\")\n",
    "            .parquet(train_path)\n",
    "        )\n",
    "\n",
    "        \n",
    "        params = {\n",
    "            \"input_dim\": self.input_dim,\n",
    "            \"hidden_layers\": self.hidden_layers,\n",
    "            \"dropout_rate\": self.dropout_rate,\n",
    "            \"learning_rate\": self.learning_rate,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"epochs\": self.epochs,\n",
    "            \"use_gpu\": self.use_gpu,\n",
    "            \"model_path\": self.model_path,\n",
    "            \"train_path\": train_path,\n",
    "        }\n",
    "\n",
    "        distributor = TorchDistributor(\n",
    "            num_processes=self.num_processes,\n",
    "            local_mode=False,\n",
    "            use_gpu=self.use_gpu,\n",
    "        )\n",
    "\n",
    "        # Ensure model path directory exists\n",
    "        model_path_obj = Path(self.model_path)\n",
    "        model_path_obj.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        print(f\"Starting distributed training. Processes: {self.num_processes}, CUDA: {self.use_gpu}\")\n",
    "        distributor.run(train_fn, params)\n",
    "\n",
    "        # load model on driver\n",
    "        self.trained_model = PyTorchMLPRegressor(\n",
    "            self.input_dim, self.hidden_layers, self.dropout_rate\n",
    "        )\n",
    "        self.trained_model.load_state_dict(torch.load(self.model_path))\n",
    "        self.trained_model.eval()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        \"\"\"\n",
    "        Uses mapInPandas for parallel inference on workers, with explicit batching\n",
    "        to avoid OOM during evaluation.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'trained_model'):\n",
    "            raise ValueError(\"Model not fitted.\")\n",
    "        \n",
    "        # Serialize model weights for broadcast\n",
    "        model_state_dict = self.trained_model.state_dict()\n",
    "        schema = df.schema.add(\"prediction\", DoubleType())\n",
    "\n",
    "        # Determine if workers should use GPU (if driver used GPU)\n",
    "        use_gpu = self.use_gpu\n",
    "        infer_batch_size = self.infer_batch_size\n",
    "\n",
    "        def predict_partition_full(iterator):\n",
    "            # 1. Setup device (GPU for inference if available)\n",
    "            device = torch.device(\"cuda\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
    "            \n",
    "            # 2. Initialize and load model ONCE per worker task\n",
    "            worker_model = PyTorchMLPRegressor(\n",
    "                self.input_dim, self.hidden_layers, self.dropout_rate\n",
    "            ).to(device)\n",
    "            worker_model.load_state_dict(model_state_dict)\n",
    "            worker_model.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for pdf_batch in iterator:\n",
    "                    # Extract features from this Spark partition batch\n",
    "                    X_np = np.stack(pdf_batch[\"features_arr\"].values)\n",
    "                    n = X_np.shape[0]\n",
    "\n",
    "                    # Pre-allocate predictions on CPU as float64 to match DoubleType\n",
    "                    preds_all = np.empty(n, dtype=np.float64)\n",
    "\n",
    "                    # --- BATCHED INFERENCE TO AVOID OOM ---\n",
    "                    for start in range(0, n, infer_batch_size):\n",
    "                        end = min(start + infer_batch_size, n)\n",
    "                        inputs = torch.from_numpy(X_np[start:end]).float().to(device)\n",
    "                        preds = worker_model(inputs).cpu().numpy().astype(np.float64)\n",
    "                        preds_all[start:end] = preds\n",
    "\n",
    "                    pdf_batch[\"prediction\"] = preds_all\n",
    "                    yield pdf_batch.drop(columns=[\"features_arr\"])\n",
    "\n",
    "        # Add the array column temporarily\n",
    "        df_with_arr = df.withColumn(\"features_arr\", vector_to_array(\"scaled_features\"))\n",
    "        \n",
    "        # Final transform\n",
    "        return df_with_arr.mapInPandas(predict_partition_full, schema=schema)\n",
    "    \n",
    "\n",
    "# =====================================================\n",
    "# 2. MLP PIPELINE WRAPPER\n",
    "# =====================================================\n",
    "\n",
    "class MLPFlightDelayPipeline:\n",
    "    \"\"\"\n",
    "    Wrapper that combines Spark preprocessing + PyTorch MLP into a single estimator.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        categorical_features,\n",
    "        numerical_features,\n",
    "        mlp_params=None,\n",
    "    ):\n",
    "        self.categorical_features = categorical_features\n",
    "        self.numerical_features = numerical_features\n",
    "        self.mlp_params = mlp_params or {}\n",
    "        \n",
    "        self.preprocessing_pipeline = None\n",
    "        self.pytorch_estimator = None\n",
    "        \n",
    "    def _build_preprocessing_pipeline(self):\n",
    "        imputer = Imputer(\n",
    "            inputCols=self.numerical_features,\n",
    "            outputCols=[f\"{col}_IMPUTED\" for col in self.numerical_features],\n",
    "            strategy=\"mean\"\n",
    "        )\n",
    "        \n",
    "        indexer = StringIndexer(\n",
    "            inputCols=self.categorical_features,\n",
    "            outputCols=[f\"{col}_INDEX\" for col in self.categorical_features],\n",
    "            handleInvalid=\"keep\"\n",
    "        )\n",
    "        \n",
    "        encoder = OneHotEncoder(\n",
    "            inputCols=[f\"{col}_INDEX\" for col in self.categorical_features],\n",
    "            outputCols=[f\"{col}_VEC\" for col in self.categorical_features],\n",
    "            dropLast=False\n",
    "        )\n",
    "        \n",
    "        assembler = VectorAssembler(\n",
    "            inputCols=[f\"{col}_VEC\" for col in self.categorical_features] + \n",
    "                      [f\"{col}_IMPUTED\" for col in self.numerical_features],\n",
    "            outputCol=\"features\",\n",
    "            handleInvalid=\"skip\"\n",
    "        )\n",
    "        \n",
    "        scaler = StandardScaler(\n",
    "            inputCol=\"features\",\n",
    "            outputCol=\"scaled_features\",\n",
    "            withMean=True,\n",
    "            withStd=True\n",
    "        )\n",
    "        \n",
    "        self.preprocessing_pipeline = Pipeline(\n",
    "            stages=[imputer, indexer, encoder, assembler, scaler]\n",
    "        )\n",
    "        \n",
    "        return self.preprocessing_pipeline\n",
    "    \n",
    "    def fit(self, df):\n",
    "        # Build and fit preprocessing pipeline\n",
    "        if self.preprocessing_pipeline is None:\n",
    "            self._build_preprocessing_pipeline()\n",
    "            # Ensure numerical columns are DoubleType before fitting the Imputer\n",
    "            temp_df = df\n",
    "            for col_name in self.numerical_features:\n",
    "                temp_df = temp_df.withColumn(col_name, F.col(col_name).cast(DoubleType()))\n",
    "                \n",
    "            self.preprocessing_pipeline = self.preprocessing_pipeline.fit(temp_df)\n",
    "        \n",
    "        # Transform training data\n",
    "        preprocessed = self.preprocessing_pipeline.transform(df)\n",
    "        \n",
    "        # Build and fit PyTorch Estimator\n",
    "        self.pytorch_estimator = SparkPyTorchEstimator(**self.mlp_params)\n",
    "        self.pytorch_estimator.fit(preprocessed)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        if self.preprocessing_pipeline is None or self.pytorch_estimator is None:\n",
    "            raise ValueError(\"Pipeline not fitted yet. Call fit() first.\")\n",
    "        \n",
    "        # Apply preprocessing\n",
    "        preprocessed = self.preprocessing_pipeline.transform(df)\n",
    "        \n",
    "        # Generate predictions\n",
    "        predictions_df = self.pytorch_estimator.transform(preprocessed)\n",
    "        \n",
    "        return predictions_df\n",
    "    \n",
    "\n",
    "# =====================================================\n",
    "# 4. USAGE WITH FLIGHTDELAYCV\n",
    "# =====================================================\n",
    "\n",
    "# Feature definitions\n",
    "categorical_features = [\n",
    "    'day_of_week', 'op_carrier', 'dep_time_blk', 'arr_time_blk', 'day_of_month', 'month'\n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    'hourlyprecipitation', 'hourlysealevelpressure', 'hourlyaltimetersetting',\n",
    "    'hourlywetbulbtemperature', 'hourlystationpressure', 'hourlywinddirection',\n",
    "    'hourlyrelativehumidity', 'hourlywindspeed', 'hourlydewpointtemperature',\n",
    "    'hourlydrybulbtemperature', 'hourlyvisibility', 'crs_elapsed_time', 'distance', 'elevation'\n",
    "]\n",
    "\n",
    "\n",
    "# PyTorch hyperparameters (updated for TorchDistributor)\n",
    "mlp_params = {\n",
    "    'hidden_layers': [512, 256, 128],\n",
    "    'dropout_rate': 0.1,\n",
    "    'learning_rate': 0.005,\n",
    "    'batch_size': 128,\n",
    "    'epochs': 30,\n",
    "    # OPTIONAL: different batch size for inference to be extra safe\n",
    "    'infer_batch_size': 128,\n",
    "    # Optional: Set num_processes to override automatic GPU/CPU detection\n",
    "    # 'num_processes': 4\n",
    "}\n",
    "\n",
    "# Initialize MLP pipeline\n",
    "mlp_pipeline = MLPFlightDelayPipeline(\n",
    "    categorical_features=categorical_features,\n",
    "    numerical_features=numerical_features,\n",
    "    mlp_params=mlp_params,\n",
    ")\n",
    "\n",
    "# Your FlightDelayCV usage (assuming cv.FlightDelayCV is defined/imported elsewhere)\n",
    "crossvalidator = FlightDelayCV(\n",
    "    estimator=mlp_pipeline,\n",
    "    version=\"60M\"\n",
    ")\n",
    "\n",
    "# Run cross-validation\n",
    "cv_results = crossvalidator.fit()\n",
    "print(\"Cross-Validation Results:\")\n",
    "print(cv_results)\n",
    "display(cv_results)\n",
    "\n",
    "# # Evaluate on held-out test fold\n",
    "# test_results = crossvalidator.evaluate()\n",
    "# print(\"\\nTest Set Results:\")\n",
    "# print(test_results)\n",
    " "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) MLP with PyTorch GPU (60M)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
