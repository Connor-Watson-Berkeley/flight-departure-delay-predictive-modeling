{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6d3daea-7b87-4ac3-a8aa-dc2e45e8ea74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "cv.py (simplified, CUSTOM-only, no parametrization)\n",
    "\n",
    "Assumptions:\n",
    "- Folds were created from split.py with N_FOLDS = 3 and CREATE_TEST_FOLD = True\n",
    "- Therefore total fold indices written = 4:\n",
    "    FOLD_1_VAL, FOLD_2_VAL, FOLD_3_VAL, FOLD_4_TEST\n",
    "- Files live in:\n",
    "    dbfs:/mnt/mids-w261/student-groups/Group_4_2/processed\n",
    "- File naming:\n",
    "    OTPW_CUSTOM_{VERSION}_FOLD_{i}_{TRAIN|VAL|TEST}.parquet\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "mlflow.autolog(disable=True)\n",
    "\n",
    "# -----------------------------\n",
    "# HARD-CODED GLOBALS\n",
    "# -----------------------------\n",
    "FOLDER_PATH = \"dbfs:/mnt/mids-w261/student-groups/Group_4_2/processed\"\n",
    "SOURCE = \"CUSTOM\"\n",
    "VERSIONS = [\"3M\", \"12M\"]\n",
    "\n",
    "# 3 CV folds + 1 test fold = 4 total fold indices\n",
    "TOTAL_FOLDS = 4\n",
    "\n",
    "\n",
    "class FlightDelayDataLoader:\n",
    "    \"\"\"\n",
    "    CUSTOM-only loader that guarantees all numerical features are cast to double.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.folder_path = \"dbfs:/mnt/mids-w261/student-groups/Group_4_2/processed\"\n",
    "        self.source = \"CUSTOM\"\n",
    "        self.folds = {}\n",
    "        self.versions = [\"3M\", \"12M\"]\n",
    "\n",
    "        self.numerical_features = [\n",
    "            'hourlyprecipitation',\n",
    "            'hourlysealevelpressure',\n",
    "            'hourlyaltimetersetting',\n",
    "            'hourlywetbulbtemperature',\n",
    "            'hourlystationpressure',\n",
    "            'hourlywinddirection',\n",
    "            'hourlyrelativehumidity',\n",
    "            'hourlywindspeed',\n",
    "            'hourlydewpointtemperature',\n",
    "            'hourlydrybulbtemperature',\n",
    "            'hourlyvisibility',\n",
    "            'crs_elapsed_time', # scheduled flight time\n",
    "            'quarter', # inferred from month\n",
    "            'flights', # number of flights? always 1?\n",
    "            'distance', # flight distance, probably important\n",
    "            'year', # excluded bc new predictions will always be in a new year\n",
    "            # latitude and longitude not very useful in linear regression\n",
    "            'origin_station_lat',\n",
    "            'origin_station_lon',\n",
    "            'origin_airport_lat',\n",
    "            'origin_airport_lon',\n",
    "            'origin_station_dis',\n",
    "            'dest_station_lat',\n",
    "            'dest_station_lon',\n",
    "            'dest_airport_lat',\n",
    "            'dest_airport_lon',\n",
    "            'dest_station_dis',\n",
    "            'latitude',\n",
    "            'longitude',\n",
    "            'elevation',\n",
    "        ]\n",
    "\n",
    "    def _cast_numerics(self, df):\n",
    "        \"\"\"\n",
    "        Safely cast all configured numeric columns to doubles.\n",
    "        Handles common bad values like '', 'NA', 'M', 'T', '.', etc.\n",
    "        \"\"\"\n",
    "\n",
    "        # Patterns that should be treated as null\n",
    "        NULL_PAT = r'^(NA|N/A|NULL|null|None|none|\\\\N|\\\\s*|\\\\.|M|T)$'\n",
    "\n",
    "        for colname in self.numerical_features:\n",
    "            if colname in df.columns:\n",
    "                df = df.withColumn(\n",
    "                    colname,\n",
    "                    F.regexp_replace(F.col(colname).cast(\"string\"), NULL_PAT, \"\")\n",
    "                    .cast(\"double\")\n",
    "                )\n",
    "\n",
    "        # Explicitly cast labels to expected numeric types\n",
    "        if \"DEP_DELAY\" in df.columns:\n",
    "            df = df.withColumn(\"DEP_DELAY\", F.col(\"DEP_DELAY\").cast(\"double\"))\n",
    "        if \"DEP_DEL15\" in df.columns:\n",
    "            df = df.withColumn(\"DEP_DEL15\", F.col(\"DEP_DEL15\").cast(\"int\"))\n",
    "        if \"SEVERE_DEL60\" in df.columns:\n",
    "            df = df.withColumn(\"SEVERE_DEL60\", F.col(\"SEVERE_DEL60\").cast(\"int\"))\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _load_parquet(self, name):\n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "        df = spark.read.parquet(f\"{self.folder_path}/{name}.parquet\")\n",
    "        df = self._cast_numerics(df)\n",
    "        return df\n",
    "\n",
    "    def _load_version(self, version):\n",
    "        folds = []\n",
    "        for fold_idx in range(1, 4 + 1):  # 3 CV folds + 1 test\n",
    "            train_name = f\"OTPW_{self.source}_{version}_FOLD_{fold_idx}_TRAIN\"\n",
    "            train_df = self._load_parquet(train_name)\n",
    "\n",
    "            if fold_idx < 4:\n",
    "                val_name = f\"OTPW_{self.source}_{version}_FOLD_{fold_idx}_VAL\"\n",
    "                val_df = self._load_parquet(val_name)\n",
    "                folds.append((train_df, val_df))\n",
    "            else:\n",
    "                test_name = f\"OTPW_{self.source}_{version}_FOLD_{fold_idx}_TEST\"\n",
    "                test_df = self._load_parquet(test_name)\n",
    "                folds.append((train_df, test_df))\n",
    "\n",
    "        return folds\n",
    "\n",
    "    def load(self):\n",
    "        for version in self.versions:\n",
    "            self.folds[version] = self._load_version(version)\n",
    "\n",
    "    def get_version(self, version):\n",
    "        return self.folds[version]\n",
    "\n",
    "# -----------------------------\n",
    "# EVALUATOR (NULL-SAFE RMSE)\n",
    "# -----------------------------\n",
    "class FlightDelayEvaluator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        prediction_col=\"prediction\",\n",
    "        numeric_label_col=\"DEP_DELAY\",\n",
    "        binary_label_col=\"DEP_DEL15\",\n",
    "        severe_label_col=\"SEVERE_DEL60\",\n",
    "    ):\n",
    "        self.prediction_col = prediction_col\n",
    "        self.numeric_label_col = numeric_label_col\n",
    "        self.binary_label_col = binary_label_col\n",
    "        self.severe_label_col = severe_label_col\n",
    "\n",
    "        self.rmse_evaluator = RegressionEvaluator(\n",
    "            predictionCol=prediction_col,\n",
    "            labelCol=numeric_label_col,\n",
    "            metricName=\"rmse\"\n",
    "        )\n",
    "\n",
    "    def calculate_rmse(self, predictions_df):\n",
    "        # Drop any residual nulls before RegressionEvaluator sees them\n",
    "        clean = predictions_df.dropna(\n",
    "            subset=[self.numeric_label_col, self.prediction_col]\n",
    "        )\n",
    "        return self.rmse_evaluator.evaluate(clean)\n",
    "\n",
    "    def _calculate_classification_metrics(self, predictions_df, threshold, label_col):\n",
    "        # Null-safe for classification too\n",
    "        df = predictions_df.dropna(subset=[self.prediction_col, label_col])\n",
    "\n",
    "        pred_binary_col = f\"pred_binary_{threshold}\"\n",
    "        df = df.withColumn(\n",
    "            pred_binary_col,\n",
    "            F.when(F.col(self.prediction_col) >= threshold, 1).otherwise(0)\n",
    "        )\n",
    "\n",
    "        tp = df.filter((F.col(pred_binary_col) == 1) & (F.col(label_col) == 1)).count()\n",
    "        fp = df.filter((F.col(pred_binary_col) == 1) & (F.col(label_col) == 0)).count()\n",
    "        tn = df.filter((F.col(pred_binary_col) == 0) & (F.col(label_col) == 0)).count()\n",
    "        fn = df.filter((F.col(pred_binary_col) == 0) & (F.col(label_col) == 1)).count()\n",
    "\n",
    "        total = tp + fp + tn + fn\n",
    "        precision = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0.0\n",
    "        accuracy = (tp + tn) / total if total else 0.0\n",
    "\n",
    "        return dict(tp=tp, fp=fp, tn=tn, fn=fn,\n",
    "                    precision=precision, recall=recall, f1=f1, accuracy=accuracy)\n",
    "\n",
    "    def calculate_otpa_metrics(self, predictions_df):\n",
    "        return self._calculate_classification_metrics(\n",
    "            predictions_df, threshold=15, label_col=self.binary_label_col\n",
    "        )[\"accuracy\"]\n",
    "\n",
    "    def calculate_sddr_metrics(self, predictions_df):\n",
    "        return self._calculate_classification_metrics(\n",
    "            predictions_df, threshold=60, label_col=self.severe_label_col\n",
    "        )[\"recall\"]\n",
    "\n",
    "    def evaluate(self, predictions_df):\n",
    "        return {\n",
    "            \"rmse\": self.calculate_rmse(predictions_df),\n",
    "            \"otpa\": self.calculate_otpa_metrics(predictions_df),\n",
    "            \"sddr\": self.calculate_sddr_metrics(predictions_df),\n",
    "        }\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# CROSS-VALIDATOR (NO PARAMS)\n",
    "# -----------------------------\n",
    "class FlightDelayCV:\n",
    "    def __init__(self, estimator, dataloader, version):\n",
    "        self.estimator = estimator\n",
    "        self.version = version\n",
    "\n",
    "        if dataloader:\n",
    "            self.data_loader = dataloader\n",
    "        else:\n",
    "            self.data_loader = FlightDelayDataLoader()\n",
    "            self.data_loader.load()\n",
    "\n",
    "        self.evaluator = FlightDelayEvaluator()\n",
    "        self.folds = self.data_loader.get_version(version)\n",
    "\n",
    "        self.metrics = []\n",
    "        self.models = []\n",
    "        self.test_metric = None\n",
    "        self.test_model = None\n",
    "\n",
    "    def fit(self):\n",
    "        # CV folds only (exclude last test fold)\n",
    "        for train_df, val_df in self.folds[:-1]:\n",
    "            model = self.estimator.fit(train_df)\n",
    "            preds = model.transform(val_df)\n",
    "\n",
    "            metric = self.evaluator.evaluate(preds)\n",
    "            self.metrics.append(metric)\n",
    "            self.models.append(model)\n",
    "\n",
    "        m = pd.DataFrame(self.metrics)\n",
    "        m.loc[\"mean\"] = m.mean()\n",
    "        m.loc[\"std\"] = m.std()\n",
    "        return m\n",
    "\n",
    "    def evaluate(self):\n",
    "        train_df, test_df = self.folds[-1]\n",
    "        self.test_model = self.estimator.fit(train_df)\n",
    "        preds = self.test_model.transform(test_df)\n",
    "        self.test_metric = self.evaluator.evaluate(preds)\n",
    "        return self.test_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7d86bcd-729e-46c2-aaa3-603e14cc1140",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_loader = FlightDelayDataLoader()\n",
    "data_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b002a4e6-98e1-40ac-adcc-cea5eaf4c813",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "folds = data_loader.get_version(\"3M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "419a18dc-f068-45bf-bad8-307502463042",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# MLP REGRESSOR INTEGRATED WITH FLIGHTDELAYCV\n",
    "# =====================================================\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F, Window\n",
    "from pyspark.ml.feature import (\n",
    "    Imputer, StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    ")\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# from horovod.spark.keras import HorovodRunner\n",
    "from tensorflow.keras import layers, regularizers, callbacks\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 1. MLP ESTIMATOR (Spark-compatible interface)\n",
    "# =====================================================\n",
    "\n",
    "\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class SparkMLPRegressor:\n",
    "    def __init__(self, hidden_layers=None, dropout_rate=0.3, learning_rate=0.001, \n",
    "                 batch_size=256, epochs=1, early_stopping=True, patience=10, verbose=1): \n",
    "                 # NOTE: epochs set to 1 here, we will loop through data manually.\n",
    "        \n",
    "        self.hidden_layers = hidden_layers or [128, 64]\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs # Total passes over data (external loop)\n",
    "        self.early_stopping = early_stopping\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.model = None\n",
    "        self.input_dim = None\n",
    "\n",
    "    def _build_model(self, input_dim):\n",
    "        # (Model building logic remains the same)\n",
    "        self.input_dim = input_dim\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Input(shape=(input_dim,)))\n",
    "        \n",
    "        # ... (layers, batch norm, dropout)\n",
    "        model.add(tf.keras.layers.Dense(self.hidden_layers[0], kernel_regularizer=tf.keras.regularizers.l2(1e-4)))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.Activation('relu'))\n",
    "        model.add(tf.keras.layers.Dropout(self.dropout_rate))\n",
    "        \n",
    "        for units in self.hidden_layers[1:]:\n",
    "            model.add(tf.keras.layers.Dense(units, kernel_regularizer=tf.keras.regularizers.l2(1e-4)))\n",
    "            model.add(tf.keras.layers.BatchNormalization())\n",
    "            model.add(tf.keras.layers.Activation('relu'))\n",
    "            model.add(tf.keras.layers.Dropout(self.dropout_rate))\n",
    "            \n",
    "        model.add(tf.keras.layers.Dense(1)) \n",
    "        \n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        # We compile with a loss, but we will use train_on_batch, not model.fit\n",
    "        model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "        self.model = model\n",
    "        return model\n",
    "\n",
    "    def fit(self, df):\n",
    "        \"\"\"\n",
    "        FIXED: Uses rdd.toLocalIterator() for sequential, chunked training on the driver.\n",
    "        This avoids the OOM crash.\n",
    "        \"\"\"\n",
    "        print(\"Starting sequential training on driver in small batches...\")\n",
    "        \n",
    "        # 1. Prepare data stream: Convert Vector to Array and filter nulls/nans\n",
    "        df_arrays = df.select(\n",
    "            vector_to_array(\"scaled_features\").alias(\"features_arr\"), \n",
    "            \"DEP_DELAY\"\n",
    "        )\n",
    "        df_arrays = df_arrays.filter(\n",
    "            F.col(\"features_arr\").isNotNull() & F.col(\"DEP_DELAY\").isNotNull()\n",
    "        )\n",
    "        df_arrays = df_arrays.filter(F.size(F.col(\"features_arr\")) > 0)\n",
    "        \n",
    "        # Get the input dimension (need to get one row to initialize model)\n",
    "        try:\n",
    "            sample_row = df_arrays.head()\n",
    "            self.input_dim = len(sample_row[\"features_arr\"])\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to get feature dimension: {e}\")\n",
    "\n",
    "        if self.model is None:\n",
    "            self._build_model(self.input_dim)\n",
    "\n",
    "        # 2. Setup training loop\n",
    "        # We use a simple counter for manual epoch tracking.\n",
    "        current_patience = 0\n",
    "        best_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            print(f\"--- Epoch {epoch + 1}/{self.epochs} ---\")\n",
    "            \n",
    "            # Get an iterator that pulls data from workers without OOM on driver\n",
    "            data_iterator = df_arrays.rdd.toLocalIterator()\n",
    "            \n",
    "            features_buffer = []\n",
    "            labels_buffer = []\n",
    "            total_loss = 0\n",
    "            total_batches = 0\n",
    "            \n",
    "            for row in data_iterator:\n",
    "                # 3. Accumulate rows into a batch\n",
    "                features_buffer.append(row[\"features_arr\"])\n",
    "                labels_buffer.append(row[\"DEP_DELAY\"])\n",
    "                \n",
    "                if len(features_buffer) >= self.batch_size:\n",
    "                    # 4. Convert buffered lists to NumPy arrays\n",
    "                    X_batch = np.stack(features_buffer)\n",
    "                    y_batch = np.array(labels_buffer)\n",
    "                    \n",
    "                    # 5. Train on the batch and get loss/metrics\n",
    "                    metrics = self.model.train_on_batch(X_batch, y_batch)\n",
    "                    total_loss += metrics[0]\n",
    "                    total_batches += 1\n",
    "                    \n",
    "                    # Clear the buffers for the next batch\n",
    "                    features_buffer = []\n",
    "                    labels_buffer = []\n",
    "            \n",
    "            # Handle the final partial batch, if any\n",
    "            if len(features_buffer) > 0:\n",
    "                X_batch = np.stack(features_buffer)\n",
    "                y_batch = np.array(labels_buffer)\n",
    "                metrics = self.model.train_on_batch(X_batch, y_batch)\n",
    "                total_loss += metrics[0]\n",
    "                total_batches += 1\n",
    "\n",
    "            # Calculate and log mean loss for the epoch\n",
    "            epoch_loss = total_loss / total_batches if total_batches > 0 else 0\n",
    "            if self.verbose:\n",
    "                print(f\"Epoch {epoch + 1} finished. Mean Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "            # 6. Manual Early Stopping check (Simple implementation)\n",
    "            if self.early_stopping:\n",
    "                if epoch_loss < best_loss:\n",
    "                    best_loss = epoch_loss\n",
    "                    current_patience = 0\n",
    "                    # NOTE: In a real system, you would save model weights here\n",
    "                else:\n",
    "                    current_patience += 1\n",
    "                    if current_patience >= self.patience:\n",
    "                        print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "                        break\n",
    "\n",
    "        print(\"Sequential training complete.\")\n",
    "        return self\n",
    "    \n",
    "    # The transform method remains the same and relies on mapInPandas.\n",
    "    # ... (transform method)\n",
    "\n",
    "# import mlflow\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from pyspark.sql import functions as F\n",
    "# from pyspark.sql.functions import pandas_udf\n",
    "# from pyspark.ml.functions import vector_to_array  # Optimization for Spark 3.0+\n",
    "# from pyspark.sql.types import DoubleType\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "\n",
    "# class SparkMLPRegressor:\n",
    "#     \"\"\"\n",
    "#     SparkMLPRegressor using HorovodRunner for distributed training.\n",
    "#     The model is now saved/loaded via MLflow, which is the best practice \n",
    "#     in a distributed Databricks environment.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, hidden_layers=None, dropout_rate=0.3, learning_rate=0.001, \n",
    "#                  batch_size=256, epochs=30, early_stopping=True, patience=3, verbose=1, \n",
    "#                  num_proc=None):\n",
    "        \n",
    "#         self.hidden_layers = hidden_layers or [128, 64]\n",
    "#         self.dropout_rate = dropout_rate\n",
    "#         self.learning_rate = learning_rate\n",
    "#         self.batch_size = batch_size\n",
    "#         self.epochs = epochs\n",
    "#         self.early_stopping = early_stopping\n",
    "#         self.patience = patience\n",
    "#         self.verbose = verbose\n",
    "#         # Horovod setting: use one process per worker core/CPU\n",
    "#         self.num_proc = num_proc \n",
    "        \n",
    "#         self.model = None\n",
    "#         self.input_dim = None\n",
    "\n",
    "#     @staticmethod\n",
    "#     def _build_model(input_dim, params):\n",
    "#         \"\"\"Helper function to build the Keras model.\"\"\"\n",
    "#         model = tf.keras.Sequential()\n",
    "#         model.add(tf.keras.layers.Input(shape=(input_dim,)))\n",
    "        \n",
    "#         # First layer\n",
    "#         model.add(tf.keras.layers.Dense(params['hidden_layers'][0], \n",
    "#                                         kernel_regularizer=tf.keras.regularizers.l2(1e-4)))\n",
    "#         model.add(tf.keras.layers.BatchNormalization())\n",
    "#         model.add(tf.keras.layers.Activation('relu'))\n",
    "#         model.add(tf.keras.layers.Dropout(params['dropout_rate']))\n",
    "        \n",
    "#         # Hidden layers\n",
    "#         for units in params['hidden_layers'][1:]:\n",
    "#             model.add(tf.keras.layers.Dense(units, \n",
    "#                                             kernel_regularizer=tf.keras.regularizers.l2(1e-4)))\n",
    "#             model.add(tf.keras.layers.BatchNormalization())\n",
    "#             model.add(tf.keras.layers.Activation('relu'))\n",
    "#             model.add(tf.keras.layers.Dropout(params['dropout_rate']))\n",
    "            \n",
    "#         model.add(tf.keras.layers.Dense(1)) \n",
    "        \n",
    "#         optimizer = tf.keras.optimizers.Adam(learning_rate=params['learning_rate'])\n",
    "#         model.compile(optimizer=optimizer, loss='mse', metrics=['mae','rmse'])\n",
    "        \n",
    "#         return model\n",
    "\n",
    "#     @staticmethod\n",
    "#     def _train_fn(input_dim, params):\n",
    "#         \"\"\"\n",
    "#         The training function executed by HorovodRunner on each worker.\n",
    "#         It must accept the Keras/TensorFlow data feed setup by Horovod.\n",
    "#         \"\"\"\n",
    "#         # 1. Import horovod inside the function, as it is only needed on the workers\n",
    "#         import horovod.tensorflow.keras as hvd\n",
    "\n",
    "#         # 2. Horovod initialization\n",
    "#         hvd.init()\n",
    "        \n",
    "#         # 3. Pin GPU (if available) - Critical for GPU clusters\n",
    "#         gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "#         if gpus:\n",
    "#             for gpu in gpus:\n",
    "#                 tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#             tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')\n",
    "\n",
    "#         # 4. Build Model\n",
    "#         model = SparkMLPRegressor._build_model(input_dim, params)\n",
    "\n",
    "#         # 5. Distributed Optimization\n",
    "#         optimizer = tf.keras.optimizers.Adam(learning_rate=params['learning_rate'] * hvd.size())\n",
    "#         optimizer = hvd.DistributedOptimizer(optimizer, backward_passes_per_step=1, average_aggregated_gradients=True)\n",
    "\n",
    "#         model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "#         # 6. Callbacks\n",
    "#         callbacks = [hvd.callbacks.BroadcastGlobalVariablesCallback(0)]\n",
    "#         if params['early_stopping']:\n",
    "#             callbacks.append(\n",
    "#                 tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=params['patience'], restore_best_weights=True)\n",
    "#             )\n",
    "#         # Log metrics only on rank 0 (the main worker)\n",
    "#         if hvd.rank() == 0:\n",
    "#             # Add other rank 0 callbacks here, like ModelCheckpoint or MLflow\n",
    "#             pass\n",
    "\n",
    "#         # 7. HorovodRunner provides the data as a tf.data.Dataset\n",
    "#         history = model.fit(\n",
    "#             epochs=params['epochs'],\n",
    "#             batch_size=params['batch_size'],\n",
    "#             callbacks=callbacks,\n",
    "#             verbose=params['verbose'] if hvd.rank() == 0 else 0, # Only rank 0 prints logs\n",
    "#             # Horovod automatically handles validation split and distributed data loading\n",
    "#             # It expects the data feed to come from the HorovodRunner call\n",
    "#         )\n",
    "\n",
    "#         # 8. Save Model to MLflow ONLY on rank 0\n",
    "#         if hvd.rank() == 0:\n",
    "#             # Log Keras model using MLflow's native method\n",
    "#             mlflow.tensorflow.log_model(model, \"model\", registered_model_name=\"MLP_Flight_Delay_Horovod\")\n",
    "#             # Store model weights locally for the transform step to find\n",
    "#             model.save('model_weights_for_transform.h5')\n",
    "\n",
    "\n",
    "#     def fit(self, df):\n",
    "#         \"\"\"\n",
    "#         Uses HorovodRunner to train the model across the cluster.\n",
    "#         Data stays distributed!\n",
    "#         \"\"\"\n",
    "#         print(\"Starting distributed training with HorovodRunner...\")\n",
    "        \n",
    "#         # 1. Get the input dimension from the data schema (essential)\n",
    "#         # We assume 'scaled_features' is a Vector, so we get its size from the metadata.\n",
    "#         # This requires the pipeline to be fitted first.\n",
    "#         try:\n",
    "#             self.input_dim = df.schema[\"scaled_features\"].metadata[\"ml_attr\"][\"num_attrs\"]\n",
    "#         except Exception:\n",
    "#             # Fallback for dynamic feature count (less reliable, may require a count())\n",
    "#             print(\"WARNING: Could not determine feature count from metadata. Collecting one row.\")\n",
    "#             first_row = df.select(vector_to_array(\"scaled_features\")).head()\n",
    "#             self.input_dim = len(first_row[0])\n",
    "\n",
    "#         # 2. Consolidate hyperparameters\n",
    "#         params = {\n",
    "#             'hidden_layers': self.hidden_layers,\n",
    "#             'dropout_rate': self.dropout_rate,\n",
    "#             'learning_rate': self.learning_rate,\n",
    "#             'batch_size': self.batch_size,\n",
    "#             'epochs': self.epochs,\n",
    "#             'early_stopping': self.early_stopping,\n",
    "#             'patience': self.patience,\n",
    "#             'verbose': self.verbose,\n",
    "#         }\n",
    "        \n",
    "#         # 3. Launch HorovodRunner\n",
    "#         hr = HorovodRunner(np=self.num_proc)\n",
    "        \n",
    "#         # Horovod expects the input DataFrame to contain a 'label' column\n",
    "#         df = df.withColumnRenamed(\"DEP_DELAY\", \"label\")\n",
    "        \n",
    "#         # HorovodRunner launches the _train_fn on workers, passing the DataFrame\n",
    "#         hr.run(\n",
    "#             self._train_fn, \n",
    "#             input_dim=self.input_dim, \n",
    "#             params=params, \n",
    "#             data_frames=[df],\n",
    "#             feature_cols=['scaled_features'],\n",
    "#             label_cols=['label'],\n",
    "#             verbose=0\n",
    "#         )\n",
    "        \n",
    "#         # 4. Load the trained model from the rank 0 worker's saved state\n",
    "#         # (This assumes the HR environment saves the file to a common path)\n",
    "#         self.model = tf.keras.models.load_model('model_weights_for_transform.h5')\n",
    "        \n",
    "#         return self\n",
    "\n",
    "#     def transform(self, df):\n",
    "#         \"\"\"\n",
    "#         Uses Iterator UDF (mapInPandas) for parallel inference on workers, \n",
    "#         loading the Horovod-trained model.\n",
    "#         \"\"\"\n",
    "#         if self.model is None:\n",
    "#             raise ValueError(\"Model not fitted.\")\n",
    "\n",
    "#         # Serialize model state (for broadcast to workers)\n",
    "#         model_json = self.model.to_json()\n",
    "#         model_weights = self.model.get_weights()\n",
    "#         schema = df.schema.add(\"prediction\", DoubleType())\n",
    "        \n",
    "#         def predict_partition_full(iterator):\n",
    "#             # 1. Initialize model ONCE per partition\n",
    "#             worker_model = tf.keras.models.model_from_json(model_json)\n",
    "#             worker_model.set_weights(model_weights)\n",
    "            \n",
    "#             for pdf_batch in iterator:\n",
    "#                 # 2. Extract features\n",
    "#                 # Use vector_to_array conversion from the previous step\n",
    "#                 X_batch = np.stack(pdf_batch[\"features_arr\"].values)\n",
    "#                 preds = worker_model.predict(X_batch, verbose=0).flatten()\n",
    "                \n",
    "#                 # 3. Assign and yield\n",
    "#                 pdf_batch[\"prediction\"] = preds\n",
    "#                 yield pdf_batch.drop(columns=[\"features_arr\"])\n",
    "\n",
    "#         # Add the array column temporarily\n",
    "#         df_with_arr = df.withColumn(\"features_arr\", vector_to_array(\"scaled_features\"))\n",
    "        \n",
    "#         # Final transform\n",
    "#         return df_with_arr.mapInPandas(predict_partition_full, schema=schema)\n",
    "\n",
    "# class SparkMLPRegressor:\n",
    "#     def __init__(self, hidden_layers=None, dropout_rate=0.3, learning_rate=0.001, \n",
    "#                  batch_size=256, epochs=100, early_stopping=True, patience=10, verbose=0):\n",
    "#         self.hidden_layers = hidden_layers or [512, 256, 128, 64]\n",
    "#         self.dropout_rate = dropout_rate\n",
    "#         self.learning_rate = learning_rate\n",
    "#         self.batch_size = batch_size\n",
    "#         self.epochs = epochs\n",
    "#         self.early_stopping = early_stopping\n",
    "#         self.patience = patience\n",
    "#         self.verbose = verbose\n",
    "        \n",
    "#         self.model = None\n",
    "#         self.input_dim = None\n",
    "\n",
    "#     def _build_model(self, input_dim):\n",
    "#         self.input_dim = input_dim\n",
    "#         model = keras.Sequential()\n",
    "#         model.add(keras.layers.Input(shape=(input_dim,)))\n",
    "        \n",
    "#         model.add(keras.layers.Dense(self.hidden_layers[0], kernel_regularizer=keras.regularizers.l2(1e-4)))\n",
    "#         model.add(keras.layers.BatchNormalization())\n",
    "#         model.add(keras.layers.Activation('relu'))\n",
    "#         model.add(keras.layers.Dropout(self.dropout_rate))\n",
    "        \n",
    "#         for units in self.hidden_layers[1:]:\n",
    "#             model.add(keras.layers.Dense(units, kernel_regularizer=keras.regularizers.l2(1e-4)))\n",
    "#             model.add(keras.layers.BatchNormalization())\n",
    "#             model.add(keras.layers.Activation('relu'))\n",
    "#             model.add(keras.layers.Dropout(self.dropout_rate))\n",
    "            \n",
    "#         model.add(keras.layers.Dense(1))\n",
    "        \n",
    "#         optimizer = keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "#         model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "#         self.model = model\n",
    "#         return model\n",
    "\n",
    "#     def fit(self, df):\n",
    "#         \"\"\"\n",
    "#         OPTIMIZED: Uses vector_to_array for fast arrow conversion.\n",
    "#         \"\"\"\n",
    "#         print(\"Collecting data to driver for training...\")\n",
    "        \n",
    "#         # 1. OPTIMIZATION: Convert Vector to Array<Double> inside Spark\n",
    "#         # This allows Arrow to transfer data as a native array, avoiding the slow lambda loop in Python.\n",
    "#         df_arrays = df.select(\n",
    "#             vector_to_array(\"scaled_features\").alias(\"features_arr\"), \n",
    "#             \"DEP_DELAY\"\n",
    "#         ).dropna()\n",
    "\n",
    "#         ##############################################################\n",
    "#         # >>> FIX FOR ValueError: Invalid dtype: str224 <<<\n",
    "#         # We must explicitly ensure that null/empty feature vectors (which might \n",
    "#         # result from unhandled nulls in the pipeline) are removed.\n",
    "#         df_arrays = df_arrays.filter(\n",
    "#             F.col(\"features_arr\").isNotNull() & F.col(\"DEP_DELAY\").isNotNull()\n",
    "#         )\n",
    "#         df_arrays = df_arrays.filter(F.size(F.col(\"features_arr\")) > 0)\n",
    "#         ##############################################################\n",
    "\n",
    "#         # Collect to Pandas (Arrow enabled)\n",
    "#         pdf = df_arrays.toPandas()\n",
    "        \n",
    "#         # 2. Fast Numpy Conversion (No loops!)\n",
    "#         # Stack the arrays directly\n",
    "#         X = np.stack(pdf[\"features_arr\"].values)\n",
    "#         y = pdf[\"DEP_DELAY\"].values\n",
    "        \n",
    "#         print(f\"Training on {X.shape[0]} samples with {X.shape[1]} features.\")\n",
    "        \n",
    "#         if self.model is None:\n",
    "#             self._build_model(X.shape[1])\n",
    "            \n",
    "#         callbacks_list = []\n",
    "#         if self.early_stopping:\n",
    "#             callbacks_list.append(\n",
    "#                 keras.callbacks.EarlyStopping(monitor='loss', patience=self.patience, restore_best_weights=True)\n",
    "#             )\n",
    "            \n",
    "#         self.model.fit(\n",
    "#             X, y,\n",
    "#             batch_size=self.batch_size,\n",
    "#             epochs=self.epochs,\n",
    "#             callbacks=callbacks_list,\n",
    "#             verbose=self.verbose\n",
    "#         )\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, df):\n",
    "#         \"\"\"\n",
    "#         OPTIMIZED: Uses Iterator UDF (mapInPandas) to load model ONCE per partition.\n",
    "#         \"\"\"\n",
    "#         if self.model is None:\n",
    "#             raise ValueError(\"Model not fitted.\")\n",
    "\n",
    "#         # Serialize model state\n",
    "#         model_json = self.model.to_json()\n",
    "#         model_weights = self.model.get_weights()\n",
    "        \n",
    "#         # Define the Iterator UDF\n",
    "#         def predict_partition(iterator):\n",
    "#             # 1. Initialize model ONCE per partition (Worker Task)\n",
    "#             # This happens only when the task starts, not for every batch.\n",
    "#             worker_model = keras.models.model_from_json(model_json)\n",
    "#             worker_model.set_weights(model_weights)\n",
    "            \n",
    "#             # 2. Process batches efficiently\n",
    "#             for pdf_batch in iterator:\n",
    "#                 # pdf_batch is a Pandas DataFrame for the current batch\n",
    "                \n",
    "#                 # Convert features to numpy array\n",
    "#                 # Note: We need to handle the list-of-lists structure from Spark Arrays\n",
    "#                 X_batch = np.stack(pdf_batch[\"features_arr\"].values)\n",
    "                \n",
    "#                 # Predict\n",
    "#                 preds = worker_model.predict(X_batch, verbose=0).flatten()\n",
    "                \n",
    "#                 yield pd.DataFrame({\"prediction\": preds})\n",
    "\n",
    "#         # Prepare data: Convert Vector to Array so the UDF receives clean data\n",
    "#         df_arrays = df.withColumn(\"features_arr\", vector_to_array(\"scaled_features\"))\n",
    "        \n",
    "#         # Apply mapInPandas\n",
    "#         # This expects a UDF that takes an iterator of DataFrames and yields DataFrames\n",
    "#         preds_df = df_arrays.select(\"features_arr\").mapInPandas(\n",
    "#             predict_partition, \n",
    "#             schema=\"prediction double\"\n",
    "#         )\n",
    "        \n",
    "#         # Join predictions back? \n",
    "#         # CAUTION: mapInPandas does not guarantee order if we just select columns.\n",
    "#         # Safer strategy: Include a unique ID or process row-wise.\n",
    "        \n",
    "#         # To keep it simple and safe for your Pipeline logic (preserving all columns):\n",
    "#         # We use mapInPandas on the WHOLE dataframe.\n",
    "        \n",
    "#         schema = df.schema.add(\"prediction\", DoubleType())\n",
    "        \n",
    "#         def predict_partition_full(iterator):\n",
    "#             worker_model = keras.models.model_from_json(model_json)\n",
    "#             worker_model.set_weights(model_weights)\n",
    "            \n",
    "#             for pdf_batch in iterator:\n",
    "#                 # Extract features\n",
    "#                 # pdf_batch[\"features_arr\"] will be lists\n",
    "#                 X_batch = np.stack(pdf_batch[\"features_arr\"].values)\n",
    "#                 preds = worker_model.predict(X_batch, verbose=0).flatten()\n",
    "                \n",
    "#                 # Assign to new column\n",
    "#                 pdf_batch[\"prediction\"] = preds\n",
    "                \n",
    "#                 # Return the full batch with the new column\n",
    "#                 # Drop the temp array column if you want, but schema must match\n",
    "#                 yield pdf_batch.drop(columns=[\"features_arr\"])\n",
    "\n",
    "#         # Add the array column temporarily\n",
    "#         df_with_arr = df.withColumn(\"features_arr\", vector_to_array(\"scaled_features\"))\n",
    "        \n",
    "#         # Final transform\n",
    "#         return df_with_arr.mapInPandas(predict_partition_full, schema=schema)\n",
    "\n",
    "# =====================================================\n",
    "# 2. MLP PIPELINE WRAPPER\n",
    "# =====================================================\n",
    "\n",
    "class MLPFlightDelayPipeline:\n",
    "    \"\"\"\n",
    "    Wrapper that combines Spark preprocessing + MLP into a single estimator.\n",
    "    Compatible with FlightDelayCV.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        categorical_features,\n",
    "        numerical_features,\n",
    "        mlp_params=None,\n",
    "    ):\n",
    "        self.categorical_features = categorical_features\n",
    "        self.numerical_features = numerical_features\n",
    "        self.mlp_params = mlp_params or {}\n",
    "        \n",
    "        self.preprocessing_pipeline = None\n",
    "        self.mlp_regressor = None\n",
    "        \n",
    "    def _build_preprocessing_pipeline(self):\n",
    "        \"\"\"\n",
    "        Build Spark ML pipeline for feature preprocessing.\n",
    "        \"\"\"\n",
    "        imputer = Imputer(\n",
    "            inputCols=self.numerical_features,\n",
    "            outputCols=[f\"{col}_IMPUTED\" for col in self.numerical_features],\n",
    "            strategy=\"mean\"\n",
    "        )\n",
    "        \n",
    "        indexer = StringIndexer(\n",
    "            inputCols=self.categorical_features,\n",
    "            outputCols=[f\"{col}_INDEX\" for col in self.categorical_features],\n",
    "            handleInvalid=\"keep\"\n",
    "        )\n",
    "        \n",
    "        encoder = OneHotEncoder(\n",
    "            inputCols=[f\"{col}_INDEX\" for col in self.categorical_features],\n",
    "            outputCols=[f\"{col}_VEC\" for col in self.categorical_features],\n",
    "            dropLast=False\n",
    "        )\n",
    "        \n",
    "        assembler = VectorAssembler(\n",
    "            inputCols=[f\"{col}_VEC\" for col in self.categorical_features] + \n",
    "                      [f\"{col}_IMPUTED\" for col in self.numerical_features],\n",
    "            outputCol=\"features\",\n",
    "            handleInvalid=\"skip\"\n",
    "        )\n",
    "        \n",
    "        scaler = StandardScaler(\n",
    "            inputCol=\"features\",\n",
    "            outputCol=\"scaled_features\",\n",
    "            withMean=True,\n",
    "            withStd=True\n",
    "        )\n",
    "        \n",
    "        self.preprocessing_pipeline = Pipeline(\n",
    "            stages=[imputer, indexer, encoder, assembler, scaler]\n",
    "        )\n",
    "        \n",
    "        return self.preprocessing_pipeline\n",
    "    \n",
    "    def fit(self, df):\n",
    "        \"\"\"\n",
    "        Fit preprocessing pipeline and MLP on training data.\n",
    "        Returns self for chaining.\n",
    "        \"\"\"\n",
    "        # Build and fit preprocessing pipeline (only first time)\n",
    "        if self.preprocessing_pipeline is None:\n",
    "            self._build_preprocessing_pipeline()\n",
    "            self.preprocessing_pipeline = self.preprocessing_pipeline.fit(df)\n",
    "        \n",
    "        # Transform training data using already-fitted pipeline\n",
    "        preprocessed = self.preprocessing_pipeline.transform(df)\n",
    "        \n",
    "        # Build and fit MLP\n",
    "        self.mlp_regressor = SparkMLPRegressor(**self.mlp_params)\n",
    "        self.mlp_regressor.fit(preprocessed)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        \"\"\"\n",
    "        Preprocess and generate predictions on new data.\n",
    "        \"\"\"\n",
    "        if self.preprocessing_pipeline is None or self.mlp_regressor is None:\n",
    "            raise ValueError(\"Pipeline not fitted yet. Call fit() first.\")\n",
    "        \n",
    "        # Apply preprocessing\n",
    "        preprocessed = self.preprocessing_pipeline.transform(df)\n",
    "        \n",
    "        # Generate predictions\n",
    "        predictions_df = self.mlp_regressor.transform(preprocessed)\n",
    "        \n",
    "        return predictions_df\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 3. FEATURE ENGINEERING (OPTIONAL ENHANCEMENT)\n",
    "# =====================================================\n",
    "\n",
    "class FlightDelayFeatureEngineer:\n",
    "    \"\"\"\n",
    "    Optionally add cyclical and interaction features before preprocessing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    def add_temporal_features(self):\n",
    "        \"\"\"Add sin/cos encoding for cyclical features.\"\"\"\n",
    "        self.df = self.df.withColumn(\n",
    "            \"month_sin\", F.sin(2 * F.lit(np.pi) * F.col(\"month\") / 12)\n",
    "        ).withColumn(\n",
    "            \"month_cos\", F.cos(2 * F.lit(np.pi) * F.col(\"month\") / 12)\n",
    "        ).withColumn(\n",
    "            \"day_of_week_sin\", F.sin(2 * F.lit(np.pi) * F.col(\"day_of_week\") / 7)\n",
    "        ).withColumn(\n",
    "            \"day_of_week_cos\", F.cos(2 * F.lit(np.pi) * F.col(\"day_of_week\") / 7)\n",
    "        )\n",
    "        return self\n",
    "    \n",
    "    def add_interaction_features(self):\n",
    "        \"\"\"Create interaction terms for MLP to learn.\"\"\"\n",
    "        self.df = self.df.withColumn(\n",
    "            \"distance_x_windspeed\", F.col(\"distance\") * F.col(\"hourlywindspeed\")\n",
    "        ).withColumn(\n",
    "            \"distance_x_visibility\", F.col(\"distance\") * F.col(\"hourlyvisibility\")\n",
    "        ).withColumn(\n",
    "            \"pressure_x_humidity\",\n",
    "            F.col(\"hourlysealevelpressure\") * F.col(\"hourlyrelativehumidity\")\n",
    "        )\n",
    "        return self\n",
    "    \n",
    "    def get(self):\n",
    "        return self.df\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 4. USAGE WITH FLIGHTDELAYCV\n",
    "# =====================================================\n",
    "\n",
    "# Feature definitions\n",
    "categorical_features = [\n",
    "    'day_of_week', 'op_carrier', 'dep_time_blk', 'arr_time_blk', 'day_of_month', 'month'\n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    'hourlyprecipitation', 'hourlysealevelpressure', 'hourlyaltimetersetting',\n",
    "    'hourlywetbulbtemperature', 'hourlystationpressure', 'hourlywinddirection',\n",
    "    'hourlyrelativehumidity', 'hourlywindspeed', 'hourlydewpointtemperature',\n",
    "    'hourlydrybulbtemperature', 'hourlyvisibility', 'crs_elapsed_time', 'distance', 'elevation'\n",
    "]\n",
    "\n",
    "# MLP hyperparameters (tunable for your data)\n",
    "mlp_params = {\n",
    "    'hidden_layers': [128, 64],  # Much smaller to reduce memory & computation\n",
    "    'dropout_rate': 0.3,\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 128,  # Smaller batch size for training\n",
    "    'epochs': 30,  # Fewer epochs\n",
    "    'early_stopping': True,\n",
    "    'patience': 3,\n",
    "    'verbose': 0,  # Set to 1 to see training progress\n",
    "}\n",
    "\n",
    "# Initialize MLP pipeline\n",
    "mlp_pipeline = MLPFlightDelayPipeline(\n",
    "    categorical_features=categorical_features,\n",
    "    numerical_features=numerical_features,\n",
    "    mlp_params=mlp_params,\n",
    ")\n",
    "\n",
    "# Use with FlightDelayCV (same interface as your Linear Regression and Random Forest)\n",
    "cv = FlightDelayCV(\n",
    "    estimator=mlp_pipeline,\n",
    "    dataloader=data_loader,\n",
    "    version=\"12M\"\n",
    ")\n",
    "\n",
    "# Run cross-validation\n",
    "cv_results = cv.fit()\n",
    "print(\"Cross-Validation Results:\")\n",
    "print(cv_results)\n",
    "\n",
    "# Evaluate on held-out test fold\n",
    "test_results = cv.evaluate()\n",
    "print(\"\\nTest Set Results:\")\n",
    "print(test_results)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 6. OPTIONAL: PERFORMANCE DIAGNOSTICS\n",
    "# =====================================================\n",
    "\n",
    "def profile_mlp_timing(data_loader, version=\"12M\"):\n",
    "    \"\"\"\n",
    "    Profile where time is spent: data loading, preprocessing, or training.\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    train_df, val_df = data_loader.get_version(version)[0]\n",
    "    \n",
    "    # Time 1: Preprocessing pipeline build + fit\n",
    "    print(\"Timing preprocessing pipeline...\")\n",
    "    start = time.time()\n",
    "    \n",
    "    preprocessing_pipeline = Pipeline(stages=[\n",
    "        Imputer(\n",
    "            inputCols=numerical_features,\n",
    "            outputCols=[f\"{col}_IMPUTED\" for col in numerical_features],\n",
    "            strategy=\"mean\"\n",
    "        ),\n",
    "        StringIndexer(\n",
    "            inputCols=categorical_features,\n",
    "            outputCols=[f\"{col}_INDEX\" for col in categorical_features],\n",
    "            handleInvalid=\"keep\"\n",
    "        ),\n",
    "        OneHotEncoder(\n",
    "            inputCols=[f\"{col}_INDEX\" for col in categorical_features],\n",
    "            outputCols=[f\"{col}_VEC\" for col in categorical_features],\n",
    "            dropLast=False\n",
    "        ),\n",
    "        VectorAssembler(\n",
    "            inputCols=[f\"{col}_VEC\" for col in categorical_features] + \n",
    "                      [f\"{col}_IMPUTED\" for col in numerical_features],\n",
    "            outputCol=\"features\",\n",
    "            handleInvalid=\"skip\"\n",
    "        ),\n",
    "        StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withMean=True, withStd=True),\n",
    "    ])\n",
    "    \n",
    "    fitted_pipeline = preprocessing_pipeline.fit(train_df)\n",
    "    preprocessed = fitted_pipeline.transform(train_df)\n",
    "    preprocessed.cache().count()  # Force evaluation\n",
    "    preprocess_time = time.time() - start\n",
    "    \n",
    "    # Time 2: Data extraction to NumPy\n",
    "    print(\"Timing data extraction...\")\n",
    "    start = time.time()\n",
    "    data_rdd = preprocessed.select(\"scaled_features\", \"DEP_DELAY\").rdd.map(\n",
    "        lambda row: (row[0].toArray(), float(row[1]) if row[1] is not None else np.nan)\n",
    "    )\n",
    "    collected = data_rdd.collect()\n",
    "    extract_time = time.time() - start\n",
    "    \n",
    "    # Time 3: MLP training\n",
    "    print(\"Timing MLP training...\")\n",
    "    start = time.time()\n",
    "    \n",
    "    mlp = SparkMLPRegressor(**mlp_params)\n",
    "    mlp.fit(preprocessed)\n",
    "    \n",
    "    train_time = time.time() - start\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TIMING BREAKDOWN:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Preprocessing pipeline: {preprocess_time:.2f}s\")\n",
    "    print(f\"Data extraction:       {extract_time:.2f}s\")\n",
    "    print(f\"MLP training:          {train_time:.2f}s\")\n",
    "    print(f\"Total:                 {preprocess_time + extract_time + train_time:.2f}s\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return {\n",
    "        'preprocess_time': preprocess_time,\n",
    "        'extract_time': extract_time,\n",
    "        'train_time': train_time,\n",
    "    }\n",
    "\n",
    "def grid_search_mlp(data_loader, version=\"12M\"):\n",
    "    \"\"\"\n",
    "    Simple grid search over MLP hyperparameters across CV folds.\n",
    "    \"\"\"\n",
    "    param_grid = [\n",
    "        {\n",
    "            'hidden_layers': [256, 128],\n",
    "            'dropout_rate': 0.2,\n",
    "            'epochs': 50,\n",
    "        },\n",
    "        {\n",
    "            'hidden_layers': [512, 256, 128, 64],\n",
    "            'dropout_rate': 0.3,\n",
    "            'epochs': 100,\n",
    "        },\n",
    "        {\n",
    "            'hidden_layers': [1024, 512, 256, 128, 64],\n",
    "            'dropout_rate': 0.4,\n",
    "            'epochs': 150,\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, params in enumerate(param_grid):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Testing Config {i+1}: {params}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        mlp_pipeline = MLPFlightDelayPipeline(\n",
    "            categorical_features=categorical_features,\n",
    "            numerical_features=numerical_features,\n",
    "            mlp_params={**mlp_params, **params},\n",
    "        )\n",
    "        \n",
    "        cv = FlightDelayCV(\n",
    "            estimator=mlp_pipeline,\n",
    "            dataloader=data_loader,\n",
    "            version=version\n",
    "        )\n",
    "        \n",
    "        cv_results = cv.fit()\n",
    "        test_results = cv.evaluate()\n",
    "        \n",
    "        results.append({\n",
    "            'config': params,\n",
    "            'cv_mean_rmse': cv_results.loc['mean', 'rmse'],\n",
    "            'test_rmse': test_results['rmse'],\n",
    "            'test_otpa': test_results['otpa'],\n",
    "            'test_sddr': test_results['sddr'],\n",
    "        })\n",
    "        \n",
    "        print(f\"Test RMSE: {test_results['rmse']:.4f}\")\n",
    "        print(f\"Test OTPA: {test_results['otpa']:.4f}\")\n",
    "        print(f\"Test SDDR: {test_results['sddr']:.4f}\")\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "MLP Model",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
