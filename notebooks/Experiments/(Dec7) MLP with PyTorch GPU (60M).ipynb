{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97023af5-5e99-4bc5-a86f-4d546db1f541",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "MLP Model using PyTorch + GPU Support\n",
    "====================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c7bfd43-169f-4ea5-be2a-01884b7fcea5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "- Here we build Distributed MLP models. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87c9b079-ed49-41dd-a9f5-16bc8878f3f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Spark Settings\n",
    "# to avoid OOM Error\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"32000\") \n",
    "\n",
    "import importlib.util\n",
    "import sys\n",
    "\n",
    "# Load cv module directly from file path\n",
    "cv_path = \"/Workspace/Shared/Team 4_2/flight-departure-delay-predictive-modeling/notebooks/Cross Validator/cv.py\"\n",
    "spec = importlib.util.spec_from_file_location(\"cv\", cv_path)\n",
    "cv = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(cv)\n",
    "\n",
    "\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pathlib import Path\n",
    "from pyspark.ml.feature import (\n",
    "    Imputer, StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    ")\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# >>> PYTORCH AND DISTRIBUTOR IMPORTS <<<\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pyspark.ml.torch.distributor import TorchDistributor \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- PYTORCH TRAIN FUNCTION (RUNS ON WORKERS) ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d36d1226-19f4-4016-b5f4-44f46b6f5093",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Training Function on Workers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4796359f-fe45-4a7a-a638-e988bae5f562",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- SHARED MODEL DEFINITION ---\n",
    "class PyTorchMLPRegressor(nn.Module):\n",
    "    \"\"\"\n",
    "    Shared MLP architecture for both Training (Workers) and Inference (Driver/Workers).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_layers, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_features = input_dim\n",
    "        for units in hidden_layers:\n",
    "            layers.append(nn.Linear(in_features, units))\n",
    "            layers.append(nn.BatchNorm1d(units))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            in_features = units\n",
    "        layers.append(nn.Linear(in_features, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x).squeeze(1)\n",
    "    \n",
    "\n",
    "def train_fn(params):\n",
    "    \"\"\"\n",
    "    Run on each worker launched by TorchDistributor.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import sys\n",
    "    import traceback\n",
    "    \n",
    "    # 1. Error Handling Wrapper\n",
    "    try:\n",
    "        import glob\n",
    "        import random\n",
    "        import torch\n",
    "        import torch.nn as nn\n",
    "        import torch.optim as optim\n",
    "        import torch.distributed as dist\n",
    "        from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "        from torch.utils.data import DataLoader, IterableDataset\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "\n",
    "        # --- Local Model Definition ---\n",
    "        class PyTorchMLPRegressor_Worker(nn.Module):\n",
    "            def __init__(self, input_dim, hidden_layers, dropout_rate=0.3):\n",
    "                super().__init__()\n",
    "                layers = []\n",
    "                in_features = input_dim\n",
    "                for units in hidden_layers:\n",
    "                    layers.append(nn.Linear(in_features, units))\n",
    "                    layers.append(nn.BatchNorm1d(units))\n",
    "                    layers.append(nn.ReLU())\n",
    "                    layers.append(nn.Dropout(dropout_rate))\n",
    "                    in_features = units\n",
    "                layers.append(nn.Linear(in_features, 1))\n",
    "                self.network = nn.Sequential(*layers)\n",
    "\n",
    "            def forward(self, x):\n",
    "                return self.network(x).squeeze(1)\n",
    "\n",
    "        # --- DDP Init ---\n",
    "        backend = \"nccl\" if params[\"use_gpu\"] else \"gloo\"\n",
    "        dist.init_process_group(backend=backend)\n",
    "\n",
    "        if params[\"use_gpu\"]:\n",
    "            local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "            device = torch.device(f\"cuda:{local_rank}\")\n",
    "            device_ids = [local_rank]\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "            device_ids = None\n",
    "\n",
    "        rank = dist.get_rank()\n",
    "        world_size = dist.get_world_size()\n",
    "\n",
    "        # Convert dbfs:/ URI to /dbfs/ path for local IO\n",
    "        def dbfs_to_local(path: str) -> str:\n",
    "            if path.startswith(\"dbfs:/\"):\n",
    "                return path.replace(\"dbfs:/\", \"/dbfs/\")\n",
    "            return path\n",
    "\n",
    "        train_path_dbfs = params[\"train_path\"]\n",
    "        train_path_local = dbfs_to_local(train_path_dbfs)\n",
    "\n",
    "        # --- Dataset ---\n",
    "        class ParquetFlightIterableDataset(IterableDataset):\n",
    "            def __init__(self, path_local: str, rank: int, world_size: int):\n",
    "                if os.path.isdir(path_local):\n",
    "                    all_files = sorted(glob.glob(os.path.join(path_local, \"*.parquet\")))\n",
    "                else:\n",
    "                    all_files = [path_local]\n",
    "\n",
    "                if not all_files:\n",
    "                    raise FileNotFoundError(f\"No parquet files found at: {path_local}\")\n",
    "\n",
    "                # Shard files by rank so each process reads a disjoint subset\n",
    "                self.files = all_files[rank::world_size]\n",
    "\n",
    "            def __iter__(self):\n",
    "                # Shuffle files at the START of every epoch (every time __iter__ is called)\n",
    "                random.shuffle(self.files)\n",
    "                \n",
    "                for f in self.files:\n",
    "                    try:\n",
    "                        # Load ONE file at a time\n",
    "                        pdf = pd.read_parquet(f, columns=[\"features_arr\", \"DEP_DELAY\"])\n",
    "                        \n",
    "                        if len(pdf) == 0:\n",
    "                            continue\n",
    "\n",
    "                        X = np.stack(pdf[\"features_arr\"].to_numpy()).astype(np.float32, copy=False)\n",
    "                        y = pdf[\"DEP_DELAY\"].to_numpy(dtype=np.float32, copy=False)\n",
    "\n",
    "                        for i in range(len(y)):\n",
    "                            yield torch.from_numpy(X[i]), torch.tensor(y[i])\n",
    "                    except Exception as e:\n",
    "                        print(f\"Skipping bad file {f}: {e}\")\n",
    "                        continue\n",
    "\n",
    "        # Initialize Dataset\n",
    "        dataset = ParquetFlightIterableDataset(train_path_local, rank, world_size)\n",
    "\n",
    "        # Initialize DataLoader\n",
    "        # REMOVED: sampler=... (Incompatible with IterableDataset)\n",
    "        # REMOVED: shuffle=True (Incompatible with IterableDataset, handled inside __iter__)\n",
    "        loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=params[\"batch_size\"],\n",
    "            drop_last=False,\n",
    "            #num_workers=0,\n",
    "            num_workers=4,        # <--- KEY CHANGE: Parallel Loading\n",
    "            prefetch_factor=2,    # <--- KEY CHANGE: Buffer next batches\n",
    "            pin_memory=params[\"use_gpu\"],\n",
    "        )\n",
    "\n",
    "        # --- USE GLOBAL MODEL CLASS ---\n",
    "        # No local definition needed; cloudpickle handles the closure\n",
    "        model = PyTorchMLPRegressor(\n",
    "            params[\"input_dim\"],\n",
    "            params[\"hidden_layers\"],\n",
    "            params[\"dropout_rate\"],\n",
    "        ).to(device)\n",
    "\n",
    "        ddp_model = DDP(model, device_ids=device_ids)\n",
    "        optimizer = optim.Adam(ddp_model.parameters(), lr=params[\"learning_rate\"])\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        # --- Training Loop ---\n",
    "        for epoch in range(params[\"epochs\"]):\n",
    "            ddp_model.train()\n",
    "            \n",
    "            # REMOVED: sampler.set_epoch(epoch) -> Not needed for IterableDataset\n",
    "            \n",
    "            total_loss = 0.0\n",
    "            num_batches = 0\n",
    "\n",
    "            for xb, yb in loader:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                out = ddp_model(xb)\n",
    "                loss = criterion(out, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "\n",
    "            # Log only on rank 0\n",
    "            if rank == 0:\n",
    "                avg_loss = total_loss / max(num_batches, 1)\n",
    "                print(f\"Epoch {epoch+1}/{params['epochs']} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # --- Save Model ---\n",
    "        dist.barrier()\n",
    "        if rank == 0:\n",
    "            torch.save(model.state_dict(), params[\"model_path\"])\n",
    "\n",
    "        dist.destroy_process_group()\n",
    "\n",
    "    except Exception:\n",
    "        # Print full traceback to driver logs if something crashes\n",
    "        print(traceback.format_exc())\n",
    "        sys.exit(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7d86bcd-729e-46c2-aaa3-603e14cc1140",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_loader = cv.FlightDelayDataLoader()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1e7602a-c9c3-4a22-b7ec-cfc50f458da0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Driver Side Code\n",
    "\n",
    "1. PyTorch MLP Regressor\n",
    "2. Spark PyTorch Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59e57e47-b9b7-4ae6-8505-3b5085aea41f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# PYTORCH MLP REGRESSOR INTEGRATED WITH TORCHDISTRIBUTOR\n",
    "# =====================================================\n",
    "\n",
    "\n",
    "# --- DRIVER-SIDE MLP MODEL DEFINITION ---\n",
    "\n",
    "\n",
    "class SparkPyTorchEstimator:\n",
    "    def __init__(self, hidden_layers=None, dropout_rate=0.3, learning_rate=0.001, \n",
    "                 batch_size=256, epochs=30, num_processes=None, infer_batch_size=None):\n",
    "        \n",
    "        self.hidden_layers = hidden_layers or [128, 64]\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "\n",
    "        self.infer_batch_size = infer_batch_size or batch_size\n",
    "        \n",
    "        self.use_gpu = torch.cuda.is_available()\n",
    "        \n",
    "        if num_processes is None:\n",
    "            if self.use_gpu:\n",
    "                self.num_processes = torch.cuda.device_count()\n",
    "            else:\n",
    "                self.num_processes = 1\n",
    "        else:\n",
    "            self.num_processes = max(1, num_processes)\n",
    "\n",
    "        self.model_path = \"/dbfs/tmp/torch_mlp_state_dict.pth\"\n",
    "        self.input_dim = None\n",
    "        \n",
    "    def fit(self, df):\n",
    "        \"\"\"\n",
    "        Uses TorchDistributor for distributed training, without collecting\n",
    "        the entire dataset to the driver.\n",
    "        \"\"\"\n",
    "        # Optional: clear old Spark caches to avoid leftover 60M stuff\n",
    "        # df.sparkSession.catalog.clearCache()\n",
    "\n",
    "        # Determine input_dim from a single row\n",
    "        if self.input_dim is None:\n",
    "            sample = df.select(vector_to_array(\"scaled_features\").alias(\"features_arr\")).limit(1).collect()\n",
    "            if not sample:\n",
    "                raise ValueError(\"No rows in training dataframe.\")\n",
    "            self.input_dim = len(sample[0][\"features_arr\"])\n",
    "\n",
    "        # Prepare train DF (features_arr + label)\n",
    "        df_train = df.select(\n",
    "            vector_to_array(\"scaled_features\").alias(\"features_arr\"),\n",
    "            F.col(\"DEP_DELAY\").cast(DoubleType())\n",
    "        ).dropna(subset=[\"features_arr\", \"DEP_DELAY\"])\n",
    "\n",
    "\n",
    "        unique_id = str(uuid.uuid4())\n",
    "        train_path = f\"dbfs:/tmp/mlp_train_{unique_id}\"\n",
    "        \n",
    "        \n",
    "        # Write training data to sharded parquet; this is streaming and\n",
    "        # does NOT collect everything to the driver.\n",
    "    \n",
    "        num_shards = max(self.num_processes * 200, 200)\n",
    "\n",
    "        (\n",
    "            df_train\n",
    "            .repartition(num_shards)\n",
    "            .write\n",
    "            .option(\"maxRecordsPerFile\", 200_000)  # adjust lower if needed\n",
    "            .mode(\"overwrite\")\n",
    "            .parquet(train_path)\n",
    "        )\n",
    "\n",
    "        \n",
    "        params = {\n",
    "            \"input_dim\": self.input_dim,\n",
    "            \"hidden_layers\": self.hidden_layers,\n",
    "            \"dropout_rate\": self.dropout_rate,\n",
    "            \"learning_rate\": self.learning_rate,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"epochs\": self.epochs,\n",
    "            \"use_gpu\": self.use_gpu,\n",
    "            \"model_path\": self.model_path,\n",
    "            \"train_path\": train_path,\n",
    "        }\n",
    "\n",
    "        distributor = TorchDistributor(\n",
    "            num_processes=self.num_processes,\n",
    "            local_mode=False,\n",
    "            use_gpu=self.use_gpu,\n",
    "        )\n",
    "\n",
    "        # Ensure model path directory exists\n",
    "        model_path_obj = Path(self.model_path)\n",
    "        model_path_obj.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        print(f\"Starting distributed training. Processes: {self.num_processes}, CUDA: {self.use_gpu}\")\n",
    "        distributor.run(train_fn, params)\n",
    "\n",
    "        # load model on driver\n",
    "        self.trained_model = PyTorchMLPRegressor(\n",
    "            self.input_dim, self.hidden_layers, self.dropout_rate\n",
    "        )\n",
    "        self.trained_model.load_state_dict(torch.load(self.model_path))\n",
    "        self.trained_model.eval()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        \"\"\"\n",
    "        Uses mapInPandas for parallel inference on workers, with explicit batching\n",
    "        to avoid OOM during evaluation.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'trained_model'):\n",
    "            raise ValueError(\"Model not fitted.\")\n",
    "        \n",
    "        # Serialize model weights for broadcast\n",
    "        model_state_dict = self.trained_model.state_dict()\n",
    "        schema = df.schema.add(\"prediction\", DoubleType())\n",
    "\n",
    "        # Determine if workers should use GPU (if driver used GPU)\n",
    "        use_gpu = self.use_gpu\n",
    "        infer_batch_size = self.infer_batch_size\n",
    "\n",
    "        def predict_partition_full(iterator):\n",
    "            # 1. Setup device (GPU for inference if available)\n",
    "            device = torch.device(\"cuda\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
    "            \n",
    "            # 2. Initialize and load model ONCE per worker task\n",
    "            worker_model = PyTorchMLPRegressor(\n",
    "                self.input_dim, self.hidden_layers, self.dropout_rate\n",
    "            ).to(device)\n",
    "            worker_model.load_state_dict(model_state_dict)\n",
    "            worker_model.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for pdf_batch in iterator:\n",
    "                    # Extract features from this Spark partition batch\n",
    "                    X_np = np.stack(pdf_batch[\"features_arr\"].values)\n",
    "                    n = X_np.shape[0]\n",
    "\n",
    "                    # Pre-allocate predictions on CPU as float64 to match DoubleType\n",
    "                    preds_all = np.empty(n, dtype=np.float64)\n",
    "\n",
    "                    # --- BATCHED INFERENCE TO AVOID OOM ---\n",
    "                    for start in range(0, n, infer_batch_size):\n",
    "                        end = min(start + infer_batch_size, n)\n",
    "                        inputs = torch.from_numpy(X_np[start:end]).float().to(device)\n",
    "                        preds = worker_model(inputs).cpu().numpy().astype(np.float64)\n",
    "                        preds_all[start:end] = preds\n",
    "\n",
    "                    pdf_batch[\"prediction\"] = preds_all\n",
    "                    yield pdf_batch.drop(columns=[\"features_arr\"])\n",
    "\n",
    "        # Add the array column temporarily\n",
    "        df_with_arr = df.withColumn(\"features_arr\", vector_to_array(\"scaled_features\"))\n",
    "        \n",
    "        # Final transform\n",
    "        return df_with_arr.mapInPandas(predict_partition_full, schema=schema)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d44d515f-dd60-4659-9f00-1d1df8fd1863",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### MLP Pipeline Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "738da181-7db7-459d-b4cf-7bcc19a13453",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =====================================================\n",
    "# 2. MLP PIPELINE WRAPPER\n",
    "# =====================================================\n",
    "\n",
    "class MLPFlightDelayPipeline:\n",
    "    \"\"\"\n",
    "    Wrapper that combines Spark preprocessing + PyTorch MLP into a single estimator.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        categorical_features,\n",
    "        numerical_features,\n",
    "        mlp_params=None,\n",
    "    ):\n",
    "        self.categorical_features = categorical_features\n",
    "        self.numerical_features = numerical_features\n",
    "        self.mlp_params = mlp_params or {}\n",
    "        \n",
    "        self.preprocessing_pipeline = None\n",
    "        self.pytorch_estimator = None\n",
    "        \n",
    "    def _build_preprocessing_pipeline(self):\n",
    "        imputer = Imputer(\n",
    "            inputCols=self.numerical_features,\n",
    "            outputCols=[f\"{col}_IMPUTED\" for col in self.numerical_features],\n",
    "            strategy=\"mean\"\n",
    "        )\n",
    "        \n",
    "        indexer = StringIndexer(\n",
    "            inputCols=self.categorical_features,\n",
    "            outputCols=[f\"{col}_INDEX\" for col in self.categorical_features],\n",
    "            handleInvalid=\"keep\"\n",
    "        )\n",
    "        \n",
    "        encoder = OneHotEncoder(\n",
    "            inputCols=[f\"{col}_INDEX\" for col in self.categorical_features],\n",
    "            outputCols=[f\"{col}_VEC\" for col in self.categorical_features],\n",
    "            dropLast=False\n",
    "        )\n",
    "        \n",
    "        assembler = VectorAssembler(\n",
    "            inputCols=[f\"{col}_VEC\" for col in self.categorical_features] + \n",
    "                      [f\"{col}_IMPUTED\" for col in self.numerical_features],\n",
    "            outputCol=\"features\",\n",
    "            handleInvalid=\"skip\"\n",
    "        )\n",
    "        \n",
    "        scaler = StandardScaler(\n",
    "            inputCol=\"features\",\n",
    "            outputCol=\"scaled_features\",\n",
    "            withMean=True,\n",
    "            withStd=True\n",
    "        )\n",
    "        \n",
    "        self.preprocessing_pipeline = Pipeline(\n",
    "            stages=[imputer, indexer, encoder, assembler, scaler]\n",
    "        )\n",
    "        \n",
    "        return self.preprocessing_pipeline\n",
    "    \n",
    "    def fit(self, df):\n",
    "        # Build and fit preprocessing pipeline\n",
    "        if self.preprocessing_pipeline is None:\n",
    "            self._build_preprocessing_pipeline()\n",
    "            # Ensure numerical columns are DoubleType before fitting the Imputer\n",
    "            temp_df = df\n",
    "            for col_name in self.numerical_features:\n",
    "                temp_df = temp_df.withColumn(col_name, F.col(col_name).cast(DoubleType()))\n",
    "                \n",
    "            self.preprocessing_pipeline = self.preprocessing_pipeline.fit(temp_df)\n",
    "        \n",
    "        # Transform training data\n",
    "        preprocessed = self.preprocessing_pipeline.transform(df)\n",
    "        \n",
    "        # Build and fit PyTorch Estimator\n",
    "        self.pytorch_estimator = SparkPyTorchEstimator(**self.mlp_params)\n",
    "        self.pytorch_estimator.fit(preprocessed)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        if self.preprocessing_pipeline is None or self.pytorch_estimator is None:\n",
    "            raise ValueError(\"Pipeline not fitted yet. Call fit() first.\")\n",
    "        \n",
    "        # Apply preprocessing\n",
    "        preprocessed = self.preprocessing_pipeline.transform(df)\n",
    "        \n",
    "        # Generate predictions\n",
    "        predictions_df = self.pytorch_estimator.transform(preprocessed)\n",
    "        \n",
    "        return predictions_df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "615b20c1-3ac7-4f36-999f-90323372f2e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Run for results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5e6c473-a4ed-4c31-bcbc-4bc032fec8ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =====================================================\n",
    "# 4. USAGE WITH FLIGHTDELAYCV\n",
    "# =====================================================\n",
    "\n",
    "# Feature definitions\n",
    "categorical_features = [\n",
    "    'day_of_week', 'op_carrier', 'dep_time_blk', 'arr_time_blk', 'day_of_month', 'month'\n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    'hourlyprecipitation', 'hourlysealevelpressure', 'hourlyaltimetersetting',\n",
    "    'hourlywetbulbtemperature', 'hourlystationpressure', 'hourlywinddirection',\n",
    "    'hourlyrelativehumidity', 'hourlywindspeed', 'hourlydewpointtemperature',\n",
    "    'hourlydrybulbtemperature', 'hourlyvisibility', 'crs_elapsed_time', 'distance', 'elevation',\n",
    "\n",
    "\n",
    "    #Flight Lineage Derived Features\n",
    "    # Scheduled time features (data leakage-free)\n",
    "    'scheduled_lineage_rotation_time_minutes',\n",
    "    'scheduled_lineage_turnover_time_minutes',\n",
    "\n",
    "    # Other known features (data leakage-free)\n",
    "    'prev_flight_distance',\n",
    "\n",
    "    # Safe features (intelligent data leakage handling)\n",
    "    'safe_lineage_rotation_time_minutes', # Duration between the known (or suspected) previous actual departure time and the planned departure time\n",
    "\n",
    "    # Other flight lineage features\n",
    "    'lineage_rank', # Number of recorded flights for that airplane\n",
    "]\n",
    "\n",
    "\n",
    "# PyTorch hyperparameters (updated for TorchDistributor)\n",
    "mlp_params = {\n",
    "    'hidden_layers': [512, 256, 128],\n",
    "    'dropout_rate': 0.1,\n",
    "    'learning_rate': 0.005,\n",
    "    'batch_size': 512,\n",
    "    'epochs': 30,\n",
    "    # OPTIONAL: different batch size for inference to be extra safe\n",
    "    'infer_batch_size': 128,\n",
    "    # Optional: Set num_processes to override automatic GPU/CPU detection\n",
    "    # 'num_processes': 4\n",
    "}\n",
    "\n",
    "# Initialize MLP pipeline\n",
    "mlp_pipeline = MLPFlightDelayPipeline(\n",
    "    categorical_features=categorical_features,\n",
    "    numerical_features=numerical_features,\n",
    "    mlp_params=mlp_params,\n",
    ")\n",
    "\n",
    "# Your FlightDelayCV usage (assuming cv.FlightDelayCV is defined/imported elsewhere)\n",
    "crossvalidator = cv.FlightDelayCV(\n",
    "    estimator=mlp_pipeline,\n",
    "    version=\"3M\"\n",
    ")\n",
    "\n",
    "# Run cross-validation\n",
    "cv_results = crossvalidator.fit()\n",
    "print(\"Cross-Validation Results:\")\n",
    "print(cv_results)\n",
    "display(cv_results)\n",
    "\n",
    "# # Evaluate on held-out test fold\n",
    "# test_results = crossvalidator.evaluate()\n",
    "# print(\"\\nTest Set Results:\")\n",
    "# print(test_results)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fa6e078-3747-4fc9-a974-0661de5ea02a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_eval = crossvalidator.evaluate()\n",
    "display(cv_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd701391-0d71-41c9-a5d8-f19bf30eca4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 12M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a51c1e70-6549-4899-937d-57f408e2cbbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Your FlightDelayCV usage (assuming cv.FlightDelayCV is defined/imported elsewhere)\n",
    "crossvalidator = cv.FlightDelayCV(\n",
    "    estimator=mlp_pipeline,\n",
    "    version=\"12M\"\n",
    ")\n",
    "\n",
    "# Run cross-validation\n",
    "cv_results = crossvalidator.fit()\n",
    "print(\"Cross-Validation Results:\")\n",
    "print(cv_results)\n",
    "display(cv_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "949f4cbd-7d6b-4c34-b789-b4ab98e7718b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 60M "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1771ad2f-d65a-446b-bfe4-03fae219c2c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Your FlightDelayCV usage (assuming cv.FlightDelayCV is defined/imported elsewhere)\n",
    "crossvalidator = cv.FlightDelayCV(\n",
    "    estimator=mlp_pipeline,\n",
    "    version=\"60M\"\n",
    ")\n",
    "\n",
    "# Run cross-validation\n",
    "cv_results = crossvalidator.fit()\n",
    "print(\"Cross-Validation Results:\")\n",
    "print(cv_results)\n",
    "display(cv_results)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Dec7) MLP with PyTorch GPU (60M)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
