{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6d3daea-7b87-4ac3-a8aa-dc2e45e8ea74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load local modules: Cross Validator\n",
    "import importlib.util\n",
    "cv_path = \"/Workspace/Shared/Team 4_2/flight-departure-delay-predictive-modeling/notebooks/Cross Validator/cv.py\"\n",
    "spec = importlib.util.spec_from_file_location(\"cv\", cv_path)\n",
    "cv = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(cv)\n",
    "\n",
    "import mlflow\n",
    "mlflow.autolog(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "541a126a-f783-4dc1-8623-7357d74a5b9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "class MedianRegressor:\n",
    "    def __init__(self):\n",
    "        self.value = None\n",
    "\n",
    "    def fit(self, df):\n",
    "        self.value = df.approxQuantile(\"DEP_DELAY\", [0.5], 0.01)[0]\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        return df.withColumn(\"prediction\", F.lit(self.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7d86bcd-729e-46c2-aaa3-603e14cc1140",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_loader = cv.FlightDelayDataLoader()\n",
    "data_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b002a4e6-98e1-40ac-adcc-cea5eaf4c813",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example on how to pull specific folds, not needed in your notebooks\n",
    "folds = data_loader.get_version(\"3M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cd3d046-ff40-4f49-940f-b6ef7b797964",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_set = cv.FlightDelayCV(\n",
    "    estimator=MedianRegressor(),\n",
    "    dataloader=data_loader,\n",
    "    version=\"3M\"\n",
    ")\n",
    "cv_set.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03c93a62-95be-48cf-ac08-c59a4399a1bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_set.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef4af7cc-91fb-4405-bc06-edea64ff85d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "outcome_vars = [\n",
    "    'arr_delay', 'arr_delay_new', 'arr_del15', 'arr_delay_group',\n",
    "    'dep_delay', 'dep_delay_new', 'dep_del15', 'dep_delay_group',\n",
    "    'actual_elapsed_time', 'air_time', 'wheels_on', 'taxi_in', \n",
    "    'arr_time', 'taxi_out', 'wheels_off', 'dep_time', 'cancelled', 'diverted'\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'day_of_week',\n",
    "    # 'op_unique_carrier', # redundant with op_carrier\n",
    "    'op_carrier',\n",
    "    'origin', # origin airport code\n",
    "    'origin_state_abr', # origin state abbreviation\n",
    "    'dest', # destination airport code\n",
    "    'dest_state_abr', # destination state abbreviation\n",
    "    # 'tail_num', # excluded bc each plane has this code, too many categories\n",
    "    'dep_time_blk', # not outcome var bc this is scheduled departure\n",
    "    'arr_time_blk', # not outcome var bc this is scheduled arrival\n",
    "    # 'report_type' # type of weather report, not super useful\n",
    "    # 'op_carrier_fl_num' # just the flight number\n",
    "    # 'distance_group', # likely important, but already captured in 'distance'\n",
    "\n",
    "    # 'crs_dep_time', # scheduled departure time, already captured in dep_time_blk\n",
    "    # 'crs_arr_time', # scheduled arrival time already captured in arr_time_blk\n",
    "    'day_of_month',\n",
    "    'month', # cyclical patterns\n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    'hourlyprecipitation',\n",
    "    'hourlysealevelpressure',\n",
    "    'hourlyaltimetersetting',\n",
    "    'hourlywetbulbtemperature',\n",
    "    'hourlystationpressure',\n",
    "    'hourlywinddirection',\n",
    "    'hourlyrelativehumidity',\n",
    "    'hourlywindspeed',\n",
    "    'hourlydewpointtemperature',\n",
    "    'hourlydrybulbtemperature',\n",
    "    'hourlyvisibility',\n",
    "    'crs_elapsed_time', # scheduled flight time\n",
    "    # 'quarter', # inferred from month\n",
    "    # 'flights', # number of flights? always 1?\n",
    "    'distance', # flight distance, probably important\n",
    "    # 'year', # excluded bc new predictions will always be in a new year\n",
    "    # # latitude and longitude not very useful in linear regression\n",
    "    # 'origin_station_lat',\n",
    "    # 'origin_station_lon',\n",
    "    # 'origin_airport_lat',\n",
    "    # 'origin_airport_lon',\n",
    "    # 'origin_station_dis',\n",
    "    # 'dest_station_lat',\n",
    "    # 'dest_station_lon',\n",
    "    # 'dest_airport_lat',\n",
    "    # 'dest_airport_lon',\n",
    "    # 'dest_station_dis',\n",
    "    # 'latitude',\n",
    "    # 'longitude',\n",
    "    'elevation',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecd5c675-c796-4b7d-a238-fe1cf8170c75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyspark.ml.feature import Imputer, StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=numerical_features,\n",
    "    outputCols=[f\"{col}_IMPUTED\" for col in numerical_features],\n",
    "    strategy=\"mean\"\n",
    ")\n",
    "\n",
    "indexer = StringIndexer(\n",
    "    inputCols=categorical_features,\n",
    "    outputCols=[f\"{col}_INDEX\" for col in categorical_features],\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=[f\"{col}_INDEX\" for col in categorical_features],\n",
    "    outputCols=[f\"{col}_VEC\" for col in categorical_features]\n",
    ")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[f\"{col}_VEC\" for col in categorical_features] + \n",
    "              [f\"{col}_IMPUTED\" for col in numerical_features],\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"scaled_features\",\n",
    "    withMean=True,\n",
    "    withStd=True\n",
    ")\n",
    "\n",
    "lr = LinearRegression(\n",
    "    featuresCol=\"scaled_features\", \n",
    "    labelCol=\"DEP_DELAY\",\n",
    "    elasticNetParam=0.0,\n",
    ")\n",
    "\n",
    "lr_pipe = Pipeline(stages=[imputer, indexer, encoder, assembler, scaler, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c36bd984-4ac5-45ea-b830-8b26f671998f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_lr = cv.FlightDelayCV(\n",
    "    estimator=lr_pipe,\n",
    "    dataloader=data_loader,\n",
    "    version=\"60M\"\n",
    ")\n",
    "cv_lr.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81ea3775-e2f8-4d65-bb94-5b804fa68cb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer, StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Feature definitions\n",
    "outcome_vars = [\n",
    "    'arr_delay', 'arr_delay_new', 'arr_del15', 'arr_delay_group',\n",
    "    'dep_delay', 'dep_delay_new', 'dep_del15', 'dep_delay_group',\n",
    "    'actual_elapsed_time', 'air_time', 'wheels_on', 'taxi_in', \n",
    "    'arr_time', 'taxi_out', 'wheels_off', 'dep_time', 'cancelled', 'diverted'\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'day_of_week',\n",
    "    'op_carrier',\n",
    "    # 'origin',\n",
    "    # 'origin_state_abr',\n",
    "    # 'dest',\n",
    "    # 'dest_state_abr',\n",
    "    'dep_time_blk',\n",
    "    'arr_time_blk',\n",
    "    'day_of_month',\n",
    "    'month',\n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    'hourlyprecipitation',\n",
    "    'hourlysealevelpressure',\n",
    "    'hourlyaltimetersetting',\n",
    "    'hourlywetbulbtemperature',\n",
    "    'hourlystationpressure',\n",
    "    'hourlywinddirection',\n",
    "    'hourlyrelativehumidity',\n",
    "    'hourlywindspeed',\n",
    "    'hourlydewpointtemperature',\n",
    "    'hourlydrybulbtemperature',\n",
    "    'hourlyvisibility',\n",
    "    'crs_elapsed_time',\n",
    "    'distance',\n",
    "    'elevation',\n",
    "]\n",
    "\n",
    "# Pipeline stages\n",
    "imputer = Imputer(\n",
    "    inputCols=numerical_features,\n",
    "    outputCols=[f\"{col}_IMPUTED\" for col in numerical_features],\n",
    "    strategy=\"mean\"\n",
    ")\n",
    "\n",
    "indexer = StringIndexer(\n",
    "    inputCols=categorical_features,\n",
    "    outputCols=[f\"{col}_INDEX\" for col in categorical_features],\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=[f\"{col}_INDEX\" for col in categorical_features],\n",
    "    outputCols=[f\"{col}_VEC\" for col in categorical_features],\n",
    "    dropLast=False  # Keep all categories for Random Forest\n",
    ")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[f\"{col}_VEC\" for col in categorical_features] + \n",
    "              [f\"{col}_IMPUTED\" for col in numerical_features],\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "# Random Forest Regressor\n",
    "# Note: No StandardScaler needed for tree-based models\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"DEP_DELAY\",\n",
    "    numTrees=100,\n",
    "    maxDepth=5,\n",
    ")\n",
    "\n",
    "# Pipeline with all categorical features one-hot encoded\n",
    "rf_pipe = Pipeline(stages=[imputer, indexer, encoder, assembler, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c56ad2b-08ff-42f9-8116-d443047602fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_rf = cv.FlightDelayCV(\n",
    "    estimator=rf_pipe,\n",
    "    dataloader=data_loader,\n",
    "    version=\"60M\"\n",
    ")\n",
    "cv_rf.evaluate()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Demo: Cross Validator",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
