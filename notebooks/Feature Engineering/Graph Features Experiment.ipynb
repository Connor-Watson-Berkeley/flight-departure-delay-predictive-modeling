{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75767145-d4e5-4591-8f0e-047f271452a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dependencies to load moduels from this repo\n",
    "import importlib.util\n",
    "import sys\n",
    "\n",
    "# Load cv module directly from file path\n",
    "cv_path = \"/Workspace/Shared/Team 4_2/flight-departure-delay-predictive-modeling/notebooks/Cross Validator/cv.py\"\n",
    "spec = importlib.util.spec_from_file_location(\"cv\", cv_path)\n",
    "cv = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(cv)\n",
    "\n",
    "# Dependencies for graph features\n",
    "from graphframes import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Other Dependencies\n",
    "import time\n",
    "\n",
    "# Path for persistent storage\n",
    "FOLDER_PATH = \"dbfs:/mnt/mids-w261/student-groups/Group_4_2/experiments\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f43e098-fc0a-4988-8575-cc8bf1dc51d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Use Cross Validator Module to Generate Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63ce7618-70f8-4f37-8f14-f01d62b1452b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_loader = cv.FlightDelayDataLoader()\n",
    "data_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53cae630-54f0-4e92-a24c-cec732525c6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "folds = data_loader.get_version(\"3M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87da433e-8316-4803-b095-308c21ae8160",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get training data from first fold to build graph\n",
    "train_df, val_df = folds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79d09745-a582-46a1-8e3d-9887c4e074a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Generate Graph from One Training Fold\n",
    "\n",
    "### Graph Construction\n",
    "Origin Airport --Flight--> Destination Airport\n",
    "\n",
    "* **Nodes**: Airport Codes\n",
    "* **Edges**: Flights\n",
    "  * **Direction**: Origin to Destination\n",
    "  * **Weight**: Number of Flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d0ca135-61ea-40d8-8ccb-fe0095a38835",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Build graph: nodes are airports, edges are flights (origin -> dest)\n",
    "# Edge weights = number of flights between airports\n",
    "\n",
    "# Create edges: (origin, dest) with count as weight\n",
    "edges = (\n",
    "    train_df\n",
    "    .select(\"origin\", \"dest\")\n",
    "    .filter(col(\"origin\").isNotNull() & col(\"dest\").isNotNull())\n",
    "    .groupBy(\"origin\", \"dest\")\n",
    "    .count()\n",
    "    .withColumnRenamed(\"origin\", \"src\")\n",
    "    .withColumnRenamed(\"dest\", \"dst\")\n",
    "    .withColumnRenamed(\"count\", \"weight\")\n",
    ")\n",
    "\n",
    "display(edges.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5a2c7a7-9105-4e55-924f-b6361946094b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Checkpoint to run notebook more quickly in the future\n",
    "edges_path = f\"{FOLDER_PATH}/graph_edges.parquet\"\n",
    "edges.write.mode(\"overwrite\").parquet(edges_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aeeee33d-9ead-45b4-866f-c2933b1f1596",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# If re-running this notebook, start here as edges are checkpointed\n",
    "edges = spark.read.parquet(edges_path)\n",
    "edges.count()  # Materialize\n",
    "display(edges.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4982c5f1-1f15-43ff-9761-177abf04d2fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create vertices: all unique airports (both origin and destination)\n",
    "src_airports = edges.select(col(\"src\").alias(\"id\")).distinct()\n",
    "dst_airports = edges.select(col(\"dst\").alias(\"id\")).distinct()\n",
    "vertices = src_airports.union(dst_airports).distinct()\n",
    "\n",
    "print(f\"Number of airports (vertices): {vertices.count()}\")\n",
    "print(f\"Number of routes (edges): {edges.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ba35774-7047-44b9-a19a-fe15319d7c14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Generate Graphframes\n",
    "**NOTE**: GraphFrames PageRank does NOT use edge weights (HW5 Q5.f). It treats all edges equally (weight = 1), ignoring the \"weight\" column. For weighted PageRank, we'd need to use the RDD approach from HW5 Q4\n",
    "\n",
    "**Correction**: GraphFrames automatically detects a column named weight in the edges DataFrame. Pagerank uses weights by default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18fdf84f-a3de-4487-9b10-07b6b4886585",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create GraphFrame\n",
    "start = time.time()\n",
    "g = GraphFrame(vertices, edges)\n",
    "print(f\"Ran in {time.time() - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85aba1b6-5a40-42a8-8f4a-23e7e10a54d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify edges DataFrame has \"weight\" column (GraphFrames automatically detects this)\n",
    "print(\"Edges DataFrame schema:\")\n",
    "edges.printSchema()\n",
    "print(\"\\nSample edges with weights:\")\n",
    "display(edges.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b344ed7-3e66-4c37-8fab-361d0dbd7b0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### \"Weighted\" GraphFrame Workaround"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b411f98-deb9-47df-80b5-701a4434e54f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create weighted graph using duplication workaround\n",
    "# Duplicate each edge based on its weight (e.g., weight=3 means 3 copies of the edge)\n",
    "# This simulates weighted PageRank since GraphFrames treats all edges equally\n",
    "start = time.time()\n",
    "\n",
    "# Get edges with weight column\n",
    "edges_with_weights = g.edges\n",
    "\n",
    "# Create sequence array [0, 1, 2, ..., weight-1] for each edge, then explode to duplicate\n",
    "edges_weighted = (\n",
    "    edges_with_weights\n",
    "    .withColumn(\"seq\", F.sequence(F.lit(0), F.col(\"weight\").cast(\"int\") - 1))\n",
    "    .select(\"src\", \"dst\", F.explode(\"seq\").alias(\"_\"))\n",
    "    .select(\"src\", \"dst\")\n",
    ")\n",
    "\n",
    "# Create weighted GraphFrame (same vertices, duplicated edges)\n",
    "g_weighted = GraphFrame(vertices, edges_weighted)\n",
    "\n",
    "print(f\"Created weighted GraphFrame (duplication workaround) in {time.time() - start:.2f} seconds\")\n",
    "print(f\"Original edges: {edges.count()}\")\n",
    "print(f\"Weighted edges (after duplication): {edges_weighted.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64477ee0-53cd-4e9d-828e-75ad0dcb83fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Graph Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc6a15ca-3c08-44e7-84ea-4d93890a973b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set checkpoint directory (required for GraphFrames algorithms like connectedComponents)\n",
    "sc = spark.sparkContext\n",
    "sc.setCheckpointDir(\"dbfs:/tmp/graphframes_checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f16c731f-f6be-4a16-9287-a92721affb66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Connectivity Analysis (referencing HW5 concepts)\n",
    "# Check for connected components (islands) - weakly connected for directed graph\n",
    "connected_components = g.connectedComponents()\n",
    "\n",
    "# Count number of distinct components\n",
    "num_components = connected_components.select(\"component\").distinct().count()\n",
    "print(f\"Number of connected components (islands): {num_components}\")\n",
    "\n",
    "# Show component sizes\n",
    "component_sizes = (\n",
    "    connected_components\n",
    "    .groupBy(\"component\")\n",
    "    .count()\n",
    "    .orderBy(F.desc(\"count\"))\n",
    ")\n",
    "\n",
    "print(\"\\nTop 10 largest components:\")\n",
    "display(component_sizes.limit(10))\n",
    "\n",
    "# Check if graph is strongly connected (all nodes reachable from all nodes)\n",
    "# For directed graphs, we check strongly connected components\n",
    "strongly_connected = g.stronglyConnectedComponents(maxIter=10)\n",
    "num_strong_components = strongly_connected.select(\"component\").distinct().count()\n",
    "print(f\"\\nNumber of strongly connected components: {num_strong_components}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ebe7278-466b-4f2d-87ac-93f2a7d924c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check for dangling nodes (nodes with no outlinks) - key issue from HW5 Q2.b\n",
    "# Dangling nodes are nodes that receive links but don't link to anything else\n",
    "out_degree = g.outDegrees\n",
    "in_degree = g.inDegrees\n",
    "\n",
    "# Find nodes with out-degree = 0 (dangling nodes)\n",
    "dangling_nodes = (\n",
    "    vertices\n",
    "    .join(out_degree, \"id\", \"left_outer\")\n",
    "    .filter(col(\"outDegree\").isNull() | (col(\"outDegree\") == 0))\n",
    ")\n",
    "\n",
    "num_dangling = dangling_nodes.count()\n",
    "print(f\"Number of dangling nodes (no outlinks): {num_dangling}\")\n",
    "\n",
    "if num_dangling > 0:\n",
    "    print(\"\\nSample dangling nodes:\")\n",
    "    display(dangling_nodes.limit(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ad69702-2915-4d5a-9c89-1e7db5e8b431",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check for nodes with no inlinks (sources)\n",
    "source_nodes = (\n",
    "    vertices\n",
    "    .join(in_degree, \"id\", \"left_outer\")\n",
    "    .filter(col(\"inDegree\").isNull() | (col(\"inDegree\") == 0))\n",
    ")\n",
    "\n",
    "num_sources = source_nodes.count()\n",
    "print(f\"Number of source nodes (no inlinks): {num_sources}\")\n",
    "\n",
    "# Summary statistics\n",
    "total_nodes = vertices.count()\n",
    "print(f\"\\n=== Graph Connectivity Summary ===\")\n",
    "print(f\"Total nodes: {total_nodes}\")\n",
    "print(f\"Weakly connected components: {num_components}\")\n",
    "print(f\"Strongly connected components: {num_strong_components}\")\n",
    "print(f\"Dangling nodes (no outlinks): {num_dangling}\")\n",
    "print(f\"Source nodes (no inlinks): {num_sources}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d471c2c0-42d1-4ea4-9c84-ddbccfec8cc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Run PageRank on Connectivity Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5be45832-c7d6-49cd-8751-534da0784038",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Run PageRank **without** edge weights\n",
    "pagerank_results_unweighted = g.pageRank(resetProbability=0.15, maxIter=10)\n",
    "\n",
    "# Display top airports by PageRank\n",
    "top_airports_unweighted = pagerank_results_unweighted.vertices.orderBy(F.desc(\"pagerank\")).limit(20)\n",
    "display(top_airports_unweighted)\n",
    "\n",
    "print(f\"Ran in {time.time() - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bda0f811-d585-493c-badb-4cca891c91fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run Weighted PageRank using the \"weight\" column (number of flights)\n",
    "pagerank_results_weighted = g_weighted.pageRank(resetProbability=0.15, maxIter=10)\n",
    "\n",
    "# Display top airports by PageRank\n",
    "top_airports_weighted = pagerank_results_weighted.vertices.orderBy(F.desc(\"pagerank\")).limit(20)\n",
    "display(top_airports_weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56eb8432-7a5e-400e-b0f0-4af52818084e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Flight Sequence Graph (For Visualization/Analysis)\n",
    "\n",
    "### GraphFrames vs Window Functions\n",
    "\n",
    "**Key Insight**: For Flight Lineage feature engineering, we DON'T need GraphFrames!\n",
    "\n",
    "The features we need (previous flight info, cumulative delays, conditional expected values) are all:\n",
    "- **Window functions** (LAG, SUM, AVG over partitions)\n",
    "- **Joins** (with pre-computed conditional expected value tables)\n",
    "- **Conditional logic** (IF-THEN calculations)\n",
    "\n",
    "**Graph algorithms** (PageRank, shortest paths, etc.) are NOT needed.\n",
    "\n",
    "**See**: `FLIGHT_LINEAGE_DESIGN.md` for detailed analysis.\n",
    "\n",
    "### When to Use This Graph\n",
    "\n",
    "This graph is useful for:\n",
    "- **Visualization**: Understanding flight sequences\n",
    "- **Analysis**: Jump patterns, sequence statistics\n",
    "- **NOT for feature engineering**: Use window functions instead\n",
    "\n",
    "### Sequential Graph Structure (No Self-Loops)\n",
    "\n",
    "To avoid self-loops and create a strictly sequential graph:\n",
    "\n",
    "**Nodes**: `{tail_num}_{FL_DATE}_{seq_num}` or `{tail_num}_{FL_DATE}_{crs_dep_time}`\n",
    "- Ensures uniqueness\n",
    "- Preserves temporal ordering\n",
    "- No self-loops\n",
    "\n",
    "**Edges**: `(node_i, node_i+1)` where nodes are consecutive flights\n",
    "- `src`: `{tail_num}_{date}_{seq_num}`\n",
    "- `dst`: `{tail_num}_{date}_{seq_num+1}`\n",
    "- Edge attributes: `dest_airport`, `air_time`, `is_jump`, etc.\n",
    "\n",
    "**Example**: A→B→A→C [Break] D→E becomes:\n",
    "- Node1: `N12345_2023-01-15_1` (A→B)\n",
    "- Node2: `N12345_2023-01-15_2` (B→A)\n",
    "- Node3: `N12345_2023-01-15_3` (A→C)\n",
    "- [Break/jump]\n",
    "- Node4: `N12345_2023-01-15_4` (D→E)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcd6226b-1304-41ce-ae8a-0a44fca1b115",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Build Sequential Flight Sequence Graph (for visualization/analysis)\n",
    "# Nodes: {tail_num}_{FL_DATE}_{seq_num} to avoid self-loops\n",
    "# This creates a strictly sequential graph: A→B→A→C [Break] D→E\n",
    "\n",
    "print(\"Building Sequential Flight Sequence Graph...\")\n",
    "print(\"Nodes: {tail_num}_{FL_DATE}_{seq_num} (unique flight identifiers)\")\n",
    "print(\"Edges: Consecutive flights in sequence (no self-loops)\\n\")\n",
    "print(\"NOTE: For feature engineering, use window functions instead of this graph.\\n\")\n",
    "\n",
    "# Check if tail_num column exists\n",
    "tail_cols = [c for c in train_df.columns if 'tail' in c.lower()]\n",
    "print(f\"Tail number columns found: {tail_cols}\")\n",
    "\n",
    "if not tail_cols:\n",
    "    print(\"WARNING: No tail_num column found. Skipping flight sequence graph.\")\n",
    "else:\n",
    "    tail_col = tail_cols[0]  # Use first matching column\n",
    "    print(f\"Using column: {tail_col}\\n\")\n",
    "    \n",
    "    # Prepare data: need tail_num, FL_DATE, origin, dest, scheduled times, delays\n",
    "    required_cols = [tail_col, 'FL_DATE', 'origin', 'dest', 'crs_dep_time']\n",
    "    available_cols = [c for c in required_cols if c in train_df.columns]\n",
    "    \n",
    "    if len(available_cols) < len(required_cols):\n",
    "        missing = set(required_cols) - set(available_cols)\n",
    "        print(f\"WARNING: Missing columns: {missing}\")\n",
    "        print(\"Skipping flight sequence graph.\")\n",
    "    else:\n",
    "        # Select and filter flight data\n",
    "        flight_seq_data = (\n",
    "            train_df\n",
    "            .select(\n",
    "                tail_col, 'FL_DATE', 'origin', 'dest', \n",
    "                'crs_dep_time', 'air_time', 'DEP_DELAY', 'ARR_DELAY'\n",
    "            )\n",
    "            .filter(\n",
    "                col(tail_col).isNotNull() & \n",
    "                col('FL_DATE').isNotNull() &\n",
    "                col('origin').isNotNull() & \n",
    "                col('dest').isNotNull() &\n",
    "                col('crs_dep_time').isNotNull()\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        print(f\"Total flights with valid tail_num: {flight_seq_data.count():,}\")\n",
    "        \n",
    "        # Order flights by tail_num, date, and scheduled departure time\n",
    "        # This ensures we track sequences correctly (A→B→A→C, not A→B & A→C)\n",
    "        from pyspark.sql.window import Window\n",
    "        \n",
    "        window_spec = Window.partitionBy(tail_col, 'FL_DATE').orderBy('crs_dep_time')\n",
    "        \n",
    "        # Add sequence number and previous flight information\n",
    "        flight_seq_ordered = (\n",
    "            flight_seq_data\n",
    "            .withColumn('seq_num', F.row_number().over(window_spec))\n",
    "            .withColumn('prev_dest', F.lag('dest', 1).over(window_spec))\n",
    "            .withColumn('prev_air_time', F.lag('air_time', 1).over(window_spec))\n",
    "            .withColumn('prev_arr_delay', F.lag('ARR_DELAY', 1).over(window_spec))\n",
    "        )\n",
    "        \n",
    "        # Detect jumps: when prev_dest != current_origin (or prev_dest is null for first flight)\n",
    "        flight_seq_with_jumps = flight_seq_ordered.withColumn(\n",
    "            'is_jump',\n",
    "            when(col('seq_num') == 1, F.lit(False))  # First flight is not a jump\n",
    "            .when(col('prev_dest').isNull(), F.lit(True))  # Missing previous flight = jump\n",
    "            .otherwise(col('prev_dest') != col('origin'))  # prev_dest != origin = jump\n",
    "        )\n",
    "        \n",
    "        # Create unique node IDs: {tail_num}_{FL_DATE}_{seq_num}\n",
    "        # This ensures no self-loops and preserves temporal ordering\n",
    "        flight_seq_with_nodes = flight_seq_with_jumps.withColumn(\n",
    "            'node_id',\n",
    "            F.concat(\n",
    "                col(tail_col), F.lit('_'),\n",
    "                col('FL_DATE'), F.lit('_'),\n",
    "                col('seq_num')\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Create edges: (node_i, node_i+1) for consecutive flights\n",
    "        # Only create edges between consecutive flights (seq_num difference = 1)\n",
    "        flight_seq_edges = (\n",
    "            flight_seq_with_nodes\n",
    "            .withColumn('next_node_id', F.lead('node_id', 1).over(window_spec))\n",
    "            .filter(col('next_node_id').isNotNull())  # Skip last flight in sequence\n",
    "            .select(\n",
    "                col('node_id').alias('src'),  # Current flight node\n",
    "                col('next_node_id').alias('dst'),  # Next flight node\n",
    "                # Edge attributes:\n",
    "                col('origin').alias('origin_airport'),\n",
    "                col('dest').alias('dest_airport'),\n",
    "                col('FL_DATE').alias('flight_date'),\n",
    "                col('crs_dep_time').alias('scheduled_dep_time'),\n",
    "                col('air_time').alias('air_time'),\n",
    "                col('is_jump').alias('is_jump'),\n",
    "                col('DEP_DELAY').alias('dep_delay'),\n",
    "                col('ARR_DELAY').alias('arr_delay'),\n",
    "                col('seq_num').alias('seq_num'),\n",
    "                col('prev_dest').alias('prev_dest_airport'),\n",
    "                col('prev_air_time').alias('prev_air_time'),\n",
    "                col('prev_arr_delay').alias('prev_arr_delay')\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Create vertices: all unique node IDs\n",
    "        flight_seq_vertices = (\n",
    "            flight_seq_with_nodes\n",
    "            .select(\n",
    "                col('node_id').alias('id'),\n",
    "                col(tail_col).alias('tail_num'),\n",
    "                col('FL_DATE').alias('flight_date'),\n",
    "                col('seq_num').alias('seq_num'),\n",
    "                col('origin').alias('origin_airport'),\n",
    "                col('dest').alias('dest_airport'),\n",
    "                col('crs_dep_time').alias('scheduled_dep_time')\n",
    "            )\n",
    "            .distinct()\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nFlight sequence edges created: {flight_seq_edges.count():,}\")\n",
    "        print(\"\\nSample flight sequence edges (showing sequential structure, no self-loops):\")\n",
    "        display(flight_seq_edges.select('src', 'dst', 'origin_airport', 'dest_airport', \n",
    "                                        'seq_num', 'is_jump', 'prev_dest_airport').limit(20))\n",
    "        \n",
    "        print(f\"\\nFlight sequence nodes (unique flight identifiers): {flight_seq_vertices.count():,}\")\n",
    "        print(\"\\nSample nodes:\")\n",
    "        display(flight_seq_vertices.limit(10))\n",
    "        \n",
    "        # Analyze jump patterns\n",
    "        jump_stats = (\n",
    "            flight_seq_edges\n",
    "            .groupBy('is_jump')\n",
    "            .agg(\n",
    "                F.count('*').alias('count'),\n",
    "                F.avg('dep_delay').alias('avg_dep_delay'),\n",
    "                F.avg('arr_delay').alias('avg_arr_delay')\n",
    "            )\n",
    "        )\n",
    "        print(\"\\nJump Statistics:\")\n",
    "        display(jump_stats)\n",
    "        \n",
    "        # Save for later use\n",
    "        flight_seq_edges_path = f\"{FOLDER_PATH}/flight_sequence_edges.parquet\"\n",
    "        flight_seq_vertices_path = f\"{FOLDER_PATH}/flight_sequence_vertices.parquet\"\n",
    "        \n",
    "        flight_seq_edges.write.mode(\"overwrite\").parquet(flight_seq_edges_path)\n",
    "        flight_seq_vertices.write.mode(\"overwrite\").parquet(flight_seq_vertices_path)\n",
    "        \n",
    "        print(f\"\\nSaved flight sequence graph components:\")\n",
    "        print(f\"  Edges: {flight_seq_edges_path}\")\n",
    "        print(f\"  Vertices: {flight_seq_vertices_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0a55992-5af4-442b-995e-c25270461d65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Graph Features Experiment",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
