{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15979228-d1cf-4741-9990-b01d6ac0b88e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dependencies to load moduels from this repo\n",
    "import importlib.util\n",
    "import sys\n",
    "\n",
    "# Load cv module directly from file path\n",
    "cv_path = \"/Workspace/Shared/Team 4_2/flight-departure-delay-predictive-modeling/notebooks/Cross Validator/cv.py\"\n",
    "spec = importlib.util.spec_from_file_location(\"cv\", cv_path)\n",
    "cv = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(cv)\n",
    "\n",
    "# Dependencies for time series features\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, to_timestamp, date_format, when\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "\n",
    "# Dependencies for EDA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import adfuller, kpss, coint\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "# Other Dependencies\n",
    "import time\n",
    "\n",
    "# Path for persistent storage\n",
    "FOLDER_PATH = \"dbfs:/mnt/mids-w261/student-groups/Group_4_2/experiments\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "174bb191-62e7-4352-b30e-97ad120b58b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad72abbd-ddb5-498b-94cf-b2760cafee51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load from data_loader and save snapshot (run once)\n",
    "ts_data_path = f\"{FOLDER_PATH}/timeseries_data_snapshot.parquet\"\n",
    "\n",
    "print(\"Loading from data_loader and saving snapshot...\")\n",
    "start = time.time()\n",
    "data_loader = cv.FlightDelayDataLoader()\n",
    "data_loader.load()\n",
    "folds = data_loader.get_version(\"60M\")\n",
    "\n",
    "# Use final fold and union training and validation to get 2 years of data for time series analysis\n",
    "# This allows us to learn yearly seasonality without data leakage\n",
    "train_df, val_df = folds[-1]\n",
    "\n",
    "# Union training and validation folds to get 2-year time period\n",
    "ts_data = train_df.union(val_df)\n",
    "\n",
    "# Check partition count and repartition if needed\n",
    "num_partitions = ts_data.rdd.getNumPartitions()\n",
    "if num_partitions > 500:\n",
    "    ts_data = ts_data.coalesce(200)\n",
    "elif num_partitions < 10:\n",
    "    ts_data = ts_data.repartition(50)\n",
    "\n",
    "# Save snapshot\n",
    "ts_data.write.mode(\"overwrite\").parquet(ts_data_path)\n",
    "print(f\"Saved snapshot in {time.time() - start:.2f} seconds\")\n",
    "\n",
    "print(f\"\\nTime series data: {ts_data.count():,} flights\")\n",
    "print(f\"Date range: {ts_data.agg(F.min('FL_DATE'), F.max('FL_DATE')).collect()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f127858-178a-4f9a-8294-77313575d12e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load from saved snapshot (run this on subsequent runs, skip above cell\n",
    "ts_data_path = f\"{FOLDER_PATH}/timeseries_data_snapshot.parquet\"\n",
    "\n",
    "print(f\"Loading timeseries data from {ts_data_path}...\")\n",
    "start = time.time()\n",
    "ts_data = spark.read.parquet(ts_data_path)\n",
    "ts_data.count()  # Materialize\n",
    "print(f\"Loaded in {time.time() - start:.2f} seconds\")\n",
    "\n",
    "print(f\"\\nTime series data: {ts_data.count():,} flights\")\n",
    "print(f\"Date range: {ts_data.agg(F.min('FL_DATE'), F.max('FL_DATE')).collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94a649d5-4f96-487a-a20e-cbd70ea143cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Generate Time-Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b7faf18-4a34-4ba0-8be7-c05640fee61b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prepare date column for aggregation\n",
    "# Convert FL_DATE to date type and filter valid data\n",
    "ts_data_prep = ts_data.withColumn(\n",
    "    \"date\", \n",
    "    to_timestamp(col(\"FL_DATE\"), \"yyyy-MM-dd\").cast(\"date\")\n",
    ").filter(\n",
    "    col(\"date\").isNotNull() & \n",
    "    col(\"DEP_DELAY\").isNotNull()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc847c23-58b9-40e1-b291-3f83a2d562a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Global Time-Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6175ffd0-6e57-4f1f-b34e-0be9438554e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# lobal time series: Average departure delay by date\n",
    "global_dep_delays_spark = (\n",
    "    ts_data_prep\n",
    "    .groupBy(\"date\")\n",
    "    .agg(\n",
    "        F.avg(\"DEP_DELAY\").alias(\"avg_dep_delay\"),\n",
    "        F.count(\"*\").alias(\"flight_count\")\n",
    "    )\n",
    "    .orderBy(\"date\")\n",
    ")\n",
    "\n",
    "# Convert to pandas for Prophet\n",
    "global_dep_delays = global_dep_delays_spark.toPandas()\n",
    "global_dep_delays['ds'] = pd.to_datetime(global_dep_delays['date'])\n",
    "global_dep_delays = global_dep_delays.rename(columns={'avg_dep_delay': 'y'})\n",
    "\n",
    "print(\"Global time series (first 10 days):\")\n",
    "print(global_dep_delays[['ds', 'y', 'flight_count']].head(10))\n",
    "print(f\"\\nTotal days: {len(global_dep_delays)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fc34acb-db47-43a3-9b61-03c442345bf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Per Airport Time-Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cd9ccf4-c140-42f1-82aa-549f56b5871f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Per-airport time series: Departure delays, arrival delays, and flight counts\n",
    "# Aggregate by airport and date\n",
    "per_airport_ts_spark = (\n",
    "    ts_data_prep\n",
    "    .groupBy(\"origin\", \"date\")\n",
    "    .agg(\n",
    "        F.avg(\"DEP_DELAY\").alias(\"avg_dep_delay\"),\n",
    "        F.avg(\"ARR_DELAY\").alias(\"avg_arr_delay\"),\n",
    "        F.count(\"*\").alias(\"flight_count\")\n",
    "    )\n",
    "    .orderBy(\"origin\", \"date\")\n",
    ")\n",
    "\n",
    "# Convert to pandas\n",
    "per_airport_ts = per_airport_ts_spark.toPandas()\n",
    "per_airport_ts['ds'] = pd.to_datetime(per_airport_ts['date'])\n",
    "\n",
    "print(f\"Per-airport time series: {len(per_airport_ts):,} rows\")\n",
    "print(f\"Number of airports: {per_airport_ts['origin'].nunique()}\")\n",
    "print(f\"Average days per airport: {len(per_airport_ts) / per_airport_ts['origin'].nunique():.1f}\")\n",
    "print(\"\\nSample (first airport):\")\n",
    "first_airport = per_airport_ts['origin'].iloc[0]\n",
    "print(per_airport_ts[per_airport_ts['origin'] == first_airport][['origin', 'ds', 'avg_dep_delay', 'avg_arr_delay', 'flight_count']].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbce5264-0345-40dc-9d00-b00114b82650",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Time-Series EDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3586a85c-68ec-426f-a506-7e3b142fdf60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "### 1. Raw Time Series Plots\n",
    "\n",
    "\n",
    "# Global departure delays\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Full time series\n",
    "axes[0].plot(global_dep_delays['ds'], global_dep_delays['y'], linewidth=0.5, alpha=0.7)\n",
    "axes[0].set_title('Global Average Departure Delay Over Time (Full Series)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Date')\n",
    "axes[0].set_ylabel('Average Departure Delay (minutes)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Zoomed view (last 6 months)\n",
    "last_6mo = global_dep_delays.tail(180)\n",
    "axes[1].plot(last_6mo['ds'], last_6mo['y'], linewidth=1, marker='o', markersize=2)\n",
    "axes[1].set_title('Global Average Departure Delay (Last 6 Months)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].set_ylabel('Average Departure Delay (minutes)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n=== Global Departure Delay Summary Statistics ===\")\n",
    "print(global_dep_delays['y'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3242be0c-2e7f-454b-b2a5-e97b33ea0512",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### 2. Distribution and Box Plots\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Histogram\n",
    "axes[0, 0].hist(global_dep_delays['y'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_title('Distribution of Daily Average Departure Delays', fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Average Departure Delay (minutes)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].axvline(global_dep_delays['y'].mean(), color='r', linestyle='--', label=f'Mean: {global_dep_delays[\"y\"].mean():.2f}')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Box plot by year\n",
    "global_dep_delays['year'] = global_dep_delays['ds'].dt.year\n",
    "sns.boxplot(data=global_dep_delays, x='year', y='y', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Departure Delays by Year', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Year')\n",
    "axes[0, 1].set_ylabel('Average Departure Delay (minutes)')\n",
    "\n",
    "# Box plot by month\n",
    "global_dep_delays['month'] = global_dep_delays['ds'].dt.month\n",
    "sns.boxplot(data=global_dep_delays, x='month', y='y', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Departure Delays by Month', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Month')\n",
    "axes[1, 0].set_ylabel('Average Departure Delay (minutes)')\n",
    "\n",
    "# Box plot by day of week\n",
    "global_dep_delays['day_of_week'] = global_dep_delays['ds'].dt.dayofweek\n",
    "sns.boxplot(data=global_dep_delays, x='day_of_week', y='y', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Departure Delays by Day of Week', fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Day of Week (0=Monday)')\n",
    "axes[1, 1].set_ylabel('Average Departure Delay (minutes)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b75bb967-0e41-4ddc-acbe-6b4f8479f99b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### 3. Stationarity Tests\n",
    "\n",
    "# Set up time series for testing (remove NaN values)\n",
    "\n",
    "ts_values = global_dep_delays['y'].dropna().values\n",
    "\n",
    "print(\"=== Stationarity Tests ===\\n\")\n",
    "\n",
    "# Augmented Dickey-Fuller Test (ADF)\n",
    "print(\"1. Augmented Dickey-Fuller Test (ADF):\")\n",
    "print(\"   H0: Series has a unit root (non-stationary)\")\n",
    "print(\"   H1: Series is stationary\\n\")\n",
    "adf_result = adfuller(ts_values)\n",
    "print(f\"   ADF Statistic: {adf_result[0]:.4f}\")\n",
    "print(f\"   p-value: {adf_result[1]:.4f}\")\n",
    "print(f\"   Critical Values:\")\n",
    "for key, value in adf_result[4].items():\n",
    "    print(f\"      {key}: {value:.4f}\")\n",
    "if adf_result[1] <= 0.05:\n",
    "    print(\"   ✓ Series is STATIONARY (reject H0, p < 0.05)\")\n",
    "else:\n",
    "    print(\"   ✗ Series is NON-STATIONARY (fail to reject H0, p >= 0.05)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# KPSS Test\n",
    "print(\"2. KPSS Test:\")\n",
    "print(\"   H0: Series is stationary\")\n",
    "print(\"   H1: Series has a unit root (non-stationary)\\n\")\n",
    "kpss_result = kpss(ts_values, regression='ct')  # 'ct' = constant and trend\n",
    "print(f\"   KPSS Statistic: {kpss_result[0]:.4f}\")\n",
    "print(f\"   p-value: {kpss_result[1]:.4f}\")\n",
    "print(f\"   Critical Values:\")\n",
    "for key, value in kpss_result[3].items():\n",
    "    print(f\"      {key}: {value:.4f}\")\n",
    "if kpss_result[1] >= 0.05:\n",
    "    print(\"   ✓ Series is STATIONARY (fail to reject H0, p >= 0.05)\")\n",
    "else:\n",
    "    print(\"   ✗ Series is NON-STATIONARY (reject H0, p < 0.05)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f192c56-4a5d-4411-a8ad-8021348ab372",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### 4. Autocorrelation Analysis\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# ACF (Autocorrelation Function)\n",
    "plot_acf(global_dep_delays['y'].dropna(), lags=50, ax=axes[0], alpha=0.05)\n",
    "axes[0].set_title('Autocorrelation Function (ACF)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Lag')\n",
    "axes[0].set_ylabel('Autocorrelation')\n",
    "\n",
    "# PACF (Partial Autocorrelation Function)\n",
    "plot_pacf(global_dep_delays['y'].dropna(), lags=50, ax=axes[1], alpha=0.05)\n",
    "axes[1].set_title('Partial Autocorrelation Function (PACF)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Lag')\n",
    "axes[1].set_ylabel('Partial Autocorrelation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Ljung-Box Test for autocorrelation\n",
    "print(\"\\n=== Ljung-Box Test for Autocorrelation ===\")\n",
    "print(\"H0: No autocorrelation\")\n",
    "print(\"H1: Autocorrelation exists\\n\")\n",
    "lb_result = acorr_ljungbox(global_dep_delays['y'].dropna(), lags=10, return_df=True)\n",
    "print(lb_result)\n",
    "if (lb_result['lb_pvalue'] < 0.05).any():\n",
    "    print(\"\\n✗ Significant autocorrelation detected (p < 0.05)\")\n",
    "else:\n",
    "    print(\"\\n✓ No significant autocorrelation (p >= 0.05)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0c0d27e-1daf-4ba3-a71c-531a7b3fac75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### 5. STL Decomposition (Seasonal and Trend decomposition using Loess)\n",
    "\n",
    "# Set date as index for STL\n",
    "ts_indexed = global_dep_delays.set_index('ds')['y'].dropna()\n",
    "\n",
    "# STL Decomposition\n",
    "# period=365 for yearly seasonality, but we can also try weekly (period=7)\n",
    "stl = STL(ts_indexed, seasonal=365, trend=None, robust=True)\n",
    "decomposition = stl.fit()\n",
    "\n",
    "fig, axes = plt.subplots(4, 1, figsize=(14, 12))\n",
    "\n",
    "# Original\n",
    "axes[0].plot(ts_indexed.index, ts_indexed.values, linewidth=0.5, alpha=0.7)\n",
    "axes[0].set_title('Original Time Series', fontweight='bold')\n",
    "axes[0].set_ylabel('Departure Delay')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Trend\n",
    "axes[1].plot(decomposition.trend.index, decomposition.trend.values, linewidth=1, color='blue')\n",
    "axes[1].set_title('Trend Component', fontweight='bold')\n",
    "axes[1].set_ylabel('Trend')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Seasonal\n",
    "axes[2].plot(decomposition.seasonal.index, decomposition.seasonal.values, linewidth=0.5, alpha=0.7, color='green')\n",
    "axes[2].set_title('Seasonal Component', fontweight='bold')\n",
    "axes[2].set_ylabel('Seasonal')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "# Residual\n",
    "axes[3].plot(decomposition.resid.index, decomposition.resid.values, linewidth=0.5, alpha=0.7, color='red')\n",
    "axes[3].set_title('Residual Component', fontweight='bold')\n",
    "axes[3].set_ylabel('Residual')\n",
    "axes[3].set_xlabel('Date')\n",
    "axes[3].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary of decomposition\n",
    "print(\"\\n=== STL Decomposition Summary ===\")\n",
    "print(f\"Trend variance: {decomposition.trend.var():.4f}\")\n",
    "print(f\"Seasonal variance: {decomposition.seasonal.var():.4f}\")\n",
    "print(f\"Residual variance: {decomposition.resid.var():.4f}\")\n",
    "print(f\"\\nResidual statistics:\")\n",
    "print(decomposition.resid.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75fedf4d-4186-45f3-9846-ac2a7883cfe0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### 6. Additional Statistical Analysis\n",
    "\n",
    "# Monthly averages\n",
    "monthly_avg = global_dep_delays.groupby('month')['y'].mean()\n",
    "print(\"=== Monthly Average Departure Delays ===\")\n",
    "for month, avg in monthly_avg.items():\n",
    "    month_name = pd.Timestamp(2020, month, 1).strftime('%B')\n",
    "    print(f\"{month_name:12s}: {avg:6.2f} minutes\")\n",
    "\n",
    "# Day of week averages\n",
    "dow_avg = global_dep_delays.groupby('day_of_week')['y'].mean()\n",
    "dow_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "print(\"\\n=== Day of Week Average Departure Delays ===\")\n",
    "for dow, avg in dow_avg.items():\n",
    "    print(f\"{dow_names[dow]:12s}: {avg:6.2f} minutes\")\n",
    "\n",
    "# Year-over-year comparison\n",
    "print(\"\\n=== Year-over-Year Comparison ===\")\n",
    "yearly_avg = global_dep_delays.groupby('year')['y'].agg(['mean', 'std', 'min', 'max'])\n",
    "print(yearly_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31202c97-2810-47d4-89a6-4ef44099c68d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**TODO**: Additional Time Series Analysis\n",
    "\n",
    "- [ ] **Delays by Airplane Model**: Time series of average departure delays by aircraft model (e.g., Boeing 737, Airbus A320)\n",
    "- [ ] **Delays by Carrier**: Time series of average departure delays by airline carrier\n",
    "- [ ] Apply same EDA (stationarity tests, STL decomposition, etc.) to these additional time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b0c4948-9be5-4f2a-8305-2e4f49370de6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### 7. Cointegration Test: Flight Count vs Average Delay\n",
    "\n",
    "# Test if number of flights and average delay are cointegrated\n",
    "# Cointegration means they have a long-term equilibrium relationship\n",
    "# even if individually non-stationary\n",
    "\n",
    "# Align the series (same dates)\n",
    "flight_count_series = global_dep_delays['flight_count'].dropna().values\n",
    "delay_series = global_dep_delays['y'].dropna().values\n",
    "\n",
    "# Ensure same length\n",
    "min_len = min(len(flight_count_series), len(delay_series))\n",
    "flight_count_series = flight_count_series[:min_len]\n",
    "delay_series = delay_series[:min_len]\n",
    "\n",
    "print(\"=== Cointegration Test: Flight Count vs Average Delay ===\\n\")\n",
    "print(\"H0: No cointegration (series are not in long-term equilibrium)\")\n",
    "print(\"H1: Cointegration exists (series have long-term relationship)\\n\")\n",
    "\n",
    "# Engle-Granger cointegration test\n",
    "coint_result = coint(delay_series, flight_count_series)\n",
    "\n",
    "print(f\"Cointegration Test Statistic: {coint_result[0]:.4f}\")\n",
    "print(f\"p-value: {coint_result[1]:.4f}\")\n",
    "print(f\"Critical Values:\")\n",
    "for key, value in coint_result[2].items():\n",
    "    print(f\"   {key}: {value:.4f}\")\n",
    "\n",
    "if coint_result[1] <= 0.05:\n",
    "    print(\"\\n✓ COINTEGRATION DETECTED (p < 0.05)\")\n",
    "    print(\"  → Flight count and delay have a long-term equilibrium relationship\")\n",
    "    print(\"  → They move together over time despite short-term deviations\")\n",
    "else:\n",
    "    print(\"\\n✗ No cointegration (p >= 0.05)\")\n",
    "    print(\"  → Flight count and delay do not have a stable long-term relationship\")\n",
    "\n",
    "# Visualize the relationship\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Plot both series\n",
    "ax1 = axes[0]\n",
    "ax1_twin = ax1.twinx()\n",
    "ax1.plot(global_dep_delays['ds'].iloc[:min_len], delay_series, 'b-', label='Avg Delay', linewidth=1, alpha=0.7)\n",
    "ax1_twin.plot(global_dep_delays['ds'].iloc[:min_len], flight_count_series, 'r-', label='Flight Count', linewidth=1, alpha=0.7)\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Average Delay (minutes)', color='b')\n",
    "ax1_twin.set_ylabel('Flight Count', color='r')\n",
    "ax1.set_title('Flight Count vs Average Delay Over Time', fontweight='bold')\n",
    "ax1.tick_params(axis='y', labelcolor='b')\n",
    "ax1_twin.tick_params(axis='y', labelcolor='r')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter plot\n",
    "axes[1].scatter(flight_count_series, delay_series, alpha=0.3, s=10)\n",
    "axes[1].set_xlabel('Flight Count')\n",
    "axes[1].set_ylabel('Average Departure Delay (minutes)')\n",
    "axes[1].set_title('Flight Count vs Average Delay (Scatter)', fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add correlation\n",
    "correlation = np.corrcoef(flight_count_series, delay_series)[0, 1]\n",
    "axes[1].text(0.05, 0.95, f'Correlation: {correlation:.3f}', \n",
    "             transform=axes[1].transAxes, fontsize=12,\n",
    "             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f817f82e-9138-4c77-80c2-b3e62b7b8689",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Prophet Feature Generation\n",
    "\n",
    "Use Prophet to extract time-series features (trend, seasonality, forecasts) that can be used as features in ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88c567da-209e-43d1-8f21-72256de22a1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fit Prophet model on global time series\n",
    "print(\"Fitting Prophet model on global departure delays...\")\n",
    "\n",
    "# Prepare data for Prophet (requires 'ds' and 'y' columns)\n",
    "prophet_data = global_dep_delays[['ds', 'y']].copy()\n",
    "prophet_data = prophet_data.dropna()\n",
    "\n",
    "# Initialize and fit Prophet model\n",
    "# Enable yearly and weekly seasonality\n",
    "prophet_model_global = Prophet(\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    daily_seasonality=False,  # Daily doesn't make sense for daily aggregated data\n",
    "    seasonality_mode='multiplicative',  # or 'additive'\n",
    "    interval_width=0.95,  # 95% confidence intervals\n",
    "    changepoint_prior_scale=0.05  # Controls flexibility of trend changes\n",
    ")\n",
    "\n",
    "prophet_model_global.fit(prophet_data)\n",
    "\n",
    "# Generate forecast for all dates in the training data (and potentially future dates)\n",
    "# This gives us trend, seasonality components, and forecast values\n",
    "forecast_global = prophet_model_global.predict(prophet_data[['ds']])\n",
    "\n",
    "print(f\"Prophet forecast generated for {len(forecast_global)} dates\")\n",
    "print(\"\\nForecast columns:\")\n",
    "print(forecast_global.columns.tolist())\n",
    "print(\"\\nSample forecast:\")\n",
    "print(forecast_global[['ds', 'yhat', 'trend', 'yearly', 'weekly']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "785e8ccb-2770-4857-bcac-c8db64bc3c86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract Prophet features for joining back to flight data\n",
    "# These features can be used in ML models\n",
    "\n",
    "prophet_features_global = forecast_global[[\n",
    "    'ds',\n",
    "    'trend',              # Long-term trend component\n",
    "    'yearly',             # Yearly seasonality component\n",
    "    'weekly',             # Weekly seasonality component\n",
    "    'yhat',               # Forecasted value (trend + seasonality)\n",
    "    'yhat_lower',         # Lower bound of forecast interval\n",
    "    'yhat_upper',         # Upper bound of forecast interval\n",
    "]].copy()\n",
    "\n",
    "# Rename for clarity when joining\n",
    "prophet_features_global = prophet_features_global.rename(columns={\n",
    "    'trend': 'prophet_trend_global',\n",
    "    'yearly': 'prophet_yearly_seasonality_global',\n",
    "    'weekly': 'prophet_weekly_seasonality_global',\n",
    "    'yhat': 'prophet_forecast_global',\n",
    "    'yhat_lower': 'prophet_forecast_lower_global',\n",
    "    'yhat_upper': 'prophet_forecast_upper_global'\n",
    "})\n",
    "\n",
    "# Convert date to string format matching FL_DATE\n",
    "prophet_features_global['date_str'] = prophet_features_global['ds'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "print(\"Global Prophet features (sample):\")\n",
    "print(prophet_features_global.head(10))\n",
    "print(f\"\\nTotal feature rows: {len(prophet_features_global)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75f5efdd-6894-455a-b4c0-9611d7118b98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Per-Airport Prophet Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7aaca66c-06ce-4e7b-acb4-ed778376e883",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fit Prophet models for each airport\n",
    "# This will take longer but provides airport-specific trend and seasonality features\n",
    "\n",
    "print(\"Fitting Prophet models for each airport...\")\n",
    "print(f\"Total airports: {per_airport_ts['origin'].nunique()}\")\n",
    "\n",
    "# Check data availability per airport\n",
    "airport_day_counts = per_airport_ts.groupby('origin')['ds'].count().sort_values(ascending=False)\n",
    "print(f\"\\nData availability statistics:\")\n",
    "print(f\"  Min days: {airport_day_counts.min()}\")\n",
    "print(f\"  Max days: {airport_day_counts.max()}\")\n",
    "print(f\"  Mean days: {airport_day_counts.mean():.1f}\")\n",
    "print(f\"  Median days: {airport_day_counts.median():.1f}\")\n",
    "\n",
    "# Use a lower threshold - need at least 14 days for weekly seasonality\n",
    "# With only 3 quarters (~270 days), we can't do yearly seasonality, but we can get trend and weekly patterns\n",
    "min_days_required = 14  # At least 2 weeks for weekly seasonality\n",
    "airports_with_sufficient_data = airport_day_counts[airport_day_counts >= min_days_required].index.tolist()\n",
    "\n",
    "print(f\"\\nAirports with >= {min_days_required} days of data: {len(airports_with_sufficient_data)}\")\n",
    "\n",
    "# Fit Prophet for a subset of airports first (for testing)\n",
    "# In production, fit for all airports\n",
    "top_airports = airports_with_sufficient_data[:20]  # Start with top 20 airports\n",
    "print(f\"\\nFitting Prophet models for {len(top_airports)} airports (sample)...\")\n",
    "\n",
    "prophet_features_per_airport = []\n",
    "\n",
    "for airport in top_airports:\n",
    "    airport_data = per_airport_ts[per_airport_ts['origin'] == airport].sort_values('ds')\n",
    "    airport_prophet_data = airport_data[['ds', 'avg_dep_delay']].copy()\n",
    "    airport_prophet_data = airport_prophet_data.rename(columns={'avg_dep_delay': 'y'})\n",
    "    airport_prophet_data = airport_prophet_data.dropna()\n",
    "    \n",
    "    if len(airport_prophet_data) < min_days_required:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Determine seasonality based on data availability\n",
    "        has_enough_for_yearly = len(airport_prophet_data) >= 365\n",
    "        has_enough_for_weekly = len(airport_prophet_data) >= 14\n",
    "        \n",
    "        # Fit Prophet model with appropriate seasonality\n",
    "        prophet_model = Prophet(\n",
    "            yearly_seasonality=has_enough_for_yearly,  # Only if >=365 days\n",
    "            weekly_seasonality=has_enough_for_weekly,  # Only if >=14 days\n",
    "            daily_seasonality=False,\n",
    "            seasonality_mode='multiplicative',\n",
    "            interval_width=0.95,\n",
    "            changepoint_prior_scale=0.05\n",
    "        )\n",
    "        prophet_model.fit(airport_prophet_data)\n",
    "        \n",
    "        # Generate forecast\n",
    "        forecast = prophet_model.predict(airport_prophet_data[['ds']])\n",
    "        \n",
    "        # Extract features (yearly may not exist if not enough data)\n",
    "        feature_cols = ['ds', 'trend', 'weekly', 'yhat', 'yhat_lower', 'yhat_upper']\n",
    "        if 'yearly' in forecast.columns:\n",
    "            feature_cols.insert(2, 'yearly')  # Insert after 'trend'\n",
    "        \n",
    "        features = forecast[feature_cols].copy()\n",
    "        features['origin'] = airport\n",
    "        \n",
    "        # Rename columns\n",
    "        rename_dict = {\n",
    "            'trend': 'prophet_trend_origin',\n",
    "            'weekly': 'prophet_weekly_seasonality_origin',\n",
    "            'yhat': 'prophet_forecast_origin',\n",
    "            'yhat_lower': 'prophet_forecast_lower_origin',\n",
    "            'yhat_upper': 'prophet_forecast_upper_origin'\n",
    "        }\n",
    "        if 'yearly' in features.columns:\n",
    "            rename_dict['yearly'] = 'prophet_yearly_seasonality_origin'\n",
    "        \n",
    "        features = features.rename(columns=rename_dict)\n",
    "        features['date_str'] = features['ds'].dt.strftime('%Y-%m-%d')\n",
    "        \n",
    "        prophet_features_per_airport.append(features)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fitting Prophet for airport {airport}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Combine all airport features\n",
    "if prophet_features_per_airport:\n",
    "    prophet_features_airport_df = pd.concat(prophet_features_per_airport, ignore_index=True)\n",
    "    print(f\"\\n✓ Generated Prophet features for {prophet_features_airport_df['origin'].nunique()} airports\")\n",
    "    print(f\"Total feature rows: {len(prophet_features_airport_df)}\")\n",
    "    print(\"\\nSample features:\")\n",
    "    print(prophet_features_airport_df.head(10))\n",
    "else:\n",
    "    print(\"\\nNo Prophet features generated for airports\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Expected Values for Flight Lineage Features\n",
    "\n",
    "For Flight Lineage feature engineering, we need conditional expected values:\n",
    "- **Per Carrier**: Expected delays, turn times, air times by carrier\n",
    "- **Per Carrier-Airport**: Expected values conditional on carrier AND airport\n",
    "- **Per Carrier-Airport-Time**: Expected values conditional on carrier, airport, AND time of day/day of week\n",
    "\n",
    "These features will be used to compute:\n",
    "- `expected_turn_time_carrier_airport`: Average time between arrival and departure for this carrier at this airport\n",
    "- `expected_turn_time_carrier_airport_time`: Conditional on time of day\n",
    "- `expected_air_time_route`: Average air time for origin-destination pair\n",
    "- `expected_air_time_route_time_of_day`: Conditional on time of day\n",
    "- And other conditional expected values needed for deterministic prediction formulas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for conditional expected values\n",
    "# Need: carrier, origin, dest, date, time components, delays\n",
    "\n",
    "print(\"Preparing data for conditional expected values...\")\n",
    "\n",
    "# Check available columns\n",
    "carrier_cols = [c for c in ts_data.columns if 'carrier' in c.lower()]\n",
    "print(f\"Carrier columns: {carrier_cols}\")\n",
    "\n",
    "# Use op_carrier if available, otherwise use first carrier column\n",
    "carrier_col = 'op_carrier' if 'op_carrier' in ts_data.columns else (carrier_cols[0] if carrier_cols else None)\n",
    "\n",
    "if carrier_col is None:\n",
    "    print(\"WARNING: No carrier column found. Skipping carrier-based features.\")\n",
    "else:\n",
    "    print(f\"Using carrier column: {carrier_col}\\n\")\n",
    "    \n",
    "    # Prepare time components\n",
    "    ts_data_cond = ts_data_prep.withColumn(\n",
    "        'hour', F.hour(to_timestamp(col('crs_dep_time').cast('string'), 'HHmm'))\n",
    "    ).withColumn(\n",
    "        'day_of_week', F.dayofweek(col('date'))\n",
    "    ).withColumn(\n",
    "        'month', F.month(col('date'))\n",
    "    )\n",
    "    \n",
    "    # Add carrier and airport info\n",
    "    if carrier_col in ts_data_cond.columns:\n",
    "        ts_data_cond = ts_data_cond.withColumn('carrier', col(carrier_col))\n",
    "    \n",
    "    print(f\"Data prepared: {ts_data_cond.count():,} flights\")\n",
    "    print(f\"Carriers: {ts_data_cond.select('carrier').distinct().count()}\")\n",
    "    print(f\"Airports: {ts_data_cond.select('origin').distinct().count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per-Carrier Expected Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-carrier expected values\n",
    "# These are unconditional averages by carrier (aggregated over all airports, times, etc.)\n",
    "\n",
    "if 'ts_data_cond' in locals() and 'carrier' in ts_data_cond.columns:\n",
    "    print(\"Computing per-carrier expected values...\")\n",
    "    \n",
    "    # Expected departure delay by carrier\n",
    "    expected_dep_delay_carrier = (\n",
    "        ts_data_cond\n",
    "        .filter(col('DEP_DELAY').isNotNull())\n",
    "        .groupBy('carrier')\n",
    "        .agg(\n",
    "            F.avg('DEP_DELAY').alias('expected_dep_delay_carrier'),\n",
    "            F.stddev('DEP_DELAY').alias('std_dep_delay_carrier'),\n",
    "            F.count('*').alias('flight_count_carrier')\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Expected arrival delay by carrier\n",
    "    expected_arr_delay_carrier = (\n",
    "        ts_data_cond\n",
    "        .filter(col('ARR_DELAY').isNotNull())\n",
    "        .groupBy('carrier')\n",
    "        .agg(\n",
    "            F.avg('ARR_DELAY').alias('expected_arr_delay_carrier'),\n",
    "            F.stddev('ARR_DELAY').alias('std_arr_delay_carrier')\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Expected air time by carrier (if available)\n",
    "    expected_air_time_carrier = None\n",
    "    if 'air_time' in ts_data_cond.columns:\n",
    "        expected_air_time_carrier = (\n",
    "            ts_data_cond\n",
    "            .filter(col('air_time').isNotNull())\n",
    "            .groupBy('carrier')\n",
    "            .agg(\n",
    "                F.avg('air_time').alias('expected_air_time_carrier'),\n",
    "                F.stddev('air_time').alias('std_air_time_carrier')\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Combine all carrier features\n",
    "    carrier_features = expected_dep_delay_carrier.join(\n",
    "        expected_arr_delay_carrier, 'carrier', 'outer'\n",
    "    )\n",
    "    \n",
    "    if expected_air_time_carrier is not None:\n",
    "        carrier_features = carrier_features.join(\n",
    "            expected_air_time_carrier, 'carrier', 'outer'\n",
    "        )\n",
    "    \n",
    "    print(f\"Per-carrier features computed for {carrier_features.count()} carriers\")\n",
    "    print(\"\\nSample per-carrier features:\")\n",
    "    display(carrier_features.limit(10))\n",
    "    \n",
    "    # Save\n",
    "    carrier_features_path = f\"{FOLDER_PATH}/expected_values_carrier.parquet\"\n",
    "    carrier_features.write.mode(\"overwrite\").parquet(carrier_features_path)\n",
    "    print(f\"\\nSaved to: {carrier_features_path}\")\n",
    "else:\n",
    "    print(\"Carrier data not available. Run previous cell first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per-Carrier-Airport Expected Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-carrier-airport expected values\n",
    "# These are conditional on both carrier AND airport\n",
    "\n",
    "if 'ts_data_cond' in locals() and 'carrier' in ts_data_cond.columns:\n",
    "    print(\"Computing per-carrier-airport expected values...\")\n",
    "    print(\"This may take a while due to the large number of combinations...\\n\")\n",
    "    \n",
    "    # Expected departure delay by carrier-airport (origin)\n",
    "    expected_dep_delay_carrier_airport = (\n",
    "        ts_data_cond\n",
    "        .filter(col('DEP_DELAY').isNotNull())\n",
    "        .groupBy('carrier', 'origin')\n",
    "        .agg(\n",
    "            F.avg('DEP_DELAY').alias('expected_dep_delay_carrier_airport'),\n",
    "            F.stddev('DEP_DELAY').alias('std_dep_delay_carrier_airport'),\n",
    "            F.count('*').alias('flight_count_carrier_airport')\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Expected arrival delay by carrier-airport (origin)\n",
    "    expected_arr_delay_carrier_airport = (\n",
    "        ts_data_cond\n",
    "        .filter(col('ARR_DELAY').isNotNull())\n",
    "        .groupBy('carrier', 'origin')\n",
    "        .agg(\n",
    "            F.avg('ARR_DELAY').alias('expected_arr_delay_carrier_airport'),\n",
    "            F.stddev('ARR_DELAY').alias('std_arr_delay_carrier_airport')\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Expected turn time by carrier-airport\n",
    "    # Turn time = time between arrival at airport and next departure from same airport\n",
    "    # We need to compute this from flight sequences, but for now we can use taxi times as proxy\n",
    "    # Or compute from actual arrival to next scheduled departure\n",
    "    \n",
    "    # For now, compute expected taxi times as proxy for turn time components\n",
    "    expected_taxi_out_carrier_airport = None\n",
    "    expected_taxi_in_carrier_airport = None\n",
    "    \n",
    "    if 'taxi_out' in ts_data_cond.columns:\n",
    "        expected_taxi_out_carrier_airport = (\n",
    "            ts_data_cond\n",
    "            .filter(col('taxi_out').isNotNull())\n",
    "            .groupBy('carrier', 'origin')\n",
    "            .agg(\n",
    "                F.avg('taxi_out').alias('expected_taxi_out_carrier_airport'),\n",
    "                F.stddev('taxi_out').alias('std_taxi_out_carrier_airport')\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    if 'taxi_in' in ts_data_cond.columns:\n",
    "        expected_taxi_in_carrier_airport = (\n",
    "            ts_data_cond\n",
    "            .filter(col('taxi_in').isNotNull())\n",
    "            .groupBy('carrier', 'dest')\n",
    "            .agg(\n",
    "                F.avg('taxi_in').alias('expected_taxi_in_carrier_airport'),\n",
    "                F.stddev('taxi_in').alias('std_taxi_in_carrier_airport')\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Combine carrier-airport features\n",
    "    carrier_airport_features = expected_dep_delay_carrier_airport.join(\n",
    "        expected_arr_delay_carrier_airport, ['carrier', 'origin'], 'outer'\n",
    "    )\n",
    "    \n",
    "    if expected_taxi_out_carrier_airport is not None:\n",
    "        carrier_airport_features = carrier_airport_features.join(\n",
    "            expected_taxi_out_carrier_airport, ['carrier', 'origin'], 'outer'\n",
    "        )\n",
    "    \n",
    "    if expected_taxi_in_carrier_airport is not None:\n",
    "        # Join on carrier and dest (rename dest to origin for join)\n",
    "        taxi_in_renamed = expected_taxi_in_carrier_airport.withColumnRenamed('dest', 'origin')\n",
    "        carrier_airport_features = carrier_airport_features.join(\n",
    "            taxi_in_renamed, ['carrier', 'origin'], 'outer'\n",
    "        )\n",
    "    \n",
    "    print(f\"Per-carrier-airport features computed for {carrier_airport_features.count()} combinations\")\n",
    "    print(\"\\nSample per-carrier-airport features:\")\n",
    "    display(carrier_airport_features.limit(10))\n",
    "    \n",
    "    # Save\n",
    "    carrier_airport_features_path = f\"{FOLDER_PATH}/expected_values_carrier_airport.parquet\"\n",
    "    carrier_airport_features.write.mode(\"overwrite\").parquet(carrier_airport_features_path)\n",
    "    print(f\"\\nSaved to: {carrier_airport_features_path}\")\n",
    "else:\n",
    "    print(\"Carrier data not available. Run previous cell first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-carrier-airport-time expected values\n",
    "# Conditional on carrier, airport, AND time components (hour, day_of_week, month)\n",
    "\n",
    "if 'ts_data_cond' in locals() and 'carrier' in ts_data_cond.columns:\n",
    "    print(\"Computing per-carrier-airport-time expected values...\")\n",
    "    print(\"This will create many combinations - using hour and day_of_week for now...\\n\")\n",
    "    \n",
    "    # Create time buckets to reduce cardinality\n",
    "    # Hour buckets: 0-5 (early morning), 6-11 (morning), 12-17 (afternoon), 18-23 (evening)\n",
    "    ts_data_cond_time = ts_data_cond.withColumn(\n",
    "        'hour_bucket',\n",
    "        when(col('hour').between(0, 5), 'early_morning')\n",
    "        .when(col('hour').between(6, 11), 'morning')\n",
    "        .when(col('hour').between(12, 17), 'afternoon')\n",
    "        .otherwise('evening')\n",
    "    )\n",
    "    \n",
    "    # Expected departure delay by carrier-airport-hour_bucket\n",
    "    expected_dep_delay_carrier_airport_time = (\n",
    "        ts_data_cond_time\n",
    "        .filter(col('DEP_DELAY').isNotNull())\n",
    "        .groupBy('carrier', 'origin', 'hour_bucket')\n",
    "        .agg(\n",
    "            F.avg('DEP_DELAY').alias('expected_dep_delay_carrier_airport_time'),\n",
    "            F.stddev('DEP_DELAY').alias('std_dep_delay_carrier_airport_time'),\n",
    "            F.count('*').alias('flight_count_carrier_airport_time')\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Expected arrival delay by carrier-airport-hour_bucket\n",
    "    expected_arr_delay_carrier_airport_time = (\n",
    "        ts_data_cond_time\n",
    "        .filter(col('ARR_DELAY').isNotNull())\n",
    "        .groupBy('carrier', 'origin', 'hour_bucket')\n",
    "        .agg(\n",
    "            F.avg('ARR_DELAY').alias('expected_arr_delay_carrier_airport_time'),\n",
    "            F.stddev('ARR_DELAY').alias('std_arr_delay_carrier_airport_time')\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Expected taxi times by carrier-airport-hour_bucket\n",
    "    expected_taxi_out_carrier_airport_time = None\n",
    "    if 'taxi_out' in ts_data_cond_time.columns:\n",
    "        expected_taxi_out_carrier_airport_time = (\n",
    "            ts_data_cond_time\n",
    "            .filter(col('taxi_out').isNotNull())\n",
    "            .groupBy('carrier', 'origin', 'hour_bucket')\n",
    "            .agg(\n",
    "                F.avg('taxi_out').alias('expected_taxi_out_carrier_airport_time'),\n",
    "                F.stddev('taxi_out').alias('std_taxi_out_carrier_airport_time')\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Combine carrier-airport-time features\n",
    "    carrier_airport_time_features = expected_dep_delay_carrier_airport_time.join(\n",
    "        expected_arr_delay_carrier_airport_time, \n",
    "        ['carrier', 'origin', 'hour_bucket'], \n",
    "        'outer'\n",
    "    )\n",
    "    \n",
    "    if expected_taxi_out_carrier_airport_time is not None:\n",
    "        carrier_airport_time_features = carrier_airport_time_features.join(\n",
    "            expected_taxi_out_carrier_airport_time,\n",
    "            ['carrier', 'origin', 'hour_bucket'],\n",
    "            'outer'\n",
    "        )\n",
    "    \n",
    "    print(f\"Per-carrier-airport-time features computed for {carrier_airport_time_features.count()} combinations\")\n",
    "    print(\"\\nSample per-carrier-airport-time features:\")\n",
    "    display(carrier_airport_time_features.limit(10))\n",
    "    \n",
    "    # Save\n",
    "    carrier_airport_time_features_path = f\"{FOLDER_PATH}/expected_values_carrier_airport_time.parquet\"\n",
    "    carrier_airport_time_features.write.mode(\"overwrite\").parquet(carrier_airport_time_features_path)\n",
    "    print(f\"\\nSaved to: {carrier_airport_time_features_path}\")\n",
    "else:\n",
    "    print(\"Carrier data not available. Run previous cell first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per-Route Expected Values\n",
    "\n",
    "For Flight Lineage features, we also need expected air times by route (origin-destination pair)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-route expected values\n",
    "# Route = (origin, destination) pair\n",
    "\n",
    "if 'ts_data_cond' in locals():\n",
    "    print(\"Computing per-route expected values...\")\n",
    "    \n",
    "    # Expected air time by route\n",
    "    expected_air_time_route = None\n",
    "    if 'air_time' in ts_data_cond.columns:\n",
    "        expected_air_time_route = (\n",
    "            ts_data_cond\n",
    "            .filter(col('air_time').isNotNull())\n",
    "            .groupBy('origin', 'dest')\n",
    "            .agg(\n",
    "                F.avg('air_time').alias('expected_air_time_route'),\n",
    "                F.stddev('air_time').alias('std_air_time_route'),\n",
    "                F.count('*').alias('flight_count_route')\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        print(f\"Per-route air time features computed for {expected_air_time_route.count()} routes\")\n",
    "        print(\"\\nSample per-route features:\")\n",
    "        display(expected_air_time_route.limit(10))\n",
    "        \n",
    "        # Save\n",
    "        route_features_path = f\"{FOLDER_PATH}/expected_values_route.parquet\"\n",
    "        expected_air_time_route.write.mode(\"overwrite\").parquet(route_features_path)\n",
    "        print(f\"\\nSaved to: {route_features_path}\")\n",
    "    else:\n",
    "        print(\"air_time column not available\")\n",
    "else:\n",
    "    print(\"Data not available. Run previous cells first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per-Route-Time Expected Values\n",
    "\n",
    "Conditional on route AND time components (for time-of-day and seasonal effects on air time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prophet-Based Conditional Expected Values\n",
    "\n",
    "Use Prophet models to generate time-series based conditional expected values with trends and seasonality.\n",
    "These features capture temporal patterns better than simple averages and can be used for Flight Lineage feature engineering.\n",
    "\n",
    "### Approach:\n",
    "1. Aggregate time series by grouping (carrier, carrier-airport, carrier-airport-time, route, route-time)\n",
    "2. Fit Prophet models for each group (if sufficient data)\n",
    "3. Extract Prophet features (trend, seasonality, forecast) for each date\n",
    "4. These features provide conditional expected values that account for temporal trends\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per-Carrier Prophet Models\n",
    "\n",
    "Generate time-series based expected delays per carrier using Prophet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate per-carrier time series and fit Prophet models\n",
    "# This provides conditional expected delays by carrier that account for temporal trends\n",
    "\n",
    "if 'ts_data_cond' in locals() and 'carrier' in ts_data_cond.columns:\n",
    "    print(\"Generating per-carrier time series for Prophet models...\")\n",
    "    \n",
    "    # Aggregate departure delays by carrier and date\n",
    "    per_carrier_ts_spark = (\n",
    "        ts_data_cond\n",
    "        .filter(col('DEP_DELAY').isNotNull())\n",
    "        .groupBy('carrier', 'date')\n",
    "        .agg(\n",
    "            F.avg('DEP_DELAY').alias('avg_dep_delay'),\n",
    "            F.count('*').alias('flight_count')\n",
    "        )\n",
    "        .orderBy('carrier', 'date')\n",
    "    )\n",
    "    \n",
    "    # Convert to pandas for Prophet\n",
    "    per_carrier_ts = per_carrier_ts_spark.toPandas()\n",
    "    per_carrier_ts['ds'] = pd.to_datetime(per_carrier_ts['date'])\n",
    "    \n",
    "    print(f\"Per-carrier time series: {len(per_carrier_ts):,} rows\")\n",
    "    print(f\"Number of carriers: {per_carrier_ts['carrier'].nunique()}\")\n",
    "    \n",
    "    # Check data availability per carrier\n",
    "    carrier_day_counts = per_carrier_ts.groupby('carrier')['ds'].count().sort_values(ascending=False)\n",
    "    print(f\"\\nData availability statistics:\")\n",
    "    print(f\"  Min days: {carrier_day_counts.min()}\")\n",
    "    print(f\"  Max days: {carrier_day_counts.max()}\")\n",
    "    print(f\"  Mean days: {carrier_day_counts.mean():.1f}\")\n",
    "    print(f\"  Median days: {carrier_day_counts.median():.1f}\")\n",
    "    \n",
    "    # Need at least 14 days for weekly seasonality, 365 for yearly\n",
    "    min_days_required = 14\n",
    "    carriers_with_sufficient_data = carrier_day_counts[carrier_day_counts >= min_days_required].index.tolist()\n",
    "    print(f\"\\nCarriers with >= {min_days_required} days of data: {len(carriers_with_sufficient_data)}\")\n",
    "    \n",
    "    # Fit Prophet models for each carrier\n",
    "    print(f\"\\nFitting Prophet models for {len(carriers_with_sufficient_data)} carriers...\")\n",
    "    \n",
    "    prophet_features_per_carrier = []\n",
    "    \n",
    "    for carrier in carriers_with_sufficient_data:\n",
    "        carrier_data = per_carrier_ts[per_carrier_ts['carrier'] == carrier].sort_values('ds')\n",
    "        carrier_prophet_data = carrier_data[['ds', 'avg_dep_delay']].copy()\n",
    "        carrier_prophet_data = carrier_prophet_data.rename(columns={'avg_dep_delay': 'y'})\n",
    "        carrier_prophet_data = carrier_prophet_data.dropna()\n",
    "        \n",
    "        if len(carrier_prophet_data) < min_days_required:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Determine seasonality based on data availability\n",
    "            has_enough_for_yearly = len(carrier_prophet_data) >= 365\n",
    "            has_enough_for_weekly = len(carrier_prophet_data) >= 14\n",
    "            \n",
    "            # Fit Prophet model\n",
    "            prophet_model = Prophet(\n",
    "                yearly_seasonality=has_enough_for_yearly,\n",
    "                weekly_seasonality=has_enough_for_weekly,\n",
    "                daily_seasonality=False,\n",
    "                seasonality_mode='multiplicative',\n",
    "                interval_width=0.95,\n",
    "                changepoint_prior_scale=0.05\n",
    "            )\n",
    "            prophet_model.fit(carrier_prophet_data)\n",
    "            \n",
    "            # Generate forecast\n",
    "            forecast = prophet_model.predict(carrier_prophet_data[['ds']])\n",
    "            \n",
    "            # Extract features\n",
    "            feature_cols = ['ds', 'trend', 'weekly', 'yhat', 'yhat_lower', 'yhat_upper']\n",
    "            if 'yearly' in forecast.columns:\n",
    "                feature_cols.insert(2, 'yearly')\n",
    "            \n",
    "            features = forecast[feature_cols].copy()\n",
    "            features['carrier'] = carrier\n",
    "            \n",
    "            # Rename columns\n",
    "            rename_dict = {\n",
    "                'trend': 'prophet_trend_carrier',\n",
    "                'weekly': 'prophet_weekly_seasonality_carrier',\n",
    "                'yhat': 'prophet_forecast_dep_delay_carrier',\n",
    "                'yhat_lower': 'prophet_forecast_lower_carrier',\n",
    "                'yhat_upper': 'prophet_forecast_upper_carrier'\n",
    "            }\n",
    "            if 'yearly' in features.columns:\n",
    "                rename_dict['yearly'] = 'prophet_yearly_seasonality_carrier'\n",
    "            \n",
    "            features = features.rename(columns=rename_dict)\n",
    "            features['date_str'] = features['ds'].dt.strftime('%Y-%m-%d')\n",
    "            \n",
    "            prophet_features_per_carrier.append(features)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fitting Prophet for carrier {carrier}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Combine all carrier features\n",
    "    if prophet_features_per_carrier:\n",
    "        prophet_features_carrier_df = pd.concat(prophet_features_per_carrier, ignore_index=True)\n",
    "        print(f\"\\n✓ Generated Prophet features for {prophet_features_carrier_df['carrier'].nunique()} carriers\")\n",
    "        print(f\"Total feature rows: {len(prophet_features_carrier_df)}\")\n",
    "        print(\"\\nSample features:\")\n",
    "        print(prophet_features_carrier_df.head(10))\n",
    "        \n",
    "        # Save to parquet (convert back to Spark for saving)\n",
    "        prophet_features_carrier_spark = spark.createDataFrame(prophet_features_carrier_df)\n",
    "        carrier_prophet_features_path = f\"{FOLDER_PATH}/prophet_features_carrier.parquet\"\n",
    "        prophet_features_carrier_spark.write.mode(\"overwrite\").parquet(carrier_prophet_features_path)\n",
    "        print(f\"\\nSaved to: {carrier_prophet_features_path}\")\n",
    "    else:\n",
    "        print(\"\\nNo Prophet features generated for carriers\")\n",
    "else:\n",
    "    print(\"Carrier data not available. Run previous cells first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per-Carrier-Airport Prophet Models\n",
    "\n",
    "Generate time-series based expected delays per carrier-airport combination using Prophet.\n",
    "This provides conditional expected values that account for both carrier and airport-specific temporal patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate per-carrier-airport time series and fit Prophet models\n",
    "# This will take longer due to many combinations, so we'll filter to those with sufficient data\n",
    "\n",
    "if 'ts_data_cond' in locals() and 'carrier' in ts_data_cond.columns:\n",
    "    print(\"Generating per-carrier-airport time series for Prophet models...\")\n",
    "    print(\"This may take a while due to the large number of combinations...\\n\")\n",
    "    \n",
    "    # Aggregate departure delays by carrier, airport, and date\n",
    "    per_carrier_airport_ts_spark = (\n",
    "        ts_data_cond\n",
    "        .filter(col('DEP_DELAY').isNotNull())\n",
    "        .groupBy('carrier', 'origin', 'date')\n",
    "        .agg(\n",
    "            F.avg('DEP_DELAY').alias('avg_dep_delay'),\n",
    "            F.count('*').alias('flight_count')\n",
    "        )\n",
    "        .orderBy('carrier', 'origin', 'date')\n",
    "    )\n",
    "    \n",
    "    # Convert to pandas for Prophet\n",
    "    per_carrier_airport_ts = per_carrier_airport_ts_spark.toPandas()\n",
    "    per_carrier_airport_ts['ds'] = pd.to_datetime(per_carrier_airport_ts['date'])\n",
    "    \n",
    "    print(f\"Per-carrier-airport time series: {len(per_carrier_airport_ts):,} rows\")\n",
    "    print(f\"Number of carrier-airport combinations: {per_carrier_airport_ts.groupby(['carrier', 'origin']).ngroups}\")\n",
    "    \n",
    "    # Check data availability per carrier-airport\n",
    "    carrier_airport_day_counts = (\n",
    "        per_carrier_airport_ts\n",
    "        .groupby(['carrier', 'origin'])['ds']\n",
    "        .count()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nData availability statistics:\")\n",
    "    print(f\"  Min days: {carrier_airport_day_counts.min()}\")\n",
    "    print(f\"  Max days: {carrier_airport_day_counts.max()}\")\n",
    "    print(f\"  Mean days: {carrier_airport_day_counts.mean():.1f}\")\n",
    "    print(f\"  Median days: {carrier_airport_day_counts.median():.1f}\")\n",
    "    \n",
    "    # Need at least 14 days for weekly seasonality\n",
    "    min_days_required = 14\n",
    "    carrier_airports_with_sufficient_data = (\n",
    "        carrier_airport_day_counts[carrier_airport_day_counts >= min_days_required]\n",
    "        .index\n",
    "        .tolist()\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nCarrier-airport combinations with >= {min_days_required} days: {len(carrier_airports_with_sufficient_data)}\")\n",
    "    \n",
    "    # Limit to top N combinations for initial testing (can remove limit for full run)\n",
    "    # For production, fit models for all combinations with sufficient data\n",
    "    max_combinations = 100  # Adjust based on compute resources\n",
    "    if len(carrier_airports_with_sufficient_data) > max_combinations:\n",
    "        print(f\"Limiting to top {max_combinations} combinations by data availability...\")\n",
    "        top_combinations = carrier_airport_day_counts.head(max_combinations).index.tolist()\n",
    "    else:\n",
    "        top_combinations = carrier_airports_with_sufficient_data\n",
    "    \n",
    "    print(f\"Fitting Prophet models for {len(top_combinations)} carrier-airport combinations...\")\n",
    "    \n",
    "    prophet_features_per_carrier_airport = []\n",
    "    \n",
    "    for idx, (carrier, origin) in enumerate(top_combinations):\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            print(f\"  Processing {idx + 1}/{len(top_combinations)}: {carrier}-{origin}\")\n",
    "        \n",
    "        carrier_airport_data = per_carrier_airport_ts[\n",
    "            (per_carrier_airport_ts['carrier'] == carrier) & \n",
    "            (per_carrier_airport_ts['origin'] == origin)\n",
    "        ].sort_values('ds')\n",
    "        \n",
    "        carrier_airport_prophet_data = carrier_airport_data[['ds', 'avg_dep_delay']].copy()\n",
    "        carrier_airport_prophet_data = carrier_airport_prophet_data.rename(columns={'avg_dep_delay': 'y'})\n",
    "        carrier_airport_prophet_data = carrier_airport_prophet_data.dropna()\n",
    "        \n",
    "        if len(carrier_airport_prophet_data) < min_days_required:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Determine seasonality based on data availability\n",
    "            has_enough_for_yearly = len(carrier_airport_prophet_data) >= 365\n",
    "            has_enough_for_weekly = len(carrier_airport_prophet_data) >= 14\n",
    "            \n",
    "            # Fit Prophet model\n",
    "            prophet_model = Prophet(\n",
    "                yearly_seasonality=has_enough_for_yearly,\n",
    "                weekly_seasonality=has_enough_for_weekly,\n",
    "                daily_seasonality=False,\n",
    "                seasonality_mode='multiplicative',\n",
    "                interval_width=0.95,\n",
    "                changepoint_prior_scale=0.05\n",
    "            )\n",
    "            prophet_model.fit(carrier_airport_prophet_data)\n",
    "            \n",
    "            # Generate forecast\n",
    "            forecast = prophet_model.predict(carrier_airport_prophet_data[['ds']])\n",
    "            \n",
    "            # Extract features\n",
    "            feature_cols = ['ds', 'trend', 'weekly', 'yhat', 'yhat_lower', 'yhat_upper']\n",
    "            if 'yearly' in forecast.columns:\n",
    "                feature_cols.insert(2, 'yearly')\n",
    "            \n",
    "            features = forecast[feature_cols].copy()\n",
    "            features['carrier'] = carrier\n",
    "            features['origin'] = origin\n",
    "            \n",
    "            # Rename columns\n",
    "            rename_dict = {\n",
    "                'trend': 'prophet_trend_carrier_airport',\n",
    "                'weekly': 'prophet_weekly_seasonality_carrier_airport',\n",
    "                'yhat': 'prophet_forecast_dep_delay_carrier_airport',\n",
    "                'yhat_lower': 'prophet_forecast_lower_carrier_airport',\n",
    "                'yhat_upper': 'prophet_forecast_upper_carrier_airport'\n",
    "            }\n",
    "            if 'yearly' in features.columns:\n",
    "                rename_dict['yearly'] = 'prophet_yearly_seasonality_carrier_airport'\n",
    "            \n",
    "            features = features.rename(columns=rename_dict)\n",
    "            features['date_str'] = features['ds'].dt.strftime('%Y-%m-%d')\n",
    "            \n",
    "            prophet_features_per_carrier_airport.append(features)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error fitting Prophet for {carrier}-{origin}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Combine all carrier-airport features\n",
    "    if prophet_features_per_carrier_airport:\n",
    "        prophet_features_carrier_airport_df = pd.concat(prophet_features_per_carrier_airport, ignore_index=True)\n",
    "        print(f\"\\n✓ Generated Prophet features for {prophet_features_carrier_airport_df.groupby(['carrier', 'origin']).ngroups} carrier-airport combinations\")\n",
    "        print(f\"Total feature rows: {len(prophet_features_carrier_airport_df)}\")\n",
    "        print(\"\\nSample features:\")\n",
    "        print(prophet_features_carrier_airport_df.head(10))\n",
    "        \n",
    "        # Save to parquet\n",
    "        prophet_features_carrier_airport_spark = spark.createDataFrame(prophet_features_carrier_airport_df)\n",
    "        carrier_airport_prophet_features_path = f\"{FOLDER_PATH}/prophet_features_carrier_airport.parquet\"\n",
    "        prophet_features_carrier_airport_spark.write.mode(\"overwrite\").parquet(carrier_airport_prophet_features_path)\n",
    "        print(f\"\\nSaved to: {carrier_airport_prophet_features_path}\")\n",
    "    else:\n",
    "        print(\"\\nNo Prophet features generated for carrier-airport combinations\")\n",
    "else:\n",
    "    print(\"Carrier data not available. Run previous cells first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per-Carrier-Airport-Time Prophet Models\n",
    "\n",
    "Generate time-series based expected delays per carrier-airport-time combination using Prophet.\n",
    "This provides conditional expected values that account for carrier, airport, AND time-of-day patterns.\n",
    "Note: We use time buckets (early_morning, morning, afternoon, evening) to reduce cardinality while still capturing time-of-day effects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate per-carrier-airport-time time series and fit Prophet models\n",
    "# Using time buckets to reduce cardinality while capturing time-of-day effects\n",
    "\n",
    "if 'ts_data_cond_time' in locals() and 'carrier' in ts_data_cond_time.columns:\n",
    "    print(\"Generating per-carrier-airport-time time series for Prophet models...\")\n",
    "    print(\"Using time buckets (early_morning, morning, afternoon, evening)...\\n\")\n",
    "    \n",
    "    # Aggregate departure delays by carrier, airport, time bucket, and date\n",
    "    per_carrier_airport_time_ts_spark = (\n",
    "        ts_data_cond_time\n",
    "        .filter(col('DEP_DELAY').isNotNull())\n",
    "        .groupBy('carrier', 'origin', 'hour_bucket', 'date')\n",
    "        .agg(\n",
    "            F.avg('DEP_DELAY').alias('avg_dep_delay'),\n",
    "            F.count('*').alias('flight_count')\n",
    "        )\n",
    "        .orderBy('carrier', 'origin', 'hour_bucket', 'date')\n",
    "    )\n",
    "    \n",
    "    # Convert to pandas for Prophet\n",
    "    per_carrier_airport_time_ts = per_carrier_airport_time_ts_spark.toPandas()\n",
    "    per_carrier_airport_time_ts['ds'] = pd.to_datetime(per_carrier_airport_time_ts['date'])\n",
    "    \n",
    "    print(f\"Per-carrier-airport-time time series: {len(per_carrier_airport_time_ts):,} rows\")\n",
    "    print(f\"Number of carrier-airport-time combinations: {per_carrier_airport_time_ts.groupby(['carrier', 'origin', 'hour_bucket']).ngroups}\")\n",
    "    \n",
    "    # Check data availability per carrier-airport-time\n",
    "    carrier_airport_time_day_counts = (\n",
    "        per_carrier_airport_time_ts\n",
    "        .groupby(['carrier', 'origin', 'hour_bucket'])['ds']\n",
    "        .count()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nData availability statistics:\")\n",
    "    print(f\"  Min days: {carrier_airport_time_day_counts.min()}\")\n",
    "    print(f\"  Max days: {carrier_airport_time_day_counts.max()}\")\n",
    "    print(f\"  Mean days: {carrier_airport_time_day_counts.mean():.1f}\")\n",
    "    print(f\"  Median days: {carrier_airport_time_day_counts.median():.1f}\")\n",
    "    \n",
    "    # Need at least 14 days for weekly seasonality\n",
    "    min_days_required = 14\n",
    "    carrier_airport_times_with_sufficient_data = (\n",
    "        carrier_airport_time_day_counts[carrier_airport_time_day_counts >= min_days_required]\n",
    "        .index\n",
    "        .tolist()\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nCarrier-airport-time combinations with >= {min_days_required} days: {len(carrier_airport_times_with_sufficient_data)}\")\n",
    "    \n",
    "    # Limit to top N combinations for initial testing\n",
    "    max_combinations = 200  # Can adjust based on compute resources\n",
    "    if len(carrier_airport_times_with_sufficient_data) > max_combinations:\n",
    "        print(f\"Limiting to top {max_combinations} combinations by data availability...\")\n",
    "        top_combinations = carrier_airport_time_day_counts.head(max_combinations).index.tolist()\n",
    "    else:\n",
    "        top_combinations = carrier_airport_times_with_sufficient_data\n",
    "    \n",
    "    print(f\"Fitting Prophet models for {len(top_combinations)} carrier-airport-time combinations...\")\n",
    "    \n",
    "    prophet_features_per_carrier_airport_time = []\n",
    "    \n",
    "    for idx, (carrier, origin, hour_bucket) in enumerate(top_combinations):\n",
    "        if (idx + 1) % 20 == 0:\n",
    "            print(f\"  Processing {idx + 1}/{len(top_combinations)}: {carrier}-{origin}-{hour_bucket}\")\n",
    "        \n",
    "        carrier_airport_time_data = per_carrier_airport_time_ts[\n",
    "            (per_carrier_airport_time_ts['carrier'] == carrier) & \n",
    "            (per_carrier_airport_time_ts['origin'] == origin) &\n",
    "            (per_carrier_airport_time_ts['hour_bucket'] == hour_bucket)\n",
    "        ].sort_values('ds')\n",
    "        \n",
    "        carrier_airport_time_prophet_data = carrier_airport_time_data[['ds', 'avg_dep_delay']].copy()\n",
    "        carrier_airport_time_prophet_data = carrier_airport_time_prophet_data.rename(columns={'avg_dep_delay': 'y'})\n",
    "        carrier_airport_time_prophet_data = carrier_airport_time_prophet_data.dropna()\n",
    "        \n",
    "        if len(carrier_airport_time_prophet_data) < min_days_required:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Determine seasonality based on data availability\n",
    "            has_enough_for_yearly = len(carrier_airport_time_prophet_data) >= 365\n",
    "            has_enough_for_weekly = len(carrier_airport_time_prophet_data) >= 14\n",
    "            \n",
    "            # Fit Prophet model\n",
    "            prophet_model = Prophet(\n",
    "                yearly_seasonality=has_enough_for_yearly,\n",
    "                weekly_seasonality=has_enough_for_weekly,\n",
    "                daily_seasonality=False,\n",
    "                seasonality_mode='multiplicative',\n",
    "                interval_width=0.95,\n",
    "                changepoint_prior_scale=0.05\n",
    "            )\n",
    "            prophet_model.fit(carrier_airport_time_prophet_data)\n",
    "            \n",
    "            # Generate forecast\n",
    "            forecast = prophet_model.predict(carrier_airport_time_prophet_data[['ds']])\n",
    "            \n",
    "            # Extract features\n",
    "            feature_cols = ['ds', 'trend', 'weekly', 'yhat', 'yhat_lower', 'yhat_upper']\n",
    "            if 'yearly' in forecast.columns:\n",
    "                feature_cols.insert(2, 'yearly')\n",
    "            \n",
    "            features = forecast[feature_cols].copy()\n",
    "            features['carrier'] = carrier\n",
    "            features['origin'] = origin\n",
    "            features['hour_bucket'] = hour_bucket\n",
    "            \n",
    "            # Rename columns\n",
    "            rename_dict = {\n",
    "                'trend': 'prophet_trend_carrier_airport_time',\n",
    "                'weekly': 'prophet_weekly_seasonality_carrier_airport_time',\n",
    "                'yhat': 'prophet_forecast_dep_delay_carrier_airport_time',\n",
    "                'yhat_lower': 'prophet_forecast_lower_carrier_airport_time',\n",
    "                'yhat_upper': 'prophet_forecast_upper_carrier_airport_time'\n",
    "            }\n",
    "            if 'yearly' in features.columns:\n",
    "                rename_dict['yearly'] = 'prophet_yearly_seasonality_carrier_airport_time'\n",
    "            \n",
    "            features = features.rename(columns=rename_dict)\n",
    "            features['date_str'] = features['ds'].dt.strftime('%Y-%m-%d')\n",
    "            \n",
    "            prophet_features_per_carrier_airport_time.append(features)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error fitting Prophet for {carrier}-{origin}-{hour_bucket}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Combine all carrier-airport-time features\n",
    "    if prophet_features_per_carrier_airport_time:\n",
    "        prophet_features_carrier_airport_time_df = pd.concat(prophet_features_per_carrier_airport_time, ignore_index=True)\n",
    "        print(f\"\\n✓ Generated Prophet features for {prophet_features_carrier_airport_time_df.groupby(['carrier', 'origin', 'hour_bucket']).ngroups} carrier-airport-time combinations\")\n",
    "        print(f\"Total feature rows: {len(prophet_features_carrier_airport_time_df)}\")\n",
    "        print(\"\\nSample features:\")\n",
    "        print(prophet_features_carrier_airport_time_df.head(10))\n",
    "        \n",
    "        # Save to parquet\n",
    "        prophet_features_carrier_airport_time_spark = spark.createDataFrame(prophet_features_carrier_airport_time_df)\n",
    "        carrier_airport_time_prophet_features_path = f\"{FOLDER_PATH}/prophet_features_carrier_airport_time.parquet\"\n",
    "        prophet_features_carrier_airport_time_spark.write.mode(\"overwrite\").parquet(carrier_airport_time_prophet_features_path)\n",
    "        print(f\"\\nSaved to: {carrier_airport_time_prophet_features_path}\")\n",
    "    else:\n",
    "        print(\"\\nNo Prophet features generated for carrier-airport-time combinations\")\n",
    "else:\n",
    "    print(\"Carrier-time data not available. Run previous cells first (including time bucket creation).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per-Route Prophet Models (Air Time)\n",
    "\n",
    "Generate time-series based expected air times per route (origin-destination pair) using Prophet.\n",
    "This provides conditional expected air times that account for temporal trends and seasonality.\n",
    "Important for Flight Lineage features to predict expected flight duration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate per-route time series for air time and fit Prophet models\n",
    "# This provides conditional expected air times by route that account for temporal trends\n",
    "\n",
    "if 'ts_data_cond' in locals() and 'air_time' in ts_data_cond.columns:\n",
    "    print(\"Generating per-route time series for air time Prophet models...\")\n",
    "    \n",
    "    # Aggregate air time by route (origin-dest) and date\n",
    "    per_route_ts_spark = (\n",
    "        ts_data_cond\n",
    "        .filter(col('air_time').isNotNull())\n",
    "        .groupBy('origin', 'dest', 'date')\n",
    "        .agg(\n",
    "            F.avg('air_time').alias('avg_air_time'),\n",
    "            F.count('*').alias('flight_count')\n",
    "        )\n",
    "        .orderBy('origin', 'dest', 'date')\n",
    "    )\n",
    "    \n",
    "    # Convert to pandas for Prophet\n",
    "    per_route_ts = per_route_ts_spark.toPandas()\n",
    "    per_route_ts['ds'] = pd.to_datetime(per_route_ts['date'])\n",
    "    \n",
    "    print(f\"Per-route time series: {len(per_route_ts):,} rows\")\n",
    "    print(f\"Number of routes: {per_route_ts.groupby(['origin', 'dest']).ngroups}\")\n",
    "    \n",
    "    # Check data availability per route\n",
    "    route_day_counts = (\n",
    "        per_route_ts\n",
    "        .groupby(['origin', 'dest'])['ds']\n",
    "        .count()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nData availability statistics:\")\n",
    "    print(f\"  Min days: {route_day_counts.min()}\")\n",
    "    print(f\"  Max days: {route_day_counts.max()}\")\n",
    "    print(f\"  Mean days: {route_day_counts.mean():.1f}\")\n",
    "    print(f\"  Median days: {route_day_counts.median():.1f}\")\n",
    "    \n",
    "    # Need at least 14 days for weekly seasonality\n",
    "    min_days_required = 14\n",
    "    routes_with_sufficient_data = (\n",
    "        route_day_counts[route_day_counts >= min_days_required]\n",
    "        .index\n",
    "        .tolist()\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nRoutes with >= {min_days_required} days: {len(routes_with_sufficient_data)}\")\n",
    "    \n",
    "    # Limit to top N routes for initial testing\n",
    "    max_routes = 150  # Can adjust based on compute resources\n",
    "    if len(routes_with_sufficient_data) > max_routes:\n",
    "        print(f\"Limiting to top {max_routes} routes by data availability...\")\n",
    "        top_routes = route_day_counts.head(max_routes).index.tolist()\n",
    "    else:\n",
    "        top_routes = routes_with_sufficient_data\n",
    "    \n",
    "    print(f\"Fitting Prophet models for {len(top_routes)} routes...\")\n",
    "    \n",
    "    prophet_features_per_route = []\n",
    "    \n",
    "    for idx, (origin, dest) in enumerate(top_routes):\n",
    "        if (idx + 1) % 20 == 0:\n",
    "            print(f\"  Processing {idx + 1}/{len(top_routes)}: {origin}-{dest}\")\n",
    "        \n",
    "        route_data = per_route_ts[\n",
    "            (per_route_ts['origin'] == origin) & \n",
    "            (per_route_ts['dest'] == dest)\n",
    "        ].sort_values('ds')\n",
    "        \n",
    "        route_prophet_data = route_data[['ds', 'avg_air_time']].copy()\n",
    "        route_prophet_data = route_prophet_data.rename(columns={'avg_air_time': 'y'})\n",
    "        route_prophet_data = route_prophet_data.dropna()\n",
    "        \n",
    "        if len(route_prophet_data) < min_days_required:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Determine seasonality based on data availability\n",
    "            has_enough_for_yearly = len(route_prophet_data) >= 365\n",
    "            has_enough_for_weekly = len(route_prophet_data) >= 14\n",
    "            \n",
    "            # Fit Prophet model\n",
    "            prophet_model = Prophet(\n",
    "                yearly_seasonality=has_enough_for_yearly,\n",
    "                weekly_seasonality=has_enough_for_weekly,\n",
    "                daily_seasonality=False,\n",
    "                seasonality_mode='multiplicative',\n",
    "                interval_width=0.95,\n",
    "                changepoint_prior_scale=0.05\n",
    "            )\n",
    "            prophet_model.fit(route_prophet_data)\n",
    "            \n",
    "            # Generate forecast\n",
    "            forecast = prophet_model.predict(route_prophet_data[['ds']])\n",
    "            \n",
    "            # Extract features\n",
    "            feature_cols = ['ds', 'trend', 'weekly', 'yhat', 'yhat_lower', 'yhat_upper']\n",
    "            if 'yearly' in forecast.columns:\n",
    "                feature_cols.insert(2, 'yearly')\n",
    "            \n",
    "            features = forecast[feature_cols].copy()\n",
    "            features['origin'] = origin\n",
    "            features['dest'] = dest\n",
    "            \n",
    "            # Rename columns\n",
    "            rename_dict = {\n",
    "                'trend': 'prophet_trend_route',\n",
    "                'weekly': 'prophet_weekly_seasonality_route',\n",
    "                'yhat': 'prophet_forecast_air_time_route',\n",
    "                'yhat_lower': 'prophet_forecast_lower_route',\n",
    "                'yhat_upper': 'prophet_forecast_upper_route'\n",
    "            }\n",
    "            if 'yearly' in features.columns:\n",
    "                rename_dict['yearly'] = 'prophet_yearly_seasonality_route'\n",
    "            \n",
    "            features = features.rename(columns=rename_dict)\n",
    "            features['date_str'] = features['ds'].dt.strftime('%Y-%m-%d')\n",
    "            \n",
    "            prophet_features_per_route.append(features)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error fitting Prophet for {origin}-{dest}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Combine all route features\n",
    "    if prophet_features_per_route:\n",
    "        prophet_features_route_df = pd.concat(prophet_features_per_route, ignore_index=True)\n",
    "        print(f\"\\n✓ Generated Prophet features for {prophet_features_route_df.groupby(['origin', 'dest']).ngroups} routes\")\n",
    "        print(f\"Total feature rows: {len(prophet_features_route_df)}\")\n",
    "        print(\"\\nSample features:\")\n",
    "        print(prophet_features_route_df.head(10))\n",
    "        \n",
    "        # Save to parquet\n",
    "        prophet_features_route_spark = spark.createDataFrame(prophet_features_route_df)\n",
    "        route_prophet_features_path = f\"{FOLDER_PATH}/prophet_features_route.parquet\"\n",
    "        prophet_features_route_spark.write.mode(\"overwrite\").parquet(route_prophet_features_path)\n",
    "        print(f\"\\nSaved to: {route_prophet_features_path}\")\n",
    "    else:\n",
    "        print(\"\\nNo Prophet features generated for routes\")\n",
    "else:\n",
    "    print(\"Route data not available. Run previous cells first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per-Route-Time Prophet Models (Air Time)\n",
    "\n",
    "Generate time-series based expected air times per route-time combination using Prophet.\n",
    "This provides conditional expected air times that account for route, time-of-day, AND temporal trends.\n",
    "Important for Flight Lineage features to predict expected flight duration conditional on time of day.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate per-route-time time series for air time and fit Prophet models\n",
    "# Using time buckets to reduce cardinality while capturing time-of-day effects\n",
    "\n",
    "if 'ts_data_cond_time' in locals() and 'air_time' in ts_data_cond_time.columns:\n",
    "    print(\"Generating per-route-time time series for air time Prophet models...\")\n",
    "    print(\"Using time buckets (early_morning, morning, afternoon, evening)...\\n\")\n",
    "    \n",
    "    # Aggregate air time by route, time bucket, and date\n",
    "    per_route_time_ts_spark = (\n",
    "        ts_data_cond_time\n",
    "        .filter(col('air_time').isNotNull())\n",
    "        .groupBy('origin', 'dest', 'hour_bucket', 'date')\n",
    "        .agg(\n",
    "            F.avg('air_time').alias('avg_air_time'),\n",
    "            F.count('*').alias('flight_count')\n",
    "        )\n",
    "        .orderBy('origin', 'dest', 'hour_bucket', 'date')\n",
    "    )\n",
    "    \n",
    "    # Convert to pandas for Prophet\n",
    "    per_route_time_ts = per_route_time_ts_spark.toPandas()\n",
    "    per_route_time_ts['ds'] = pd.to_datetime(per_route_time_ts['date'])\n",
    "    \n",
    "    print(f\"Per-route-time time series: {len(per_route_time_ts):,} rows\")\n",
    "    print(f\"Number of route-time combinations: {per_route_time_ts.groupby(['origin', 'dest', 'hour_bucket']).ngroups}\")\n",
    "    \n",
    "    # Check data availability per route-time\n",
    "    route_time_day_counts = (\n",
    "        per_route_time_ts\n",
    "        .groupby(['origin', 'dest', 'hour_bucket'])['ds']\n",
    "        .count()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nData availability statistics:\")\n",
    "    print(f\"  Min days: {route_time_day_counts.min()}\")\n",
    "    print(f\"  Max days: {route_time_day_counts.max()}\")\n",
    "    print(f\"  Mean days: {route_time_day_counts.mean():.1f}\")\n",
    "    print(f\"  Median days: {route_time_day_counts.median():.1f}\")\n",
    "    \n",
    "    # Need at least 14 days for weekly seasonality\n",
    "    min_days_required = 14\n",
    "    route_times_with_sufficient_data = (\n",
    "        route_time_day_counts[route_time_day_counts >= min_days_required]\n",
    "        .index\n",
    "        .tolist()\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nRoute-time combinations with >= {min_days_required} days: {len(route_times_with_sufficient_data)}\")\n",
    "    \n",
    "    # Limit to top N combinations for initial testing\n",
    "    max_combinations = 200  # Can adjust based on compute resources\n",
    "    if len(route_times_with_sufficient_data) > max_combinations:\n",
    "        print(f\"Limiting to top {max_combinations} combinations by data availability...\")\n",
    "        top_combinations = route_time_day_counts.head(max_combinations).index.tolist()\n",
    "    else:\n",
    "        top_combinations = route_times_with_sufficient_data\n",
    "    \n",
    "    print(f\"Fitting Prophet models for {len(top_combinations)} route-time combinations...\")\n",
    "    \n",
    "    prophet_features_per_route_time = []\n",
    "    \n",
    "    for idx, (origin, dest, hour_bucket) in enumerate(top_combinations):\n",
    "        if (idx + 1) % 20 == 0:\n",
    "            print(f\"  Processing {idx + 1}/{len(top_combinations)}: {origin}-{dest}-{hour_bucket}\")\n",
    "        \n",
    "        route_time_data = per_route_time_ts[\n",
    "            (per_route_time_ts['origin'] == origin) & \n",
    "            (per_route_time_ts['dest'] == dest) &\n",
    "            (per_route_time_ts['hour_bucket'] == hour_bucket)\n",
    "        ].sort_values('ds')\n",
    "        \n",
    "        route_time_prophet_data = route_time_data[['ds', 'avg_air_time']].copy()\n",
    "        route_time_prophet_data = route_time_prophet_data.rename(columns={'avg_air_time': 'y'})\n",
    "        route_time_prophet_data = route_time_prophet_data.dropna()\n",
    "        \n",
    "        if len(route_time_prophet_data) < min_days_required:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Determine seasonality based on data availability\n",
    "            has_enough_for_yearly = len(route_time_prophet_data) >= 365\n",
    "            has_enough_for_weekly = len(route_time_prophet_data) >= 14\n",
    "            \n",
    "            # Fit Prophet model\n",
    "            prophet_model = Prophet(\n",
    "                yearly_seasonality=has_enough_for_yearly,\n",
    "                weekly_seasonality=has_enough_for_weekly,\n",
    "                daily_seasonality=False,\n",
    "                seasonality_mode='multiplicative',\n",
    "                interval_width=0.95,\n",
    "                changepoint_prior_scale=0.05\n",
    "            )\n",
    "            prophet_model.fit(route_time_prophet_data)\n",
    "            \n",
    "            # Generate forecast\n",
    "            forecast = prophet_model.predict(route_time_prophet_data[['ds']])\n",
    "            \n",
    "            # Extract features\n",
    "            feature_cols = ['ds', 'trend', 'weekly', 'yhat', 'yhat_lower', 'yhat_upper']\n",
    "            if 'yearly' in forecast.columns:\n",
    "                feature_cols.insert(2, 'yearly')\n",
    "            \n",
    "            features = forecast[feature_cols].copy()\n",
    "            features['origin'] = origin\n",
    "            features['dest'] = dest\n",
    "            features['hour_bucket'] = hour_bucket\n",
    "            \n",
    "            # Rename columns\n",
    "            rename_dict = {\n",
    "                'trend': 'prophet_trend_route_time',\n",
    "                'weekly': 'prophet_weekly_seasonality_route_time',\n",
    "                'yhat': 'prophet_forecast_air_time_route_time',\n",
    "                'yhat_lower': 'prophet_forecast_lower_route_time',\n",
    "                'yhat_upper': 'prophet_forecast_upper_route_time'\n",
    "            }\n",
    "            if 'yearly' in features.columns:\n",
    "                rename_dict['yearly'] = 'prophet_yearly_seasonality_route_time'\n",
    "            \n",
    "            features = features.rename(columns=rename_dict)\n",
    "            features['date_str'] = features['ds'].dt.strftime('%Y-%m-%d')\n",
    "            \n",
    "            prophet_features_per_route_time.append(features)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error fitting Prophet for {origin}-{dest}-{hour_bucket}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Combine all route-time features\n",
    "    if prophet_features_per_route_time:\n",
    "        prophet_features_route_time_df = pd.concat(prophet_features_per_route_time, ignore_index=True)\n",
    "        print(f\"\\n✓ Generated Prophet features for {prophet_features_route_time_df.groupby(['origin', 'dest', 'hour_bucket']).ngroups} route-time combinations\")\n",
    "        print(f\"Total feature rows: {len(prophet_features_route_time_df)}\")\n",
    "        print(\"\\nSample features:\")\n",
    "        print(prophet_features_route_time_df.head(10))\n",
    "        \n",
    "        # Save to parquet\n",
    "        prophet_features_route_time_spark = spark.createDataFrame(prophet_features_route_time_df)\n",
    "        route_time_prophet_features_path = f\"{FOLDER_PATH}/prophet_features_route_time.parquet\"\n",
    "        prophet_features_route_time_spark.write.mode(\"overwrite\").parquet(route_time_prophet_features_path)\n",
    "        print(f\"\\nSaved to: {route_time_prophet_features_path}\")\n",
    "    else:\n",
    "        print(\"\\nNo Prophet features generated for route-time combinations\")\n",
    "else:\n",
    "    print(\"Route-time data not available. Run previous cells first (including time bucket creation).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Prophet-Based Conditional Expected Values\n",
    "\n",
    "### Generated Features\n",
    "\n",
    "The following Prophet-based features have been generated and saved to parquet files:\n",
    "\n",
    "1. **Per-Carrier Features** (`prophet_features_carrier.parquet`)\n",
    "   - `prophet_forecast_dep_delay_carrier`: Expected departure delay by carrier (with temporal trends)\n",
    "   - `prophet_trend_carrier`: Long-term trend component\n",
    "   - `prophet_weekly_seasonality_carrier`: Weekly seasonality component\n",
    "   - `prophet_yearly_seasonality_carrier`: Yearly seasonality component (if sufficient data)\n",
    "   - Join keys: `carrier`, `date_str`\n",
    "\n",
    "2. **Per-Carrier-Airport Features** (`prophet_features_carrier_airport.parquet`)\n",
    "   - `prophet_forecast_dep_delay_carrier_airport`: Expected departure delay by carrier-airport\n",
    "   - Similar trend/seasonality components\n",
    "   - Join keys: `carrier`, `origin`, `date_str`\n",
    "\n",
    "3. **Per-Carrier-Airport-Time Features** (`prophet_features_carrier_airport_time.parquet`)\n",
    "   - `prophet_forecast_dep_delay_carrier_airport_time`: Expected departure delay by carrier-airport-time\n",
    "   - Conditional on time bucket (early_morning, morning, afternoon, evening)\n",
    "   - Join keys: `carrier`, `origin`, `hour_bucket`, `date_str`\n",
    "\n",
    "4. **Per-Route Features** (`prophet_features_route.parquet`)\n",
    "   - `prophet_forecast_air_time_route`: Expected air time by route (origin-dest)\n",
    "   - Important for Flight Lineage features to predict flight duration\n",
    "   - Join keys: `origin`, `dest`, `date_str`\n",
    "\n",
    "5. **Per-Route-Time Features** (`prophet_features_route_time.parquet`)\n",
    "   - `prophet_forecast_air_time_route_time`: Expected air time by route-time\n",
    "   - Conditional on time bucket\n",
    "   - Join keys: `origin`, `dest`, `hour_bucket`, `date_str`\n",
    "\n",
    "### Usage in Flight Lineage Features\n",
    "\n",
    "These Prophet features can be used to replace or supplement the simple average-based conditional expected values:\n",
    "\n",
    "- **Expected Turn Time**: Use `prophet_forecast_dep_delay_carrier_airport_time` to predict delays that affect turn time\n",
    "- **Expected Air Time**: Use `prophet_forecast_air_time_route_time` for route-specific air time predictions\n",
    "- **Conditional Expected Delays**: Use carrier/airport/time-specific forecasts for more accurate delay expectations\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Load these features in Flight Lineage Features Experiment notebook\n",
    "2. Join to flight data using the appropriate join keys\n",
    "3. Use Prophet forecasts as features in the deterministic prediction formulas\n",
    "4. Compare performance with simple average-based features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate per-carrier-airport time series and fit Prophet models\n",
    "# This will take longer due to many combinations, so we'll filter to those with sufficient data\n",
    "\n",
    "if 'ts_data_cond' in locals() and 'carrier' in ts_data_cond.columns:\n",
    "    print(\"Generating per-carrier-airport time series for Prophet models...\")\n",
    "    print(\"This may take a while due to the large number of combinations...\\n\")\n",
    "    \n",
    "    # Aggregate departure delays by carrier, airport, and date\n",
    "    per_carrier_airport_ts_spark = (\n",
    "        ts_data_cond\n",
    "        .filter(col('DEP_DELAY').isNotNull())\n",
    "        .groupBy('carrier', 'origin', 'date')\n",
    "        .agg(\n",
    "            F.avg('DEP_DELAY').alias('avg_dep_delay'),\n",
    "            F.count('*').alias('flight_count')\n",
    "        )\n",
    "        .orderBy('carrier', 'origin', 'date')\n",
    "    )\n",
    "    \n",
    "    # Convert to pandas for Prophet\n",
    "    per_carrier_airport_ts = per_carrier_airport_ts_spark.toPandas()\n",
    "    per_carrier_airport_ts['ds'] = pd.to_datetime(per_carrier_airport_ts['date'])\n",
    "    \n",
    "    print(f\"Per-carrier-airport time series: {len(per_carrier_airport_ts):,} rows\")\n",
    "    print(f\"Number of carrier-airport combinations: {per_carrier_airport_ts.groupby(['carrier', 'origin']).ngroups}\")\n",
    "    \n",
    "    # Check data availability per carrier-airport\n",
    "    carrier_airport_day_counts = (\n",
    "        per_carrier_airport_ts\n",
    "        .groupby(['carrier', 'origin'])['ds']\n",
    "        .count()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nData availability statistics:\")\n",
    "    print(f\"  Min days: {carrier_airport_day_counts.min()}\")\n",
    "    print(f\"  Max days: {carrier_airport_day_counts.max()}\")\n",
    "    print(f\"  Mean days: {carrier_airport_day_counts.mean():.1f}\")\n",
    "    print(f\"  Median days: {carrier_airport_day_counts.median():.1f}\")\n",
    "    \n",
    "    # Need at least 14 days for weekly seasonality\n",
    "    min_days_required = 14\n",
    "    carrier_airports_with_sufficient_data = (\n",
    "        carrier_airport_day_counts[carrier_airport_day_counts >= min_days_required]\n",
    "        .index\n",
    "        .tolist()\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nCarrier-airport combinations with >= {min_days_required} days: {len(carrier_airports_with_sufficient_data)}\")\n",
    "    \n",
    "    # Limit to top N combinations for initial testing (can remove limit for full run)\n",
    "    # For production, fit models for all combinations with sufficient data\n",
    "    max_combinations = 100  # Adjust based on compute resources\n",
    "    if len(carrier_airports_with_sufficient_data) > max_combinations:\n",
    "        print(f\"Limiting to top {max_combinations} combinations by data availability...\")\n",
    "        top_combinations = carrier_airport_day_counts.head(max_combinations).index.tolist()\n",
    "    else:\n",
    "        top_combinations = carrier_airports_with_sufficient_data\n",
    "    \n",
    "    print(f\"Fitting Prophet models for {len(top_combinations)} carrier-airport combinations...\")\n",
    "    \n",
    "    prophet_features_per_carrier_airport = []\n",
    "    \n",
    "    for idx, (carrier, origin) in enumerate(top_combinations):\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            print(f\"  Processing {idx + 1}/{len(top_combinations)}: {carrier}-{origin}\")\n",
    "        \n",
    "        carrier_airport_data = per_carrier_airport_ts[\n",
    "            (per_carrier_airport_ts['carrier'] == carrier) & \n",
    "            (per_carrier_airport_ts['origin'] == origin)\n",
    "        ].sort_values('ds')\n",
    "        \n",
    "        carrier_airport_prophet_data = carrier_airport_data[['ds', 'avg_dep_delay']].copy()\n",
    "        carrier_airport_prophet_data = carrier_airport_prophet_data.rename(columns={'avg_dep_delay': 'y'})\n",
    "        carrier_airport_prophet_data = carrier_airport_prophet_data.dropna()\n",
    "        \n",
    "        if len(carrier_airport_prophet_data) < min_days_required:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Determine seasonality based on data availability\n",
    "            has_enough_for_yearly = len(carrier_airport_prophet_data) >= 365\n",
    "            has_enough_for_weekly = len(carrier_airport_prophet_data) >= 14\n",
    "            \n",
    "            # Fit Prophet model\n",
    "            prophet_model = Prophet(\n",
    "                yearly_seasonality=has_enough_for_yearly,\n",
    "                weekly_seasonality=has_enough_for_weekly,\n",
    "                daily_seasonality=False,\n",
    "                seasonality_mode='multiplicative',\n",
    "                interval_width=0.95,\n",
    "                changepoint_prior_scale=0.05\n",
    "            )\n",
    "            prophet_model.fit(carrier_airport_prophet_data)\n",
    "            \n",
    "            # Generate forecast\n",
    "            forecast = prophet_model.predict(carrier_airport_prophet_data[['ds']])\n",
    "            \n",
    "            # Extract features\n",
    "            feature_cols = ['ds', 'trend', 'weekly', 'yhat', 'yhat_lower', 'yhat_upper']\n",
    "            if 'yearly' in forecast.columns:\n",
    "                feature_cols.insert(2, 'yearly')\n",
    "            \n",
    "            features = forecast[feature_cols].copy()\n",
    "            features['carrier'] = carrier\n",
    "            features['origin'] = origin\n",
    "            \n",
    "            # Rename columns\n",
    "            rename_dict = {\n",
    "                'trend': 'prophet_trend_carrier_airport',\n",
    "                'weekly': 'prophet_weekly_seasonality_carrier_airport',\n",
    "                'yhat': 'prophet_forecast_dep_delay_carrier_airport',\n",
    "                'yhat_lower': 'prophet_forecast_lower_carrier_airport',\n",
    "                'yhat_upper': 'prophet_forecast_upper_carrier_airport'\n",
    "            }\n",
    "            if 'yearly' in features.columns:\n",
    "                rename_dict['yearly'] = 'prophet_yearly_seasonality_carrier_airport'\n",
    "            \n",
    "            features = features.rename(columns=rename_dict)\n",
    "            features['date_str'] = features['ds'].dt.strftime('%Y-%m-%d')\n",
    "            \n",
    "            prophet_features_per_carrier_airport.append(features)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error fitting Prophet for {carrier}-{origin}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Combine all carrier-airport features\n",
    "    if prophet_features_per_carrier_airport:\n",
    "        prophet_features_carrier_airport_df = pd.concat(prophet_features_per_carrier_airport, ignore_index=True)\n",
    "        print(f\"\\n✓ Generated Prophet features for {prophet_features_carrier_airport_df.groupby(['carrier', 'origin']).ngroups} carrier-airport combinations\")\n",
    "        print(f\"Total feature rows: {len(prophet_features_carrier_airport_df)}\")\n",
    "        print(\"\\nSample features:\")\n",
    "        print(prophet_features_carrier_airport_df.head(10))\n",
    "        \n",
    "        # Save to parquet\n",
    "        prophet_features_carrier_airport_spark = spark.createDataFrame(prophet_features_carrier_airport_df)\n",
    "        carrier_airport_prophet_features_path = f\"{FOLDER_PATH}/prophet_features_carrier_airport.parquet\"\n",
    "        prophet_features_carrier_airport_spark.write.mode(\"overwrite\").parquet(carrier_airport_prophet_features_path)\n",
    "        print(f\"\\nSaved to: {carrier_airport_prophet_features_path}\")\n",
    "    else:\n",
    "        print(\"\\nNo Prophet features generated for carrier-airport combinations\")\n",
    "else:\n",
    "    print(\"Carrier data not available. Run previous cells first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-route-time expected values\n",
    "# Conditional on route AND time components\n",
    "\n",
    "if 'ts_data_cond_time' in locals() and 'air_time' in ts_data_cond_time.columns:\n",
    "    print(\"Computing per-route-time expected values...\")\n",
    "    \n",
    "    # Expected air time by route-hour_bucket\n",
    "    expected_air_time_route_time = (\n",
    "        ts_data_cond_time\n",
    "        .filter(col('air_time').isNotNull())\n",
    "        .groupBy('origin', 'dest', 'hour_bucket')\n",
    "        .agg(\n",
    "            F.avg('air_time').alias('expected_air_time_route_time'),\n",
    "            F.stddev('air_time').alias('std_air_time_route_time'),\n",
    "            F.count('*').alias('flight_count_route_time')\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    print(f\"Per-route-time air time features computed for {expected_air_time_route_time.count()} combinations\")\n",
    "    print(\"\\nSample per-route-time features:\")\n",
    "    display(expected_air_time_route_time.limit(10))\n",
    "    \n",
    "    # Save\n",
    "    route_time_features_path = f\"{FOLDER_PATH}/expected_values_route_time.parquet\"\n",
    "    expected_air_time_route_time.write.mode(\"overwrite\").parquet(route_time_features_path)\n",
    "    print(f\"\\nSaved to: {route_time_features_path}\")\n",
    "else:\n",
    "    print(\"Data not available. Run previous cells first.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Time-Series Features Experiment",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
