{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7badbcab-af68-4860-a1c7-3838afca3705",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Demo: MVP Conditional Expected Values with Linear Regression\n",
    "\n",
    "This demo shows how to use the MVP conditional expected values estimator in a linear regression pipeline with cross-validation.\n",
    "\n",
    "**Key Features:**\n",
    "- Flight lineage features (already applied in `split.py` when folds are created)\n",
    "- Conditional expected values (computed on-the-fly from training data)\n",
    "- Simple linear regression model\n",
    "- Cross-validation with automatic fold handling\n",
    "\n",
    "**Note:** Flight lineage features are pre-applied to all folds by `split.py`, so they don't need to be added in the pipeline.\n",
    "\n",
    "**Naming Convention:**\n",
    "- **\"crs\"** prefix = Raw data columns from source (or direct LAGs of raw data)\n",
    "  - Examples: `crs_elapsed_time`, `prev_flight_crs_elapsed_time`\n",
    "- **\"scheduled\"** prefix = Engineered/computed features derived from CRS data\n",
    "  - Examples: `scheduled_lineage_rotation_time_minutes`, `prev_flight_scheduled_flight_time_minutes`\n",
    "\n",
    "**Label and Prediction Handling:**\n",
    "- **Log Transform**: DEP_DELAY is transformed using `SIGN(DEP_DELAY) * LOG(ABS(DEP_DELAY) + 1.0)` to handle the right-skewed distribution while preserving sign\n",
    "- **Inverse Transform**: Predictions are transformed back using `SIGN(pred) * (EXP(ABS(pred)) - 1)` and floored to 0\n",
    "- This approach:\n",
    "  - Better handles the skewed delay distribution\n",
    "  - Preserves information about negative delays (early departures) during training\n",
    "  - Ensures final predictions are non-negative (matches hypothesis: `departure_delay ≈ max(0, required_time - rotation_time)`)\n",
    "  - Prevents penalizing the model for \"under-predicting\" negative delays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c18a761-8c49-478b-92f1-f8d7fd4f9aa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efe8b902-bbdb-4c46-b779-54d3c4133e26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# Load modules from our Databricks repo\n",
    "import importlib.util\n",
    "\n",
    "# CV module\n",
    "cv_path = \"/Workspace/Shared/Team 4_2/flight-departure-delay-predictive-modeling/notebooks/Cross Validator/cv.py\"\n",
    "spec = importlib.util.spec_from_file_location(\"cv\", cv_path)\n",
    "cv = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(cv)\n",
    "\n",
    "# Conditional expected values MVP estimator\n",
    "cond_exp_mvp_path = \"/Workspace/Shared/Team 4_2/flight-departure-delay-predictive-modeling/notebooks/Feature Engineering/flight_lineage/mvp_simple/conditional_expected_values_mvp.py\"\n",
    "spec = importlib.util.spec_from_file_location(\"conditional_expected_values_mvp\", cond_exp_mvp_path)\n",
    "conditional_expected_values_mvp = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(conditional_expected_values_mvp)\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import Imputer, StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler, SQLTransformer\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3baab45-a597-4199-8348-4da56ce3d573",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ab4bb39-3df0-4bb5-af36-a546e8f81bcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "categorical_features = [\n",
    "    # 'day_of_week',\n",
    "    # 'op_carrier',\n",
    "    # 'origin',\n",
    "\n",
    "    # Binary flag: True when required_time > rotation_time (impossible to depart on time)\n",
    "    # Non-linear indicator that captures \"impossible on-time\" scenarios\n",
    "    # If True: Not enough time between previous departure and scheduled departure\n",
    "    # Always data leakage-free\n",
    "    'safe_impossible_on_time_flag',\n",
    "]\n",
    "\n",
    "# Raw numerical features (pared back to test hypothesis)\n",
    "raw_numerical_features = [\n",
    "    # 'hourlyprecipitation',\n",
    "    # 'hourlysealevelpressure',\n",
    "    # 'hourlyaltimetersetting',\n",
    "    # 'hourlywetbulbtemperature',\n",
    "    # 'hourlystationpressure',\n",
    "    # 'crs_elapsed_time',\n",
    "    # 'distance',\n",
    "    # ============================================================================\n",
    "    # Flight Lineage Features (pre-applied in split.py)\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Cumulative count of recorded flights for this tail_num across all time in dataset\n",
    "    # Rank 1 = first recorded flight for this aircraft (plane already at airport, large buffer)\n",
    "    # Higher ranks = later flights in aircraft's history (may have accumulated delays from previous flights)\n",
    "    # Note: This is NOT per-day; it's cumulative across the entire dataset\n",
    "    'lineage_rank',\n",
    "    \n",
    "    # Safe rotation time: Time from previous departure to current scheduled departure\n",
    "    # Handles data leakage intelligently:\n",
    "    #   - If prev flight already departed: use actual departure time\n",
    "    #   - If prev flight hasn't departed yet: use scheduled time or \"right now\" (prediction cutoff)\n",
    "    # Always data leakage-free, safe for training\n",
    "    'safe_lineage_rotation_time_minutes',\n",
    "    \n",
    "    # Scheduled rotation time: Time from previous scheduled departure to current scheduled departure\n",
    "    # Entire scheduled sequence: prev_crs_dep → flight → arrival → turnover → curr_crs_dep\n",
    "    # Always data leakage-free (uses scheduled times only)\n",
    "    'scheduled_lineage_rotation_time_minutes',\n",
    "    \n",
    "    # Scheduled turnover time: Time from previous scheduled arrival to current scheduled departure\n",
    "    # Ground time between flights (component of rotation time)\n",
    "    # Rotation Time = Air Time + Turnover Time\n",
    "    # Always data leakage-free (uses scheduled times only)\n",
    "    'scheduled_lineage_turnover_time_minutes',\n",
    "    \n",
    "    # Previous flight's scheduled flight duration (air time)\n",
    "    # Computed as: prev_flight_crs_arr_time - prev_flight_crs_dep_time\n",
    "    # Hypothesis: When rotation_time >> scheduled_flight_time, we have more buffer = likely to depart on time\n",
    "    # Imputed to 0.0 for first flights or jumps\n",
    "    'prev_flight_scheduled_flight_time_minutes',\n",
    "    \n",
    "    # Required time: Expected air_time + expected_turnover_time (hypothesis feature)\n",
    "    # Computed as: prev_flight_crs_elapsed_time + scheduled_lineage_turnover_time_minutes\n",
    "    # Note: At flight lineage computation time (split.py), conditional expected values don't exist yet\n",
    "    # The function CAN use conditional expected values if available (from ConditionalExpectedValuesEstimator),\n",
    "    # but when computed during lineage feature engineering, it falls back to scheduled times\n",
    "    # Core hypothesis: departure_delay ≈ max(0, required_time - rotation_time)\n",
    "    # Always data leakage-free (uses scheduled times which are known in advance)\n",
    "    'safe_required_time_prev_flight_minutes',\n",
    "        \n",
    "    # Current flight's scheduled elapsed time (air time)\n",
    "    # Helps capture cascading delay effects - longer flights may have more variability\n",
    "    'crs_elapsed_time',\n",
    "    # ============================================================================\n",
    "    # Conditional Expected Values (added by ConditionalExpectedValuesMVPEstimator)\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Expected air time: Average historical air time for route (origin → dest)\n",
    "    # Non-temporal baseline (all-time average)\n",
    "    # Captures route-specific characteristics (distance, typical weather patterns)\n",
    "    'expected_air_time_route_minutes',\n",
    "    \n",
    "    # Expected air time: Average historical air time for route × month\n",
    "    # Temporal baseline (monthly average)\n",
    "    # Captures seasonal effects (jet streams, weather patterns vary by month)\n",
    "    # Falls back to non-temporal route average if month data insufficient\n",
    "    'expected_air_time_route_month_minutes',\n",
    "    \n",
    "    # Expected turnover time: Average historical turnover time for carrier × airport\n",
    "    # Non-temporal baseline (all-time average)\n",
    "    # Captures carrier-specific operational efficiency at specific airports\n",
    "    'expected_turnover_time_carrier_airport_minutes',\n",
    "    \n",
    "    # Expected turnover time: Average historical turnover time for carrier × airport × month\n",
    "    # Temporal baseline (monthly average)\n",
    "    # Captures seasonal operational patterns (holidays, weather, seasonal demand)\n",
    "    # Falls back to carrier-airport → airport-month → airport → 0.0\n",
    "    'expected_turnover_time_carrier_airport_month_minutes',\n",
    "    \n",
    "    # Expected turnover time: Average historical turnover time for airport (all carriers)\n",
    "    # Non-temporal baseline, airport-level fallback\n",
    "    # Used when carrier-airport data is sparse\n",
    "    'expected_turnover_time_airport_minutes',\n",
    "    \n",
    "    # Expected turnover time: Average historical turnover time for airport × month\n",
    "    # Temporal baseline, airport-level fallback with seasonality\n",
    "    # Used when carrier-airport-month data is sparse\n",
    "    'expected_turnover_time_airport_month_minutes',\n",
    "]\n",
    "\n",
    "\n",
    "numerical_features = raw_numerical_features\n",
    "\n",
    "print(f\"Using {len(numerical_features)} numerical features:\")\n",
    "for i, feat in enumerate(numerical_features, 1):\n",
    "    print(f\"  {i}. {feat}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5426543a-0721-496b-8131-184bd2de4608",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Construct Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21d18157-6e3f-457c-bffb-4de98ff99468",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Conditional Expected Values MVP Estimator\n",
    "# (Computes conditional means on-the-fly from training data)\n",
    "# Note: Flight lineage features are already in the folds (applied by split.py),\n",
    "# so turnover time columns are available for conditional expected values computation\n",
    "cond_exp_estimator = conditional_expected_values_mvp.ConditionalExpectedValuesMVPEstimator()\n",
    "\n",
    "# Step 2: Imputer\n",
    "imputer = Imputer(\n",
    "    inputCols=numerical_features,\n",
    "    outputCols=[f\"{col}_IMPUTED\" for col in numerical_features],\n",
    "    strategy=\"mean\"\n",
    ")\n",
    "\n",
    "# Step 3: Vector Assembler (no categorical features, so skip indexer/encoder)\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[f\"{col}_IMPUTED\" for col in numerical_features],\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "# Step 4: Standard Scaler\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"scaled_features\",\n",
    "    withMean=True,\n",
    "    withStd=True\n",
    ")\n",
    "\n",
    "# Step 5: Log transform DEP_DELAY\n",
    "# This preserves the sign while applying log transform\n",
    "# SIGN(DEP_DELAY) * LOG(ABS(DEP_DELAY) + 1.0) handles both positive and negative delays\n",
    "# Note: Filters out NULL DEP_DELAY values (required for LinearRegression)\n",
    "log_transform = SQLTransformer(\n",
    "    statement=\"\"\"\n",
    "    SELECT *, \n",
    "           SIGN(DEP_DELAY) * LOG(ABS(DEP_DELAY) + 1.0) AS DEP_DELAY_log\n",
    "    FROM __THIS__\n",
    "    WHERE DEP_DELAY IS NOT NULL\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Step 6: Linear Regression (train on log-transformed label)\n",
    "lr = LinearRegression(\n",
    "    featuresCol=\"scaled_features\", \n",
    "    labelCol=\"DEP_DELAY_log\",  # Use log-transformed target\n",
    "    predictionCol=\"prediction_log\",  # Predictions in log space\n",
    "    elasticNetParam=0.0,\n",
    ")\n",
    "\n",
    "# Step 7: Inverse log transform (convert predictions back from log space)\n",
    "# Inverse of SIGN(y) * LOG(ABS(y) + 1) is SIGN(y) * (EXP(ABS(y)) - 1)\n",
    "# Also floor to 0 to match hypothesis: departure_delay ≈ max(0, required_time - rotation_time)\n",
    "exp_transform = SQLTransformer(\n",
    "    statement=\"\"\"\n",
    "    SELECT *, \n",
    "           GREATEST(\n",
    "               CASE \n",
    "                   WHEN prediction_log IS NOT NULL THEN \n",
    "                       SIGN(prediction_log) * (EXP(ABS(prediction_log)) - 1.0)\n",
    "                   ELSE 0.0\n",
    "               END,\n",
    "               0.0\n",
    "           ) AS prediction\n",
    "    FROM __THIS__\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Complete Pipeline\n",
    "lr_pipe = Pipeline(stages=[\n",
    "    cond_exp_estimator,           # Adds conditional expected values (uses pre-existing flight lineage features)\n",
    "    imputer,                      # Imputes missing values\n",
    "    assembler,                    # Assembles feature vector\n",
    "    scaler,                       # Standardizes features\n",
    "    log_transform,                # Log transform DEP_DELAY (SIGN * LOG(ABS + 1))\n",
    "    lr,                           # Linear regression model (trained on log-transformed label)\n",
    "    exp_transform                 # Inverse log transform and floor to 0\n",
    "])\n",
    "\n",
    "print(\"Pipeline configured with:\")\n",
    "print(\"  - Log transform: SIGN(DEP_DELAY) * LOG(ABS(DEP_DELAY) + 1.0)\")\n",
    "print(\"  - Inverse transform: SIGN(pred) * (EXP(ABS(pred)) - 1), floored to 0\")\n",
    "print(\"  - Handles both positive and negative delays while preserving sign\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f858eb74-fef3-449e-a43b-53a9233b61b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Run Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59f148f4-714c-4947-a78b-6697d7d93329",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize cross-validator\n",
    "# FlightDelayCV automatically sets version and fold_index on estimators that support it\n",
    "cv_obj = cv.FlightDelayCV(\n",
    "    estimator=lr_pipe,\n",
    "    version=\"60M\"\n",
    ")\n",
    "\n",
    "# Run cross-validation (fits on folds 0-2, excludes test fold)\n",
    "metrics_df = cv_obj.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b438d1a-665a-4300-8c47-939d7df3605d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CROSS-VALIDATION RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(metrics_df)\n",
    "\n",
    "# Print coefficients for each fold\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL COEFFICIENTS (by fold)\")\n",
    "print(\"=\" * 80)\n",
    "for i, model in enumerate(cv_obj.models):\n",
    "    print(f\"\\n--- Fold {i+1} ---\")\n",
    "    lr_model = model.stages[-2]  # Linear regression is second-to-last (exp_transform is last)\n",
    "    \n",
    "    # Get feature names\n",
    "    feature_names = numerical_features\n",
    "    \n",
    "    # Get coefficients\n",
    "    coefficients = lr_model.coefficients.toArray()\n",
    "    intercept = lr_model.intercept\n",
    "    \n",
    "    print(f\"Intercept: {intercept:.4f}\")\n",
    "    print(\"\\nCoefficients:\")\n",
    "    for name, coef in zip(feature_names, coefficients):\n",
    "        print(f\"  {name:50s}: {coef:10.4f}\")\n",
    "    \n",
    "    # Print model summary stats\n",
    "    print(f\"\\nModel Summary:\")\n",
    "    print(f\"  R²: {lr_model.summary.r2:.4f}\")\n",
    "    print(f\"  RMSE: {lr_model.summary.rootMeanSquaredError:.4f}\")\n",
    "    print(f\"  MSE: {lr_model.summary.meanSquaredError:.4f}\")\n",
    "\n",
    "# Analyze feature importance (absolute coefficient values)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FEATURE IMPORTANCE (Average |coefficient| across folds)\")\n",
    "print(\"=\" * 80)\n",
    "avg_coefficients = {}\n",
    "for i, model in enumerate(cv_obj.models):\n",
    "    lr_model = model.stages[-2]  # Linear regression is second-to-last (exp_transform is last)\n",
    "    coefficients = lr_model.coefficients.toArray()\n",
    "    for name, coef in zip(numerical_features, coefficients):\n",
    "        if name not in avg_coefficients:\n",
    "            avg_coefficients[name] = []\n",
    "        avg_coefficients[name].append(abs(coef))\n",
    "\n",
    "for name in sorted(avg_coefficients.keys(), key=lambda x: sum(avg_coefficients[x])/len(avg_coefficients[x]), reverse=True):\n",
    "    avg_abs_coef = sum(avg_coefficients[name]) / len(avg_coefficients[name])\n",
    "    print(f\"  {name:50s}: {avg_abs_coef:10.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "599972e6-9d30-495d-a3b0-bbf23ff26faa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5f215a9-2c73-4cf8-9cc7-634537071a11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate on held-out test fold\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "test_results = cv_obj.evaluate()\n",
    "print(test_results)\n",
    "\n",
    "# Print test model coefficients\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEST MODEL COEFFICIENTS\")\n",
    "print(\"=\" * 80)\n",
    "test_lr_model = cv_obj.test_model.stages[-1]\n",
    "test_coefficients = test_lr_model.coefficients.toArray()\n",
    "test_intercept = test_lr_model.intercept\n",
    "\n",
    "print(f\"Intercept: {test_intercept:.4f}\")\n",
    "print(\"\\nCoefficients:\")\n",
    "for name, coef in zip(numerical_features, test_coefficients):\n",
    "    print(f\"  {name:50s}: {coef:10.4f}\")\n",
    "\n",
    "print(f\"\\nTest Model Summary:\")\n",
    "print(f\"  R²: {test_lr_model.summary.r2:.4f}\")\n",
    "print(f\"  RMSE: {test_lr_model.summary.rootMeanSquaredError:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25bcbcfc-06fd-4b13-9e9f-e43080544937",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48c573c8-2622-4b49-88f2-92e1c5679163",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sanity check for flight lineage features\n",
    "print(\"=\" * 80)\n",
    "print(\"SANITY CHECK: FLIGHT LINEAGE FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Statistics for lineage features\n",
    "lineage_features = [\n",
    "    'lineage_rank', \n",
    "    'safe_lineage_rotation_time_minutes', \n",
    "    'scheduled_lineage_rotation_time_minutes', \n",
    "    'scheduled_lineage_turnover_time_minutes',\n",
    "    'prev_flight_scheduled_flight_time_minutes',\n",
    "    'safe_required_time_prev_flight_minutes',\n",
    "    'safe_impossible_on_time_flag',\n",
    "    'crs_elapsed_time'\n",
    "]\n",
    "\n",
    "for col_name in lineage_features:\n",
    "    if col_name in train_transformed.columns:\n",
    "        print(f\"\\n{col_name}:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Basic statistics\n",
    "        stats = train_transformed.select(\n",
    "            F.avg(col_name).alias('mean'),\n",
    "            F.min(col_name).alias('min'),\n",
    "            F.max(col_name).alias('max'),\n",
    "            F.stddev(col_name).alias('std'),\n",
    "            F.count(col_name).alias('count'),\n",
    "            F.percentile_approx(col_name, 0.5).alias('median'),\n",
    "            F.percentile_approx(col_name, 0.25).alias('q25'),\n",
    "            F.percentile_approx(col_name, 0.75).alias('q75')\n",
    "        ).collect()[0]\n",
    "        \n",
    "        total_count = stats['count']\n",
    "        print(f\"  Count:   {total_count:,}\")\n",
    "        print(f\"  Mean:    {stats['mean']:.4f}\")\n",
    "        print(f\"  Median:  {stats['median']:.4f if stats['median'] else 'N/A'}\")\n",
    "        print(f\"  Q25:     {stats['q25']:.4f if stats['q25'] else 'N/A'}\")\n",
    "        print(f\"  Q75:     {stats['q75']:.4f if stats['q75'] else 'N/A'}\")\n",
    "        std_val = f\"{stats['std']:.4f}\" if stats['std'] is not None else \"N/A\"\n",
    "        print(f\"  Std:     {std_val}\")\n",
    "        min_val = f\"{stats['min']:.4f}\" if stats['min'] is not None else \"N/A\"\n",
    "        max_val = f\"{stats['max']:.4f}\" if stats['max'] is not None else \"N/A\"\n",
    "        print(f\"  Min:     {min_val}\")\n",
    "        print(f\"  Max:     {max_val}\")\n",
    "        \n",
    "        # Null and zero counts\n",
    "        null_count = train_transformed.filter(F.col(col_name).isNull()).count()\n",
    "        zero_count = train_transformed.filter(F.col(col_name) == 0.0).count()\n",
    "        pct_null = (null_count / total_count * 100) if total_count > 0 else 0\n",
    "        pct_zero = (zero_count / total_count * 100) if total_count > 0 else 0\n",
    "        print(f\"  Nulls:   {null_count:,} ({pct_null:.2f}%)\")\n",
    "        print(f\"  Zeros:   {zero_count:,} ({pct_zero:.2f}%)\")\n",
    "        \n",
    "        # Correlation with DEP_DELAY\n",
    "        corr = train_transformed.select(\n",
    "            F.corr(col_name, 'DEP_DELAY').alias('corr')\n",
    "        ).collect()[0]['corr']\n",
    "        print(f\"  Corr with DEP_DELAY: {corr:.4f}\" if corr else \"  Corr with DEP_DELAY: N/A\")\n",
    "\n",
    "# Check relationship between safe and scheduled rotation time\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAFE vs SCHEDULED ROTATION TIME COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'safe_lineage_rotation_time_minutes' in train_transformed.columns and 'scheduled_lineage_rotation_time_minutes' in train_transformed.columns:\n",
    "    comparison = train_transformed.select(\n",
    "        F.col('safe_lineage_rotation_time_minutes').alias('safe'),\n",
    "        F.col('scheduled_lineage_rotation_time_minutes').alias('scheduled')\n",
    "    )\n",
    "    \n",
    "    # Where they differ (data leakage detected)\n",
    "    different = comparison.filter(\n",
    "        F.col('safe') != F.col('scheduled')\n",
    "    ).count()\n",
    "    \n",
    "    # Where safe is less than scheduled (imputed due to data leakage)\n",
    "    safe_less = comparison.filter(\n",
    "        F.col('safe') < F.col('scheduled')\n",
    "    ).count()\n",
    "    \n",
    "    total = train_transformed.count()\n",
    "    same_count = total - different\n",
    "    pct_different = (different / total * 100) if total > 0 else 0\n",
    "    pct_safe_less = (safe_less / total * 100) if total > 0 else 0\n",
    "    \n",
    "    print(f\"\\nTotal rows: {total:,}\")\n",
    "    print(f\"Rows where safe == scheduled: {same_count:,} ({100-pct_different:.2f}%)\")\n",
    "    print(f\"Rows where safe != scheduled: {different:,} ({pct_different:.2f}%)\")\n",
    "    print(f\"Rows where safe < scheduled (data leakage imputed): {safe_less:,} ({pct_safe_less:.2f}%)\")\n",
    "    \n",
    "    # Sample rows where they differ\n",
    "    if different > 0:\n",
    "        sample_diff = train_transformed.filter(\n",
    "            F.col('safe_lineage_rotation_time_minutes') != F.col('scheduled_lineage_rotation_time_minutes')\n",
    "        ).select(\n",
    "            'FL_DATE', 'origin', 'dest',\n",
    "            'lineage_rank',\n",
    "            'safe_lineage_rotation_time_minutes',\n",
    "            'scheduled_lineage_rotation_time_minutes'\n",
    "        ).limit(10).toPandas()\n",
    "        print(f\"\\nSample rows where safe != scheduled (data leakage detected):\")\n",
    "        print(sample_diff.to_string())\n",
    "\n",
    "# Check lineage_rank distribution\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"LINEAGE RANK DISTRIBUTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'lineage_rank' in train_transformed.columns:\n",
    "    rank_dist = train_transformed.groupBy('lineage_rank').agg(\n",
    "        F.count('*').alias('count')\n",
    "    ).orderBy('lineage_rank').limit(20).toPandas()\n",
    "    \n",
    "    total = train_transformed.count()\n",
    "    print(f\"\\nDistribution of lineage_rank (first 20 ranks):\")\n",
    "    print(\"-\" * 80)\n",
    "    for _, row in rank_dist.iterrows():\n",
    "        pct = (row['count'] / total * 100) if total > 0 else 0\n",
    "        print(f\"  Rank {int(row['lineage_rank']):3d}: {row['count']:8,} flights ({pct:5.2f}%)\")\n",
    "    \n",
    "    # Check if rank=1 flights have rotation_time=0 (first flight in lineage)\n",
    "    rank_1_rotation = train_transformed.filter(F.col('lineage_rank') == 1.0).select(\n",
    "        F.avg('safe_lineage_rotation_time_minutes').alias('avg_rotation_rank1'),\n",
    "        F.avg('scheduled_lineage_rotation_time_minutes').alias('avg_scheduled_rank1')\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(f\"\\nRank 1 (first flight) statistics:\")\n",
    "    print(f\"  Avg safe rotation time: {rank_1_rotation['avg_rotation_rank1']:.4f}\" if rank_1_rotation['avg_rotation_rank1'] else \"  Avg safe rotation time: N/A\")\n",
    "    print(f\"  Avg scheduled rotation time: {rank_1_rotation['avg_scheduled_rank1']:.4f}\" if rank_1_rotation['avg_scheduled_rank1'] else \"  Avg scheduled rotation time: N/A\")\n",
    "    print(f\"  (Should be ~1440 minutes (24 hours) for first flight - indicates plane already at airport, plenty of buffer)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94131f54-c2bc-4b90-865e-e02de143721a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Demo: Expected Flight Duration Features",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
