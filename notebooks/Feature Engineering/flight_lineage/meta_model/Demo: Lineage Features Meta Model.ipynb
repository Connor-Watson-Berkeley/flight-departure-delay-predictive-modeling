{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "932c2b38-726f-419d-b476-b816f8064718",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Demo: Phase 2 Meta-Model for Departure Delay Prediction\n",
    "\n",
    "This demo demonstrates Phase 2 of the flight delay prediction approach using **meta-models** to predict previous flight components.\n",
    "\n",
    "**Key Features:**\n",
    "- **Meta-Models**: Random Forest models predict previous flight components:\n",
    "  - `predicted_prev_flight_air_time`: Predicted actual air time for previous flight\n",
    "  - `predicted_prev_flight_taxi_time`: Predicted actual taxi time for previous flight\n",
    "  - `predicted_prev_flight_total_duration`: Predicted actual total duration for previous flight\n",
    "- **Final Model**: Random Forest using meta-model predictions + comprehensive covariates\n",
    "- Cross-validation with automatic fold handling\n",
    "\n",
    "**Meta-Model Approach:**\n",
    "- Train Random Forest models on training folds to predict previous flight components\n",
    "- Use comprehensive covariates: weather, temporal features, carrier, state-level location\n",
    "- CV-safe: Meta-models train only on training data, applied to validation/test\n",
    "- **These predictions ARE the conditional expected values** for previous flight components\n",
    "\n",
    "**Meta-Model Covariates:**\n",
    "- Weather variables at previous flight origin and destination (comprehensive set: precipitation, pressure, wind, temperature, visibility, etc.)\n",
    "- Temporal features (month, day_of_week, day_of_month, time blocks)\n",
    "- Route characteristics (previous flight route, scheduled times, distance)\n",
    "- Carrier and state-level location\n",
    "- Airport characteristics (elevation if available)\n",
    "\n",
    "**Final Model Features:**\n",
    "- Flight lineage features (rotation time, turnover time, etc.)\n",
    "- Meta-model predictions (predicted previous flight components - these ARE the conditional expected values)\n",
    "- Weather variables (current origin)\n",
    "- Temporal features (month, day_of_week, day_of_month, dep_time_blk, arr_time_blk) - aligned with MLP PyTorch notebook\n",
    "- Flight characteristics (distance, scheduled times, elevation)\n",
    "\n",
    "**Log Transform:**\n",
    "- Applied to `DEP_DELAY` to handle right-skewed distribution: `SIGN(DEP_DELAY) * LOG(ABS(DEP_DELAY) + 1.0)`\n",
    "- Inverse transform: `SIGN(pred) * (EXP(ABS(pred)) - 1)`, floored to 0\n",
    "- Similar to MVP demo approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a455c86-c6bd-4052-b4ec-b8aa49008196",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f9f0e7b-4716-4795-a36a-a1ca8d343e02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# Load modules from our Databricks repo\n",
    "import importlib.util\n",
    "\n",
    "# CV module\n",
    "cv_path = \"/Workspace/Shared/Team 4_2/flight-departure-delay-predictive-modeling/notebooks/Cross Validator/cv.py\"\n",
    "spec = importlib.util.spec_from_file_location(\"cv\", cv_path)\n",
    "cv = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(cv)\n",
    "\n",
    "# Meta-model estimator\n",
    "meta_model_path = \"/Workspace/Shared/Team 4_2/flight-departure-delay-predictive-modeling/notebooks/Feature Engineering/meta_model_estimator.py\"\n",
    "spec = importlib.util.spec_from_file_location(\"meta_model_estimator\", meta_model_path)\n",
    "meta_model_estimator = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(meta_model_estimator)\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import Imputer, StringIndexer, OneHotEncoder, VectorAssembler, SQLTransformer\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml import Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63682c59-7378-466a-9a5d-07a5a0c8a685",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define Features\n",
    "\n",
    "**Categorical Features**: State-level location, carrier, temporal features (avoiding high-cardinality airport codes)\n",
    "\n",
    "**Numerical Features**: \n",
    "- Flight lineage features (rotation time, turnover time, etc.)\n",
    "- Meta-model predictions (predicted previous flight components - these ARE the conditional expected values)\n",
    "- Weather variables (current origin)\n",
    "- Flight characteristics (distance, scheduled times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f93b60be-48fd-4d93-b3e5-eabb899ba511",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Categorical features (state-level to avoid high cardinality)\n",
    "# Note: Avoiding origin/dest directly as they have 200+ categories\n",
    "# Based on MLP PyTorch notebook and other models\n",
    "categorical_features = [\n",
    "    'op_carrier',              # Carrier\n",
    "    # 'origin_state_abr',        # Origin state (lower cardinality than airport)\n",
    "    # 'dest_state_abr',          # Destination state\n",
    "    'day_of_week',             # Day of week\n",
    "    'month',                   # Month\n",
    "    # 'day_of_month',            # Day of month (from MLP notebook)\n",
    "    'dep_time_blk',            # Departure time block\n",
    "    'arr_time_blk',            # Arrival time block (from MLP notebook)\n",
    "]\n",
    "\n",
    "# Numerical features\n",
    "raw_numerical_features = [\n",
    "    # ============================================================================\n",
    "    # Flight Lineage Features (pre-applied in split.py)\n",
    "    # ============================================================================\n",
    "    'lineage_rank',\n",
    "    'safe_lineage_rotation_time_minutes',\n",
    "    'scheduled_lineage_rotation_time_minutes',\n",
    "    'scheduled_lineage_turnover_time_minutes',\n",
    "    'prev_flight_scheduled_flight_time_minutes',\n",
    "    'safe_required_time_prev_flight_minutes',\n",
    "        \n",
    "    # ============================================================================\n",
    "    # Note: Meta-model predictions (predicted_prev_flight_*) are added by MetaModelEstimator\n",
    "    # and do NOT need imputation (they're already predicted values)\n",
    "    # They will be added to final_model_features after meta-models run\n",
    "    # ============================================================================\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Weather Variables (Current Origin)\n",
    "    # ============================================================================\n",
    "    'hourlyprecipitation',\n",
    "    'hourlysealevelpressure',\n",
    "    'hourlyaltimetersetting',\n",
    "    'hourlywetbulbtemperature',\n",
    "    'hourlystationpressure',\n",
    "    'hourlywinddirection',\n",
    "    'hourlyrelativehumidity',\n",
    "    'hourlywindspeed',\n",
    "    'hourlydewpointtemperature',\n",
    "    'hourlydrybulbtemperature',\n",
    "    'hourlyvisibility',\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Flight Characteristics\n",
    "    # ============================================================================\n",
    "    'crs_elapsed_time',        # Scheduled elapsed time\n",
    "    'distance',                # Flight distance\n",
    "    'elevation',               # Airport elevation (if available)\n",
    "]\n",
    "\n",
    "numerical_features = raw_numerical_features\n",
    "\n",
    "print(f\"Using {len(categorical_features)} categorical features:\")\n",
    "for i, feat in enumerate(categorical_features, 1):\n",
    "    print(f\"  {i}. {feat}\")\n",
    "\n",
    "print(f\"\\nUsing {len(numerical_features)} numerical features:\")\n",
    "for i, feat in enumerate(numerical_features, 1):\n",
    "    print(f\"  {i}. {feat}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d247822d-d550-4bf3-90a4-9fd7f772597a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Construct Model Pipeline\n",
    "\n",
    "Pipeline stages:\n",
    "1. **Shared Feature Processing**: Imputer + StringIndexer + OneHotEncoder (processes ALL features once)\n",
    "2. **Meta-Model Estimator** (Uses preprocessed features to predict previous flight components)\n",
    "3. **Final Model Feature Assembly**: VectorAssembler (combines processed features + meta-model predictions)\n",
    "4. **Random Forest Regressor** (Final model)\n",
    "\n",
    "**Note**: Feature processing runs FIRST so both meta-models and final model use consistently encoded features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3e2c81b-f816-46b9-90bb-d6bcaf231c1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: Shared Feature Processing (runs FIRST for consistency)\n",
    "# ============================================================================\n",
    "# Process ALL features upfront so meta-models and final model use consistently encoded features\n",
    "\n",
    "# Get ALL features (current flight + previous flight) for processing\n",
    "# Meta-models use prev_flight_* features, final model uses current flight features\n",
    "all_categorical_features = categorical_features.copy()\n",
    "all_numerical_features = numerical_features.copy()\n",
    "\n",
    "# Add previous flight features that meta-models need (but final model doesn't use directly)\n",
    "meta_model_categorical = [\n",
    "    'prev_flight_op_carrier',\n",
    "    'prev_flight_origin_state_abr',\n",
    "    'prev_flight_dest_state_abr',\n",
    "    'prev_flight_day_of_week',\n",
    "    'prev_flight_month',\n",
    "    'prev_flight_dep_time_blk',  # For taxi time model\n",
    "    'prev_flight_arr_time_blk',  # For taxi time model\n",
    "]\n",
    "\n",
    "meta_model_numerical = [\n",
    "    'prev_flight_crs_elapsed_time',\n",
    "    'prev_flight_distance',\n",
    "    'prev_flight_crs_dep_time',\n",
    "    'prev_flight_crs_arr_time',\n",
    "    'prev_flight_day_of_month',\n",
    "    # Weather at previous origin/dest (add if available)\n",
    "    'prev_flight_origin_hourlyprecipitation',\n",
    "    'prev_flight_origin_hourlysealevelpressure',\n",
    "    'prev_flight_origin_hourlyaltimetersetting',\n",
    "    'prev_flight_origin_hourlywetbulbtemperature',\n",
    "    'prev_flight_origin_hourlystationpressure',\n",
    "    'prev_flight_origin_hourlywinddirection',\n",
    "    'prev_flight_origin_hourlyrelativehumidity',\n",
    "    'prev_flight_origin_hourlywindspeed',\n",
    "    'prev_flight_origin_hourlydewpointtemperature',\n",
    "    'prev_flight_origin_hourlydrybulbtemperature',\n",
    "    'prev_flight_origin_hourlyvisibility',\n",
    "    'prev_flight_dest_hourlyprecipitation',\n",
    "    'prev_flight_dest_hourlysealevelpressure',\n",
    "    'prev_flight_dest_hourlyaltimetersetting',\n",
    "    'prev_flight_dest_hourlywetbulbtemperature',\n",
    "    'prev_flight_dest_hourlystationpressure',\n",
    "    'prev_flight_dest_hourlywinddirection',\n",
    "    'prev_flight_dest_hourlyrelativehumidity',\n",
    "    'prev_flight_dest_hourlywindspeed',\n",
    "    'prev_flight_dest_hourlydewpointtemperature',\n",
    "    'prev_flight_dest_hourlydrybulbtemperature',\n",
    "    'prev_flight_dest_hourlyvisibility',\n",
    "]\n",
    "\n",
    "# Combine all features for processing\n",
    "all_categorical_features.extend([f for f in meta_model_categorical if f not in all_categorical_features])\n",
    "all_numerical_features.extend([f for f in meta_model_numerical if f not in all_numerical_features])\n",
    "\n",
    "# Note: We'll filter to only existing features when we actually build the pipeline\n",
    "# For now, this defines the complete feature set we want to process\n",
    "print(f\"Target feature set: {len(all_categorical_features)} categorical and {len(all_numerical_features)} numerical features\")\n",
    "print(\"(Actual features used will be filtered to only those available in the data)\")\n",
    "\n",
    "# Note: Feature lists will be filtered to available features in the next cell\n",
    "# Step 1a: Imputer (all numerical features)\n",
    "# Will be updated after we check available features\n",
    "imputer = None  # Will be set after feature filtering\n",
    "\n",
    "# Step 1b: StringIndexer (all categorical features)  \n",
    "indexer = None  # Will be set after feature filtering\n",
    "\n",
    "# Step 1c: OneHotEncoder (all categorical features)\n",
    "encoder = None  # Will be set after feature filtering\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Meta-Model Estimator (uses preprocessed features)\n",
    "# ============================================================================\n",
    "# Trains Random Forest models to predict previous flight components\n",
    "# Note: use_preprocessed_features=True so it uses already-processed features\n",
    "meta_model_est = meta_model_estimator.MetaModelEstimator(\n",
    "    num_trees=50,\n",
    "    max_depth=20,  # Increased for high-cardinality categoricals (3000+ routes)\n",
    "    # Random Forest can handle high cardinality, but needs depth to capture route-specific patterns\n",
    "    # Depth 20 balances: enough depth for route-specific splits vs overfitting risk\n",
    "    min_instances_per_node=20,  # Increased to prevent overfitting with deeper trees\n",
    "    use_preprocessed_features=True  # Use features processed in Step 1\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: Final Model Feature Assembly\n",
    "# ============================================================================\n",
    "# Assemble features for final model: processed current flight features + meta-model predictions\n",
    "# Note: final_model_categorical and final_model_numerical will be defined after feature filtering\n",
    "assembler = None  # Will be set after feature filtering\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: Log Transform (for skewed target distribution)\n",
    "# ============================================================================\n",
    "# Log transform DEP_DELAY to handle right-skewed distribution\n",
    "# SIGN(DEP_DELAY) * LOG(ABS(DEP_DELAY) + 1.0) preserves sign for negative delays\n",
    "log_transform = SQLTransformer(\n",
    "    statement=\"\"\"\n",
    "    SELECT *, \n",
    "           SIGN(DEP_DELAY) * LOG(ABS(DEP_DELAY) + 1.0) AS DEP_DELAY_log\n",
    "    FROM __THIS__\n",
    "    WHERE DEP_DELAY IS NOT NULL\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: Final Model\n",
    "# ============================================================================\n",
    "# Random Forest Regressor (final model)\n",
    "# Note: No StandardScaler needed for tree-based models\n",
    "# Training on log-transformed target to handle skewed distribution\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"DEP_DELAY_log\",  # Use log-transformed target\n",
    "    predictionCol=\"prediction_log\",  # Predictions in log space\n",
    "    numTrees=50,  # Reduced from 100 to save memory (60M rows is large)\n",
    "    maxDepth=12,  # Reduced from 15 to save memory and reduce overfitting\n",
    "    minInstancesPerNode=50,  # Increased from 20 to reduce tree size and memory usage\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: Inverse Log Transform\n",
    "# ============================================================================\n",
    "# Convert predictions back from log space\n",
    "# Inverse of SIGN(y) * LOG(ABS(y) + 1) is SIGN(y) * (EXP(ABS(y)) - 1)\n",
    "# Floor to 0 to match hypothesis: departure_delay ≈ max(0, required_time - rotation_time)\n",
    "exp_transform = SQLTransformer(\n",
    "    statement=\"\"\"\n",
    "    SELECT *, \n",
    "           GREATEST(\n",
    "               CASE \n",
    "                   WHEN prediction_log IS NOT NULL THEN \n",
    "                       SIGN(prediction_log) * (EXP(ABS(prediction_log)) - 1.0)\n",
    "                   ELSE 0.0\n",
    "               END,\n",
    "               0.0\n",
    "           ) AS prediction\n",
    "    FROM __THIS__\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Complete Pipeline will be built after feature filtering (in next cell)\n",
    "# Order matters: Feature processing FIRST, then meta-models (which use processed features), then final model\n",
    "rf_pipe = None  # Will be built after feature filtering\n",
    "\n",
    "print(\"Pipeline configured with:\")\n",
    "print(\"  - Step 1: Shared Feature Processing (Imputer + Indexer + Encoder)\")\n",
    "print(\"    → Processes ALL features once for consistent encoding\")\n",
    "print(\"  - Step 2: Meta-Model Estimator (uses preprocessed features)\")\n",
    "print(\"    → Predicts: air_time, taxi_time, total_duration (these ARE the conditional expected values)\")\n",
    "print(\"  - Step 3: Final Model Assembly (processed current flight features + meta-model predictions)\")\n",
    "print(\"  - Step 4: Log Transform (SIGN(DEP_DELAY) * LOG(ABS(DEP_DELAY) + 1.0))\")\n",
    "print(\"    → Handles right-skewed distribution, preserves sign for negative delays\")\n",
    "print(\"  - Step 5: Random Forest Regressor (trained on log-transformed target)\")\n",
    "print(\"  - Step 6: Inverse Log Transform (convert back and floor to 0)\")\n",
    "print(\"\\nBenefits:\")\n",
    "print(\"  ✓ Consistent feature encoding across meta-models and final model\")\n",
    "print(\"  ✓ No duplicate processing\")\n",
    "print(\"  ✓ Same categories get same indices/encoding\")\n",
    "print(\"  ✓ Log transform handles skewed delay distribution\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24bdeacf-78c7-473c-bfbd-53b1ab5baff9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Dynamic Feature Filtering\n",
    "\n",
    "Before building the pipeline, we need to filter features to only those that exist in the data. This ensures we don't try to process columns that don't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c79a1c1b-a2eb-436d-a304-9d38642c50e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load a sample of data to check which features are available\n",
    "# (We'll do this dynamically in the pipeline, but for documentation purposes)\n",
    "data_loader = cv.FlightDelayDataLoader()\n",
    "data_loader.load()\n",
    "folds = data_loader.get_version(\"60M\")\n",
    "sample_train_df, _ = folds[0]\n",
    "\n",
    "# Filter to only features that exist in the data\n",
    "available_categorical = [f for f in all_categorical_features if f in sample_train_df.columns]\n",
    "available_numerical = [f for f in all_numerical_features if f in sample_train_df.columns]\n",
    "\n",
    "print(f\"Available categorical features: {len(available_categorical)}/{len(all_categorical_features)}\")\n",
    "print(f\"Available numerical features: {len(available_numerical)}/{len(all_numerical_features)}\")\n",
    "\n",
    "# Update feature lists to only include available features\n",
    "all_categorical_features = available_categorical\n",
    "all_numerical_features = available_numerical\n",
    "\n",
    "# Also update final model features to only include what's available\n",
    "final_model_categorical = [f for f in categorical_features if f in sample_train_df.columns]\n",
    "final_model_numerical = [f for f in numerical_features if f in sample_train_df.columns]\n",
    "\n",
    "# Now build the final model assembler with filtered features\n",
    "final_model_features = (\n",
    "    [f\"{col}_VEC\" for col in final_model_categorical] + \n",
    "    [f\"{col}_IMPUTED\" for col in final_model_numerical] +\n",
    "    ['predicted_prev_flight_air_time', 'predicted_prev_flight_taxi_time', 'predicted_prev_flight_total_duration']\n",
    ")\n",
    "\n",
    "# Now create the feature processing stages with filtered features\n",
    "# Step 1a: Imputer (all numerical features)\n",
    "imputer = Imputer(\n",
    "    inputCols=all_numerical_features,\n",
    "    outputCols=[f\"{col}_IMPUTED\" for col in all_numerical_features],\n",
    "    strategy=\"mean\"\n",
    ") if all_numerical_features else None\n",
    "\n",
    "# Step 1b: StringIndexer (all categorical features)\n",
    "# StringIndexer can handle boolean columns, but we need to ensure NULLs are handled\n",
    "# The flight_lineage_features.py already imputes safe_impossible_on_time_flag to False\n",
    "# handleInvalid=\"keep\" will assign a new index for NULL/invalid values\n",
    "indexer = StringIndexer(\n",
    "    inputCols=all_categorical_features,\n",
    "    outputCols=[f\"{col}_INDEX\" for col in all_categorical_features],\n",
    "    handleInvalid=\"keep\"  # Handles NULLs and unknown categories by assigning a new index\n",
    ") if all_categorical_features else None\n",
    "\n",
    "# Step 1c: OneHotEncoder (all categorical features)\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=[f\"{col}_INDEX\" for col in all_categorical_features],\n",
    "    outputCols=[f\"{col}_VEC\" for col in all_categorical_features],\n",
    "    dropLast=False  # Keep all categories for Random Forest\n",
    ")\n",
    "\n",
    "# Step 3: Final Model Feature Assembly\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=final_model_features,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "# Build the complete pipeline with filtered features\n",
    "# Filter out None stages (in case we have no numerical or categorical features)\n",
    "pipeline_stages = []\n",
    "if imputer is not None:\n",
    "    pipeline_stages.append(imputer)       # Step 1a: Impute ALL numerical features\n",
    "if indexer is not None:\n",
    "    pipeline_stages.append(indexer)       # Step 1b: Index ALL categorical features\n",
    "pipeline_stages.extend([\n",
    "    encoder,                   # Step 1c: One-hot encode ALL categorical features\n",
    "    meta_model_est,            # Step 2: Meta-models (use preprocessed features to predict prev flight components)\n",
    "    assembler,                 # Step 3: Assemble final model features (processed current flight + meta-model predictions)\n",
    "    log_transform,             # Step 4: Log transform DEP_DELAY (SIGN * LOG(ABS + 1))\n",
    "    rf,                        # Step 5: Random Forest (trained on log-transformed target)\n",
    "    exp_transform              # Step 6: Inverse log transform and floor to 0\n",
    "])\n",
    "\n",
    "rf_pipe = Pipeline(stages=pipeline_stages)\n",
    "\n",
    "print(f\"\\nFinal model will use:\")\n",
    "print(f\"  - {len(final_model_categorical)} categorical features (current flight)\")\n",
    "print(f\"  - {len(final_model_numerical)} numerical features (current flight)\")\n",
    "print(f\"  - 3 meta-model predictions (predicted_prev_flight_*)\")\n",
    "print(\"\\n✓ Pipeline built with filtered features!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0cbc50e-4242-4df1-bd11-beda32b3b952",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Run Cross-Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86c774a6-3857-4144-803b-45928df1a211",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize cross-validator\n",
    "# FlightDelayCV automatically sets version and fold_index on estimators that support it\n",
    "cv_rf_meta_12 = cv.FlightDelayCV(\n",
    "    estimator=rf_pipe,\n",
    "    version=\"12M\"  # Use 60M for better data coverage\n",
    ")\n",
    "\n",
    "# Run cross-validation (fits on folds 0-2, excludes test fold)\n",
    "print(\"=\" * 80)\n",
    "print(\"RUNNING CROSS-VALIDATION WITH RF & META-MODELS\")\n",
    "print(\"=\" * 80)\n",
    "metrics_df = cv_rf_meta_12.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2dfa936-0364-421d-bd24-1c50e833fe58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_rf_meta_12.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d57638e-5ebe-4efb-867c-391ca112364b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 60M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5810dd16-a8af-45d0-8cad-15508c94c805",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize cross-validator\n",
    "# FlightDelayCV automatically sets version and fold_index on estimators that support it\n",
    "cv_rf_meta_60 = cv.FlightDelayCV(\n",
    "    estimator=rf_pipe,\n",
    "    version=\"60M\"  # Use 60M for better data coverage\n",
    ")\n",
    "\n",
    "# Run cross-validation (fits on folds 0-2, excludes test fold)\n",
    "print(\"=\" * 80)\n",
    "print(\"RUNNING CROSS-VALIDATION WITH RF & META-MODELS\")\n",
    "print(\"=\" * 80)\n",
    "metrics_df = cv_rf_meta_60.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84432e97-5d41-4659-aa30-5679f44eeb95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_rf_meta_60.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c02e190-fefc-40e6-8e75-21b8da218c41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## View Cross-Validation Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc8acb1f-b69d-4c8a-9ed6-98f000bf4d23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CROSS-VALIDATION RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(metrics_df)\n",
    "\n",
    "# Print feature importance for each fold\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FEATURE IMPORTANCE (by fold)\")\n",
    "print(\"=\" * 80)\n",
    "for i, model in enumerate(cv_obj.models):\n",
    "    print(f\"\\n--- Fold {i+1} ---\")\n",
    "    # Random Forest is second-to-last (exp_transform is last)\n",
    "    rf_model = model.stages[-2]\n",
    "    feature_importance = rf_model.featureImportances.toArray()\n",
    "    \n",
    "    # Get feature names (use filtered lists from cell 8)\n",
    "    feature_names = [f\"{col}_VEC\" for col in final_model_categorical] + [f\"{col}_IMPUTED\" for col in final_model_numerical] + [\n",
    "        'predicted_prev_flight_air_time', 'predicted_prev_flight_taxi_time', 'predicted_prev_flight_total_duration'\n",
    "    ]\n",
    "    \n",
    "    # Sort by importance\n",
    "    importance_pairs = list(zip(feature_names, feature_importance))\n",
    "    importance_pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"Top 20 Features:\")\n",
    "    for name, importance in importance_pairs[:20]:\n",
    "        print(f\"  {name:50s}: {importance:10.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "316577d9-3cc0-42fc-8d69-5dc397efdcbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Evaluate on Test Set\n",
    "\n",
    "Evaluate the final model on the held-out test fold (fold 4). This gives us an unbiased estimate of model performance on unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49524e1b-2414-4471-9b74-05232b8e7afd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate on held-out test fold\n",
    "print(\"=\" * 80)\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_results = cv_obj.evaluate()\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(test_results)\n",
    "\n",
    "# Print test model feature importance\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEST MODEL FEATURE IMPORTANCE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_rf_model = cv_obj.test_model.stages[-2]  # Random Forest is second-to-last (exp_transform is last)\n",
    "feature_importance = test_rf_model.featureImportances.toArray()\n",
    "\n",
    "# Get feature names (use filtered lists from cell 8)\n",
    "feature_names = [f\"{col}_VEC\" for col in final_model_categorical] + [f\"{col}_IMPUTED\" for col in final_model_numerical] + [\n",
    "    'predicted_prev_flight_air_time', 'predicted_prev_flight_taxi_time', 'predicted_prev_flight_total_duration'\n",
    "]\n",
    "\n",
    "# Sort by importance\n",
    "importance_pairs = list(zip(feature_names, feature_importance))\n",
    "importance_pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nTop 20 Features (Test Model):\")\n",
    "for name, importance in importance_pairs[:20]:\n",
    "    print(f\"  {name:50s}: {importance:10.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Demo: Lineage Features Meta Model",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
