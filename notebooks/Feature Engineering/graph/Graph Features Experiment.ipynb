{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75767145-d4e5-4591-8f0e-047f271452a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dependencies to load moduels from this repo\n",
    "import importlib.util\n",
    "import sys\n",
    "\n",
    "# Load cv module directly from file path\n",
    "cv_path = \"/Workspace/Shared/Team 4_2/flight-departure-delay-predictive-modeling/notebooks/Cross Validator/cv.py\"\n",
    "spec = importlib.util.spec_from_file_location(\"cv\", cv_path)\n",
    "cv = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(cv)\n",
    "\n",
    "# Dependencies for graph features\n",
    "from graphframes import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Other Dependencies\n",
    "import time\n",
    "\n",
    "# Path for persistent storage\n",
    "FOLDER_PATH = \"dbfs:/mnt/mids-w261/student-groups/Group_4_2/experiments\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f43e098-fc0a-4988-8575-cc8bf1dc51d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Use Cross Validator Module to Generate Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63ce7618-70f8-4f37-8f14-f01d62b1452b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_loader = cv.FlightDelayDataLoader()\n",
    "data_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53cae630-54f0-4e92-a24c-cec732525c6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "folds = data_loader.get_version(\"3M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87da433e-8316-4803-b095-308c21ae8160",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get training data from first fold to build graph\n",
    "train_df, val_df = folds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79d09745-a582-46a1-8e3d-9887c4e074a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Generate Graph from One Training Fold\n",
    "\n",
    "### Graph Construction\n",
    "Origin Airport --Flight--> Destination Airport\n",
    "\n",
    "* **Nodes**: Airport Codes\n",
    "* **Edges**: Flights\n",
    "  * **Direction**: Origin to Destination\n",
    "  * **Weight**: Number of Flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d0ca135-61ea-40d8-8ccb-fe0095a38835",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Build graph: nodes are airports, edges are flights (origin -> dest)\n",
    "# Edge weights = number of flights between airports\n",
    "\n",
    "# Create edges: (origin, dest) with count as weight\n",
    "edges = (\n",
    "    train_df\n",
    "    .select(\"origin\", \"dest\")\n",
    "    .filter(col(\"origin\").isNotNull() & col(\"dest\").isNotNull())\n",
    "    .groupBy(\"origin\", \"dest\")\n",
    "    .count()\n",
    "    .withColumnRenamed(\"origin\", \"src\")\n",
    "    .withColumnRenamed(\"dest\", \"dst\")\n",
    "    .withColumnRenamed(\"count\", \"weight\")\n",
    ")\n",
    "\n",
    "display(edges.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5a2c7a7-9105-4e55-924f-b6361946094b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Checkpoint to run notebook more quickly in the future\n",
    "edges_path = f\"{FOLDER_PATH}/graph_edges.parquet\"\n",
    "edges.write.mode(\"overwrite\").parquet(edges_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aeeee33d-9ead-45b4-866f-c2933b1f1596",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# If re-running this notebook, start here as edges are checkpointed\n",
    "edges = spark.read.parquet(edges_path)\n",
    "edges.count()  # Materialize\n",
    "display(edges.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4982c5f1-1f15-43ff-9761-177abf04d2fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create vertices: all unique airports (both origin and destination)\n",
    "src_airports = edges.select(col(\"src\").alias(\"id\")).distinct()\n",
    "dst_airports = edges.select(col(\"dst\").alias(\"id\")).distinct()\n",
    "vertices = src_airports.union(dst_airports).distinct()\n",
    "\n",
    "print(f\"Number of airports (vertices): {vertices.count()}\")\n",
    "print(f\"Number of routes (edges): {edges.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ba35774-7047-44b9-a19a-fe15319d7c14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Generate Graphframes\n",
    "**NOTE**: GraphFrames PageRank does NOT use edge weights (HW5 Q5.f). It treats all edges equally (weight = 1), ignoring the \"weight\" column. For weighted PageRank, we'd need to use the RDD approach from HW5 Q4\n",
    "\n",
    "**Correction**: GraphFrames automatically detects a column named weight in the edges DataFrame. Pagerank uses weights by default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18fdf84f-a3de-4487-9b10-07b6b4886585",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create GraphFrame\n",
    "start = time.time()\n",
    "g = GraphFrame(vertices, edges)\n",
    "print(f\"Ran in {time.time() - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85aba1b6-5a40-42a8-8f4a-23e7e10a54d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify edges DataFrame has \"weight\" column (GraphFrames automatically detects this)\n",
    "print(\"Edges DataFrame schema:\")\n",
    "edges.printSchema()\n",
    "print(\"\\nSample edges with weights:\")\n",
    "display(edges.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b344ed7-3e66-4c37-8fab-361d0dbd7b0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### \"Weighted\" GraphFrame Workaround"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b411f98-deb9-47df-80b5-701a4434e54f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create weighted graph using duplication workaround\n",
    "# Duplicate each edge based on its weight (e.g., weight=3 means 3 copies of the edge)\n",
    "# This simulates weighted PageRank since GraphFrames treats all edges equally\n",
    "start = time.time()\n",
    "\n",
    "# Get edges with weight column\n",
    "edges_with_weights = g.edges\n",
    "\n",
    "# Create sequence array [0, 1, 2, ..., weight-1] for each edge, then explode to duplicate\n",
    "edges_weighted = (\n",
    "    edges_with_weights\n",
    "    .withColumn(\"seq\", F.sequence(F.lit(0), F.col(\"weight\").cast(\"int\") - 1))\n",
    "    .select(\"src\", \"dst\", F.explode(\"seq\").alias(\"_\"))\n",
    "    .select(\"src\", \"dst\")\n",
    ")\n",
    "\n",
    "# Create weighted GraphFrame (same vertices, duplicated edges)\n",
    "g_weighted = GraphFrame(vertices, edges_weighted)\n",
    "\n",
    "print(f\"Created weighted GraphFrame (duplication workaround) in {time.time() - start:.2f} seconds\")\n",
    "print(f\"Original edges: {edges.count()}\")\n",
    "print(f\"Weighted edges (after duplication): {edges_weighted.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64477ee0-53cd-4e9d-828e-75ad0dcb83fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Graph Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc6a15ca-3c08-44e7-84ea-4d93890a973b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set checkpoint directory (required for GraphFrames algorithms like connectedComponents)\n",
    "sc = spark.sparkContext\n",
    "sc.setCheckpointDir(\"dbfs:/tmp/graphframes_checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f16c731f-f6be-4a16-9287-a92721affb66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Connectivity Analysis (referencing HW5 concepts)\n",
    "# Check for connected components (islands) - weakly connected for directed graph\n",
    "connected_components = g.connectedComponents()\n",
    "\n",
    "# Count number of distinct components\n",
    "num_components = connected_components.select(\"component\").distinct().count()\n",
    "print(f\"Number of connected components (islands): {num_components}\")\n",
    "\n",
    "# Show component sizes\n",
    "component_sizes = (\n",
    "    connected_components\n",
    "    .groupBy(\"component\")\n",
    "    .count()\n",
    "    .orderBy(F.desc(\"count\"))\n",
    ")\n",
    "\n",
    "print(\"\\nTop 10 largest components:\")\n",
    "display(component_sizes.limit(10))\n",
    "\n",
    "# Check if graph is strongly connected (all nodes reachable from all nodes)\n",
    "# For directed graphs, we check strongly connected components\n",
    "strongly_connected = g.stronglyConnectedComponents(maxIter=10)\n",
    "num_strong_components = strongly_connected.select(\"component\").distinct().count()\n",
    "print(f\"\\nNumber of strongly connected components: {num_strong_components}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ebe7278-466b-4f2d-87ac-93f2a7d924c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check for dangling nodes (nodes with no outlinks) - key issue from HW5 Q2.b\n",
    "# Dangling nodes are nodes that receive links but don't link to anything else\n",
    "out_degree = g.outDegrees\n",
    "in_degree = g.inDegrees\n",
    "\n",
    "# Find nodes with out-degree = 0 (dangling nodes)\n",
    "dangling_nodes = (\n",
    "    vertices\n",
    "    .join(out_degree, \"id\", \"left_outer\")\n",
    "    .filter(col(\"outDegree\").isNull() | (col(\"outDegree\") == 0))\n",
    ")\n",
    "\n",
    "num_dangling = dangling_nodes.count()\n",
    "print(f\"Number of dangling nodes (no outlinks): {num_dangling}\")\n",
    "\n",
    "if num_dangling > 0:\n",
    "    print(\"\\nSample dangling nodes:\")\n",
    "    display(dangling_nodes.limit(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ad69702-2915-4d5a-9c89-1e7db5e8b431",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check for nodes with no inlinks (sources)\n",
    "source_nodes = (\n",
    "    vertices\n",
    "    .join(in_degree, \"id\", \"left_outer\")\n",
    "    .filter(col(\"inDegree\").isNull() | (col(\"inDegree\") == 0))\n",
    ")\n",
    "\n",
    "num_sources = source_nodes.count()\n",
    "print(f\"Number of source nodes (no inlinks): {num_sources}\")\n",
    "\n",
    "# Summary statistics\n",
    "total_nodes = vertices.count()\n",
    "print(f\"\\n=== Graph Connectivity Summary ===\")\n",
    "print(f\"Total nodes: {total_nodes}\")\n",
    "print(f\"Weakly connected components: {num_components}\")\n",
    "print(f\"Strongly connected components: {num_strong_components}\")\n",
    "print(f\"Dangling nodes (no outlinks): {num_dangling}\")\n",
    "print(f\"Source nodes (no inlinks): {num_sources}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d471c2c0-42d1-4ea4-9c84-ddbccfec8cc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Run PageRank on Connectivity Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5be45832-c7d6-49cd-8751-534da0784038",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Run PageRank **without** edge weights\n",
    "pagerank_results_unweighted = g.pageRank(resetProbability=0.15, maxIter=10)\n",
    "\n",
    "# Display top airports by PageRank\n",
    "top_airports_unweighted = pagerank_results_unweighted.vertices.orderBy(F.desc(\"pagerank\")).limit(20)\n",
    "display(top_airports_unweighted)\n",
    "\n",
    "print(f\"Ran in {time.time() - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flight Sequence Graph (For Visualization/Analysis)\n",
    "\n",
    "### ⚠️ Important: GraphFrames vs Window Functions\n",
    "\n",
    "**Key Insight**: For Flight Lineage feature engineering, we DON'T need GraphFrames!\n",
    "\n",
    "The features we need (previous flight info, cumulative delays, conditional expected values) are all:\n",
    "- **Window functions** (LAG, SUM, AVG over partitions)\n",
    "- **Joins** (with pre-computed conditional expected value tables)\n",
    "- **Conditional logic** (IF-THEN calculations)\n",
    "\n",
    "**Graph algorithms** (PageRank, shortest paths, etc.) are NOT needed.\n",
    "\n",
    "**See**: `FLIGHT_LINEAGE_DESIGN.md` for detailed analysis.\n",
    "\n",
    "### When to Use This Graph\n",
    "\n",
    "This graph is useful for:\n",
    "- **Visualization**: Understanding flight sequences\n",
    "- **Analysis**: Jump patterns, sequence statistics\n",
    "- **NOT for feature engineering**: Use window functions instead\n",
    "\n",
    "### Sequential Graph Structure (No Self-Loops)\n",
    "\n",
    "To avoid self-loops and create a strictly sequential graph:\n",
    "\n",
    "**Nodes**: `{tail_num}_{FL_DATE}_{seq_num}` or `{tail_num}_{FL_DATE}_{crs_dep_time}`\n",
    "- Ensures uniqueness\n",
    "- Preserves temporal ordering\n",
    "- No self-loops\n",
    "\n",
    "**Edges**: `(node_i, node_i+1)` where nodes are consecutive flights\n",
    "- `src`: `{tail_num}_{date}_{seq_num}`\n",
    "- `dst`: `{tail_num}_{date}_{seq_num+1}`\n",
    "- Edge attributes: `dest_airport`, `air_time`, `is_jump`, etc.\n",
    "\n",
    "**Example**: A→B→A→C [Break] D→E becomes:\n",
    "- Node1: `N12345_2023-01-15_1` (A→B)\n",
    "- Node2: `N12345_2023-01-15_2` (B→A)\n",
    "- Node3: `N12345_2023-01-15_3` (A→C)\n",
    "- [Break/jump]\n",
    "- Node4: `N12345_2023-01-15_4` (D→E)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Sequential Flight Sequence Graph (for visualization/analysis)\n",
    "# Nodes: {tail_num}_{FL_DATE}_{seq_num} to avoid self-loops\n",
    "# This creates a strictly sequential graph: A→B→A→C [Break] D→E\n",
    "\n",
    "print(\"Building Sequential Flight Sequence Graph...\")\n",
    "print(\"Nodes: {tail_num}_{FL_DATE}_{seq_num} (unique flight identifiers)\")\n",
    "print(\"Edges: Consecutive flights in sequence (no self-loops)\\n\")\n",
    "print(\"NOTE: For feature engineering, use window functions instead of this graph.\\n\")\n",
    "\n",
    "# Check if tail_num column exists\n",
    "tail_cols = [c for c in train_df.columns if 'tail' in c.lower()]\n",
    "print(f\"Tail number columns found: {tail_cols}\")\n",
    "\n",
    "if not tail_cols:\n",
    "    print(\"WARNING: No tail_num column found. Skipping flight sequence graph.\")\n",
    "else:\n",
    "    tail_col = tail_cols[0]  # Use first matching column\n",
    "    print(f\"Using column: {tail_col}\\n\")\n",
    "    \n",
    "    # Prepare data: need tail_num, FL_DATE, origin, dest, scheduled times, delays\n",
    "    required_cols = [tail_col, 'FL_DATE', 'origin', 'dest', 'crs_dep_time']\n",
    "    available_cols = [c for c in required_cols if c in train_df.columns]\n",
    "    \n",
    "    if len(available_cols) < len(required_cols):\n",
    "        missing = set(required_cols) - set(available_cols)\n",
    "        print(f\"WARNING: Missing columns: {missing}\")\n",
    "        print(\"Skipping flight sequence graph.\")\n",
    "    else:\n",
    "        # Select and filter flight data\n",
    "        flight_seq_data = (\n",
    "            train_df\n",
    "            .select(\n",
    "                tail_col, 'FL_DATE', 'origin', 'dest', \n",
    "                'crs_dep_time', 'air_time', 'DEP_DELAY', 'ARR_DELAY'\n",
    "            )\n",
    "            .filter(\n",
    "                col(tail_col).isNotNull() & \n",
    "                col('FL_DATE').isNotNull() &\n",
    "                col('origin').isNotNull() & \n",
    "                col('dest').isNotNull() &\n",
    "                col('crs_dep_time').isNotNull()\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        print(f\"Total flights with valid tail_num: {flight_seq_data.count():,}\")\n",
    "        \n",
    "        # Order flights by tail_num, date, and scheduled departure time\n",
    "        # This ensures we track sequences correctly (A→B→A→C, not A→B & A→C)\n",
    "        from pyspark.sql.window import Window\n",
    "        \n",
    "        window_spec = Window.partitionBy(tail_col, 'FL_DATE').orderBy('crs_dep_time')\n",
    "        \n",
    "        # Add sequence number and previous flight information\n",
    "        flight_seq_ordered = (\n",
    "            flight_seq_data\n",
    "            .withColumn('seq_num', F.row_number().over(window_spec))\n",
    "            .withColumn('prev_dest', F.lag('dest', 1).over(window_spec))\n",
    "            .withColumn('prev_air_time', F.lag('air_time', 1).over(window_spec))\n",
    "            .withColumn('prev_arr_delay', F.lag('ARR_DELAY', 1).over(window_spec))\n",
    "        )\n",
    "        \n",
    "        # Detect jumps: when prev_dest != current_origin (or prev_dest is null for first flight)\n",
    "        flight_seq_with_jumps = flight_seq_ordered.withColumn(\n",
    "            'is_jump',\n",
    "            when(col('seq_num') == 1, F.lit(False))  # First flight is not a jump\n",
    "            .when(col('prev_dest').isNull(), F.lit(True))  # Missing previous flight = jump\n",
    "            .otherwise(col('prev_dest') != col('origin'))  # prev_dest != origin = jump\n",
    "        )\n",
    "        \n",
    "        # Create unique node IDs: {tail_num}_{FL_DATE}_{seq_num}\n",
    "        # This ensures no self-loops and preserves temporal ordering\n",
    "        flight_seq_with_nodes = flight_seq_with_jumps.withColumn(\n",
    "            'node_id',\n",
    "            F.concat(\n",
    "                col(tail_col), F.lit('_'),\n",
    "                col('FL_DATE'), F.lit('_'),\n",
    "                col('seq_num')\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Create edges: (node_i, node_i+1) for consecutive flights\n",
    "        # Only create edges between consecutive flights (seq_num difference = 1)\n",
    "        flight_seq_edges = (\n",
    "            flight_seq_with_nodes\n",
    "            .withColumn('next_node_id', F.lead('node_id', 1).over(window_spec))\n",
    "            .filter(col('next_node_id').isNotNull())  # Skip last flight in sequence\n",
    "            .select(\n",
    "                col('node_id').alias('src'),  # Current flight node\n",
    "                col('next_node_id').alias('dst'),  # Next flight node\n",
    "                # Edge attributes:\n",
    "                col('origin').alias('origin_airport'),\n",
    "                col('dest').alias('dest_airport'),\n",
    "                col('FL_DATE').alias('flight_date'),\n",
    "                col('crs_dep_time').alias('scheduled_dep_time'),\n",
    "                col('air_time').alias('air_time'),\n",
    "                col('is_jump').alias('is_jump'),\n",
    "                col('DEP_DELAY').alias('dep_delay'),\n",
    "                col('ARR_DELAY').alias('arr_delay'),\n",
    "                col('seq_num').alias('seq_num'),\n",
    "                col('prev_dest').alias('prev_dest_airport'),\n",
    "                col('prev_air_time').alias('prev_air_time'),\n",
    "                col('prev_arr_delay').alias('prev_arr_delay')\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Create vertices: all unique node IDs\n",
    "        flight_seq_vertices = (\n",
    "            flight_seq_with_nodes\n",
    "            .select(\n",
    "                col('node_id').alias('id'),\n",
    "                col(tail_col).alias('tail_num'),\n",
    "                col('FL_DATE').alias('flight_date'),\n",
    "                col('seq_num').alias('seq_num'),\n",
    "                col('origin').alias('origin_airport'),\n",
    "                col('dest').alias('dest_airport'),\n",
    "                col('crs_dep_time').alias('scheduled_dep_time')\n",
    "            )\n",
    "            .distinct()\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nFlight sequence edges created: {flight_seq_edges.count():,}\")\n",
    "        print(\"\\nSample flight sequence edges (showing sequential structure, no self-loops):\")\n",
    "        display(flight_seq_edges.select('src', 'dst', 'origin_airport', 'dest_airport', \n",
    "                                        'seq_num', 'is_jump', 'prev_dest_airport').limit(20))\n",
    "        \n",
    "        print(f\"\\nFlight sequence nodes (unique flight identifiers): {flight_seq_vertices.count():,}\")\n",
    "        print(\"\\nSample nodes:\")\n",
    "        display(flight_seq_vertices.limit(10))\n",
    "        \n",
    "        # Analyze jump patterns\n",
    "        jump_stats = (\n",
    "            flight_seq_edges\n",
    "            .groupBy('is_jump')\n",
    "            .agg(\n",
    "                F.count('*').alias('count'),\n",
    "                F.avg('dep_delay').alias('avg_dep_delay'),\n",
    "                F.avg('arr_delay').alias('avg_arr_delay')\n",
    "            )\n",
    "        )\n",
    "        print(\"\\nJump Statistics:\")\n",
    "        display(jump_stats)\n",
    "        \n",
    "        # Save for later use\n",
    "        flight_seq_edges_path = f\"{FOLDER_PATH}/flight_sequence_edges.parquet\"\n",
    "        flight_seq_vertices_path = f\"{FOLDER_PATH}/flight_sequence_vertices.parquet\"\n",
    "        \n",
    "        flight_seq_edges.write.mode(\"overwrite\").parquet(flight_seq_edges_path)\n",
    "        flight_seq_vertices.write.mode(\"overwrite\").parquet(flight_seq_vertices_path)\n",
    "        \n",
    "        print(f\"\\nSaved flight sequence graph components:\")\n",
    "        print(f\"  Edges: {flight_seq_edges_path}\")\n",
    "        print(f\"  Vertices: {flight_seq_vertices_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load flight sequence graph (if re-running)\n",
    "flight_seq_edges_path = f\"{FOLDER_PATH}/flight_sequence_edges.parquet\"\n",
    "flight_seq_vertices_path = f\"{FOLDER_PATH}/flight_sequence_vertices.parquet\"\n",
    "\n",
    "try:\n",
    "    flight_seq_edges = spark.read.parquet(flight_seq_edges_path)\n",
    "    flight_seq_vertices = spark.read.parquet(flight_seq_vertices_path)\n",
    "    \n",
    "    print(f\"Loaded flight sequence graph:\")\n",
    "    print(f\"  Edges: {flight_seq_edges.count():,}\")\n",
    "    print(f\"  Vertices: {flight_seq_vertices.count():,}\")\n",
    "    \n",
    "    print(\"\\nSample edges (showing sequence and jump detection):\")\n",
    "    display(flight_seq_edges.select('src', 'dst', 'origin_airport', 'dest_airport', \n",
    "                                    'seq_num', 'is_jump', 'prev_dest_airport').limit(10))\n",
    "    \n",
    "    # Show jump statistics\n",
    "    jump_stats = (\n",
    "        flight_seq_edges\n",
    "        .groupBy('is_jump')\n",
    "        .agg(F.count('*').alias('count'))\n",
    "    )\n",
    "    print(\"\\nJump Statistics:\")\n",
    "    display(jump_stats)\n",
    "except Exception as e:\n",
    "    print(f\"Could not load flight sequence graph: {e}\")\n",
    "    print(\"Run the previous cell to build it first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GraphFrame Creation and Limitations\n",
    "\n",
    "**Note**: This graph uses self-loops (edges from tail_num to the same tail_num). GraphFrames may have limitations with self-loops for certain algorithms like PageRank. We'll test and document any issues.\n",
    "    start = time.time()\n",
    "    \n",
    "    g_flight_seq = GraphFrame(flight_seq_vertices, flight_seq_edges_agg)\n",
    "    \n",
    "    print(f\"Created GraphFrame in {time.time() - start:.2f} seconds\")\n",
    "    print(f\"  Vertices: {g_flight_seq.vertices.count():,}\")\n",
    "    print(f\"  Edges: {g_flight_seq.edges.count():,}\")\n",
    "    \n",
    "    # Compute basic graph statistics\n",
    "    print(\"\\n=== Flight Sequence Graph Statistics ===\")\n",
    "    \n",
    "    # Connected components\n",
    "    connected_components_fs = g_flight_seq.connectedComponents()\n",
    "    num_components_fs = connected_components_fs.select(\"component\").distinct().count()\n",
    "    print(f\"Connected components: {num_components_fs}\")\n",
    "    \n",
    "    # In/out degrees\n",
    "    in_degree_fs = g_flight_seq.inDegrees\n",
    "    out_degree_fs = g_flight_seq.outDegrees\n",
    "    \n",
    "    print(f\"Nodes with in-degree > 0: {in_degree_fs.count():,}\")\n",
    "    print(f\"Nodes with out-degree > 0: {out_degree_fs.count():,}\")\n",
    "    \n",
    "    # Sample nodes\n",
    "    print(\"\\nSample nodes (first 10):\")\n",
    "    display(g_flight_seq.vertices.limit(10))\n",
    "else:\n",
    "    print(\"Flight sequence graph not available. Run previous cells to build it.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GraphFrame for flight sequence graph\n",
    "# NOTE: This graph has self-loops (src == dst), which may limit some GraphFrames algorithms\n",
    "\n",
    "if 'flight_seq_edges' in locals() and 'flight_seq_vertices' in locals():\n",
    "    print(\"Creating GraphFrame for flight sequence graph...\")\n",
    "    print(\"WARNING: This graph contains self-loops (same tail_num as src and dst)\")\n",
    "    print(\"Some GraphFrames algorithms (e.g., PageRank) may not work correctly with self-loops.\\n\")\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        g_flight_seq = GraphFrame(flight_seq_vertices, flight_seq_edges)\n",
    "        \n",
    "        print(f\"Created GraphFrame in {time.time() - start:.2f} seconds\")\n",
    "        print(f\"  Vertices: {g_flight_seq.vertices.count():,}\")\n",
    "        print(f\"  Edges: {g_flight_seq.edges.count():,}\")\n",
    "        \n",
    "        # Check for self-loops\n",
    "        self_loops = (\n",
    "            g_flight_seq.edges\n",
    "            .filter(col('src') == col('dst'))\n",
    "            .count()\n",
    "        )\n",
    "        print(f\"  Self-loops: {self_loops:,} ({100*self_loops/g_flight_seq.edges.count():.1f}%)\")\n",
    "        \n",
    "        # Compute basic graph statistics\n",
    "        print(\"\\n=== Flight Sequence Graph Statistics ===\")\n",
    "        \n",
    "        # In/out degrees\n",
    "        in_degree_fs = g_flight_seq.inDegrees\n",
    "        out_degree_fs = g_flight_seq.outDegrees\n",
    "        \n",
    "        print(f\"Nodes with in-degree > 0: {in_degree_fs.count():,}\")\n",
    "        print(f\"Nodes with out-degree > 0: {out_degree_fs.count():,}\")\n",
    "        \n",
    "        # Sample nodes\n",
    "        print(\"\\nSample nodes (first 10):\")\n",
    "        display(g_flight_seq.vertices.limit(10))\n",
    "        \n",
    "        # Sample edges with attributes\n",
    "        print(\"\\nSample edges with attributes:\")\n",
    "        display(g_flight_seq.edges.select('src', 'dst', 'origin_airport', 'dest_airport', \n",
    "                                          'seq_num', 'is_jump').limit(10))\n",
    "        \n",
    "        # Test PageRank (may fail with self-loops)\n",
    "        print(\"\\n=== Testing PageRank (may fail with self-loops) ===\")\n",
    "        try:\n",
    "            pagerank_fs = g_flight_seq.pageRank(resetProbability=0.15, maxIter=10)\n",
    "            print(\"✓ PageRank completed successfully\")\n",
    "            print(\"\\nTop 10 tail_nums by PageRank:\")\n",
    "            display(pagerank_fs.vertices.orderBy(F.desc('pagerank')).limit(10))\n",
    "        except Exception as e:\n",
    "            print(f\"✗ PageRank failed (likely due to self-loops): {e}\")\n",
    "            print(\"  This is expected - we may need a different graph structure for PageRank\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating GraphFrame: {e}\")\n",
    "        print(\"This may be due to self-loops or other graph structure issues.\")\n",
    "else:\n",
    "    print(\"Flight sequence graph not available. Run previous cells to build it.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering Approach (Window Functions)\n",
    "\n",
    "**For actual feature engineering, use window functions instead of the graph:**\n",
    "\n",
    "This section demonstrates how to compute Flight Lineage features using window functions, which is the recommended approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Flight Lineage Feature Engineering using Window Functions\n",
    "# This is the RECOMMENDED approach for feature engineering\n",
    "\n",
    "if 'train_df' in locals():\n",
    "    print(\"Computing Flight Lineage features using window functions...\")\n",
    "    \n",
    "    # Find tail_num column\n",
    "    tail_cols = [c for c in train_df.columns if 'tail' in c.lower()]\n",
    "    if tail_cols:\n",
    "        tail_col = tail_cols[0]\n",
    "        print(f\"Using tail_num column: {tail_col}\\n\")\n",
    "        \n",
    "        # Prepare data\n",
    "        lineage_features = (\n",
    "            train_df\n",
    "            .select(\n",
    "                tail_col, 'FL_DATE', 'origin', 'dest', 'crs_dep_time',\n",
    "                'DEP_DELAY', 'ARR_DELAY', 'air_time', 'op_carrier'\n",
    "            )\n",
    "            .filter(\n",
    "                col(tail_col).isNotNull() & \n",
    "                col('FL_DATE').isNotNull() &\n",
    "                col('origin').isNotNull() & \n",
    "                col('dest').isNotNull() &\n",
    "                col('crs_dep_time').isNotNull()\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Window specification: partition by tail_num and date, order by scheduled departure time\n",
    "        from pyspark.sql.window import Window\n",
    "        window_spec = Window.partitionBy(tail_col, 'FL_DATE').orderBy('crs_dep_time')\n",
    "        \n",
    "        # 1. Previous flight information (using LAG)\n",
    "        lineage_features = lineage_features.withColumn(\n",
    "            'prev_flight_dest', F.lag('dest', 1).over(window_spec)\n",
    "        ).withColumn(\n",
    "            'prev_flight_arr_delay', F.lag('ARR_DELAY', 1).over(window_spec)\n",
    "        ).withColumn(\n",
    "            'prev_flight_dep_delay', F.lag('DEP_DELAY', 1).over(window_spec)\n",
    "        ).withColumn(\n",
    "            'prev_flight_air_time', F.lag('air_time', 1).over(window_spec)\n",
    "        )\n",
    "        \n",
    "        # 2. Sequence number\n",
    "        lineage_features = lineage_features.withColumn(\n",
    "            'seq_num', F.row_number().over(window_spec)\n",
    "        )\n",
    "        \n",
    "        # 3. Jump detection\n",
    "        lineage_features = lineage_features.withColumn(\n",
    "            'is_jump',\n",
    "            when(col('seq_num') == 1, F.lit(False))\n",
    "            .when(col('prev_flight_dest').isNull(), F.lit(True))\n",
    "            .otherwise(col('prev_flight_dest') != col('origin'))\n",
    "        )\n",
    "        \n",
    "        # 4. Cumulative delay (sum of previous flights' delays)\n",
    "        lineage_features = lineage_features.withColumn(\n",
    "            'cumulative_delay_since_3am',\n",
    "            F.sum('DEP_DELAY').over(\n",
    "                window_spec.rowsBetween(Window.unboundedPreceding, -1)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # 5. Number of previous flights\n",
    "        lineage_features = lineage_features.withColumn(\n",
    "            'num_previous_flights_today',\n",
    "            F.count('*').over(\n",
    "                window_spec.rowsBetween(Window.unboundedPreceding, -1)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # 6. Average delay per previous flight\n",
    "        lineage_features = lineage_features.withColumn(\n",
    "            'avg_delay_per_previous_flight',\n",
    "            F.avg('DEP_DELAY').over(\n",
    "                window_spec.rowsBetween(Window.unboundedPreceding, -1)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        print(\"✓ Flight Lineage features computed using window functions\")\n",
    "        print(f\"\\nSample features:\")\n",
    "        display_cols = [tail_col, 'FL_DATE', 'origin', 'dest', 'seq_num', 'is_jump',\n",
    "                       'prev_flight_dest', 'prev_flight_arr_delay', \n",
    "                       'cumulative_delay_since_3am', 'num_previous_flights_today']\n",
    "        display(lineage_features.select(display_cols).limit(10))\n",
    "        \n",
    "        print(\"\\n=== Summary Statistics ===\")\n",
    "        jump_stats = (\n",
    "            lineage_features\n",
    "            .groupBy('is_jump')\n",
    "            .agg(\n",
    "                F.count('*').alias('count'),\n",
    "                F.avg('cumulative_delay_since_3am').alias('avg_cumulative_delay')\n",
    "            )\n",
    "        )\n",
    "        display(jump_stats)\n",
    "        \n",
    "        print(\"\\nNOTE: For conditional expected values (turn time, air time),\")\n",
    "        print(\"      join with pre-computed tables from Time-Series Features Experiment.\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"KEY INSIGHT: Pulling Engineered Features from Previous Flight\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"\"\"\n",
    "To use ENGINEERED features from the previous flight (not just raw values):\n",
    "1. First, compute the feature for the current flight using previous flight's RAW values\n",
    "2. Then, use LAG to pull that computed feature for the NEXT flight\n",
    "\n",
    "Example:\n",
    "  Flight 2: Compute expected_arrival_time using prev_actual_dep_time\n",
    "  Flight 3: Use LAG(expected_arrival_time) to get prev_expected_arrival_time\n",
    "  Flight 3: Compute expected_departure_time using prev_expected_arrival_time\n",
    "\n",
    "This requires multiple passes with withColumn, but Spark handles this correctly.\n",
    "        \"\"\")\n",
    "    else:\n",
    "        print(\"tail_num column not found. Skipping feature engineering example.\")\n",
    "else:\n",
    "    print(\"train_df not available. Load data first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: Pulling Engineered Features from Previous Flight\n",
    "\n",
    "This example shows how to compute features that depend on **engineered features** from the previous flight, not just raw values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Computing features that depend on previous flight's ENGINEERED features\n",
    "# This demonstrates the multi-pass approach\n",
    "\n",
    "if 'train_df' in locals():\n",
    "    print(\"Example: Computing features with dependencies on previous flight's engineered features...\\n\")\n",
    "    \n",
    "    tail_cols = [c for c in train_df.columns if 'tail' in c.lower()]\n",
    "    if tail_cols:\n",
    "        tail_col = tail_cols[0]\n",
    "        \n",
    "        # Prepare data\n",
    "        df = (\n",
    "            train_df\n",
    "            .select(\n",
    "                tail_col, 'FL_DATE', 'origin', 'dest', 'crs_dep_time',\n",
    "                'dep_time', 'arr_time', 'DEP_DELAY', 'ARR_DELAY', \n",
    "                'air_time', 'op_carrier'\n",
    "            )\n",
    "            .filter(\n",
    "                col(tail_col).isNotNull() & \n",
    "                col('FL_DATE').isNotNull() &\n",
    "                col('origin').isNotNull() & \n",
    "                col('dest').isNotNull() &\n",
    "                col('crs_dep_time').isNotNull()\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        window_spec = Window.partitionBy(tail_col, 'FL_DATE').orderBy('crs_dep_time')\n",
    "        \n",
    "        # ============================================================\n",
    "        # PASS 1: Pull RAW values from previous flight\n",
    "        # ============================================================\n",
    "        print(\"Pass 1: Pulling raw values from previous flight...\")\n",
    "        df = df.withColumn('seq_num', F.row_number().over(window_spec))\n",
    "        df = df.withColumn('prev_actual_dep_time', F.lag('dep_time', 1).over(window_spec))\n",
    "        df = df.withColumn('prev_air_time', F.lag('air_time', 1).over(window_spec))\n",
    "        df = df.withColumn('prev_dest', F.lag('dest', 1).over(window_spec))\n",
    "        \n",
    "        # ============================================================\n",
    "        # PASS 2: Compute ENGINEERED features for CURRENT flight\n",
    "        #         using previous flight's RAW values\n",
    "        # ============================================================\n",
    "        print(\"Pass 2: Computing engineered features using previous flight's raw values...\")\n",
    "        \n",
    "        # For this example, we'll use a simple expected air time (in real code, join with lookup table)\n",
    "        # In production, you'd join with expected_air_time_route from Time-Series Features\n",
    "        df = df.withColumn('expected_air_time_route', \n",
    "            when(col('air_time').isNotNull(), col('air_time'))  # Use actual if available\n",
    "            .otherwise(F.lit(120))  # Placeholder - in production, join with lookup table\n",
    "        )\n",
    "        \n",
    "        # Compute expected arrival time of PREVIOUS flight\n",
    "        # This is computed for the CURRENT flight, but represents the previous flight's expected arrival\n",
    "        df = df.withColumn('expected_arrival_time_prev_flight',\n",
    "            when(\n",
    "                (col('prev_actual_dep_time').isNotNull()) & \n",
    "                (col('expected_air_time_route').isNotNull()),\n",
    "                col('prev_actual_dep_time') + col('expected_air_time_route')\n",
    "            )\n",
    "            .otherwise(None)\n",
    "        )\n",
    "        \n",
    "        # ============================================================\n",
    "        # PASS 3: Pull ENGINEERED features from previous flight\n",
    "        #         for use in CURRENT flight's calculations\n",
    "        # ============================================================\n",
    "        print(\"Pass 3: Pulling engineered features from previous flight...\")\n",
    "        \n",
    "        # Now pull the computed feature for use in current flight\n",
    "        df = df.withColumn('prev_expected_arrival_time',\n",
    "            F.lag('expected_arrival_time_prev_flight', 1).over(window_spec)\n",
    "        )\n",
    "        \n",
    "        # ============================================================\n",
    "        # PASS 4: Use previous flight's ENGINEERED features to compute\n",
    "        #         current flight's features\n",
    "        # ============================================================\n",
    "        print(\"Pass 4: Computing current flight features using previous flight's engineered features...\")\n",
    "        \n",
    "        # For this example, use a simple expected turn time (in production, join with lookup table)\n",
    "        df = df.withColumn('expected_turn_time_carrier_airport', F.lit(45))  # Placeholder\n",
    "        \n",
    "        # Compute expected departure time for CURRENT flight\n",
    "        # This uses the previous flight's EXPECTED ARRIVAL TIME (an engineered feature)\n",
    "        df = df.withColumn('expected_departure_time_current',\n",
    "            when(\n",
    "                (col('prev_expected_arrival_time').isNotNull()) &\n",
    "                (col('expected_turn_time_carrier_airport').isNotNull()),\n",
    "                col('prev_expected_arrival_time') + col('expected_turn_time_carrier_airport')\n",
    "            )\n",
    "            .otherwise(None)\n",
    "        )\n",
    "        \n",
    "        # Compute time buffer and impossible on-time flag\n",
    "        df = df.withColumn('time_buffer',\n",
    "            when(\n",
    "                (col('expected_departure_time_current').isNotNull()) &\n",
    "                (col('crs_dep_time').isNotNull()),\n",
    "                col('crs_dep_time') - col('expected_departure_time_current')\n",
    "            )\n",
    "            .otherwise(None)\n",
    "        )\n",
    "        \n",
    "        df = df.withColumn('impossible_on_time_flag',\n",
    "            when(col('time_buffer').isNotNull(),\n",
    "                 when(col('time_buffer') < 0, 1).otherwise(0))\n",
    "            .otherwise(None)\n",
    "        )\n",
    "        \n",
    "        print(\"\\n✓ Multi-pass feature engineering complete!\")\n",
    "        print(\"\\nSample results showing feature dependencies:\")\n",
    "        display_cols = [\n",
    "            tail_col, 'seq_num', 'origin', 'dest',\n",
    "            'prev_actual_dep_time',  # Raw value from prev flight\n",
    "            'expected_arrival_time_prev_flight',  # Engineered for prev flight\n",
    "            'prev_expected_arrival_time',  # Pulled engineered feature\n",
    "            'expected_departure_time_current',  # Uses prev engineered feature\n",
    "            'time_buffer', 'impossible_on_time_flag'\n",
    "        ]\n",
    "        display(df.select(display_cols).limit(10))\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Key Points:\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"1. LAG can pull ANY column, including ones you just computed\")\n",
    "        print(\"2. Order matters: compute features in dependency order\")\n",
    "        print(\"3. Each withColumn can see results of previous withColumn calls\")\n",
    "        print(\"4. In production, join with lookup tables for expected values\")\n",
    "        print(\"5. Handle NULLs carefully (first flight, jumps, missing data)\")\n",
    "    else:\n",
    "        print(\"tail_num column not found.\")\n",
    "else:\n",
    "    print(\"train_df not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bda0f811-d585-493c-badb-4cca891c91fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run Weighted PageRank using the \"weight\" column (number of flights)\n",
    "pagerank_results_weighted = g_weighted.pageRank(resetProbability=0.15, maxIter=10)\n",
    "\n",
    "# Display top airports by PageRank\n",
    "top_airports_weighted = pagerank_results_weighted.vertices.orderBy(F.desc(\"pagerank\")).limit(20)\n",
    "display(top_airports_weighted)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Graph Features Experiment",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
