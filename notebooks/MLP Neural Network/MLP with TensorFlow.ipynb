{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6d3daea-7b87-4ac3-a8aa-dc2e45e8ea74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "cv.py (simplified, CUSTOM-only, no parametrization)\n",
    "\n",
    "Assumptions:\n",
    "- Folds were created from split.py with N_FOLDS = 3 and CREATE_TEST_FOLD = True\n",
    "- Therefore total fold indices written = 4:\n",
    "    FOLD_1_VAL, FOLD_2_VAL, FOLD_3_VAL, FOLD_4_TEST\n",
    "- Files live in:\n",
    "    dbfs:/mnt/mids-w261/student-groups/Group_4_2/processed\n",
    "- File naming:\n",
    "    OTPW_CUSTOM_{VERSION}_FOLD_{i}_{TRAIN|VAL|TEST}.parquet\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "mlflow.autolog(disable=True)\n",
    "\n",
    "# -----------------------------\n",
    "# HARD-CODED GLOBALS\n",
    "# -----------------------------\n",
    "FOLDER_PATH = \"dbfs:/mnt/mids-w261/student-groups/Group_4_2/processed\"\n",
    "SOURCE = \"CUSTOM\"\n",
    "VERSIONS = [\"3M\", \"12M\"]\n",
    "\n",
    "# 3 CV folds + 1 test fold = 4 total fold indices\n",
    "TOTAL_FOLDS = 4\n",
    "\n",
    "\n",
    "class FlightDelayDataLoader:\n",
    "    \"\"\"\n",
    "    CUSTOM-only loader that guarantees all numerical features are cast to double.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.folder_path = \"dbfs:/mnt/mids-w261/student-groups/Group_4_2/processed\"\n",
    "        self.source = \"CUSTOM\"\n",
    "        self.folds = {}\n",
    "        self.versions = [\"3M\", \"12M\"]\n",
    "\n",
    "        self.numerical_features = [\n",
    "            'hourlyprecipitation',\n",
    "            'hourlysealevelpressure',\n",
    "            'hourlyaltimetersetting',\n",
    "            'hourlywetbulbtemperature',\n",
    "            'hourlystationpressure',\n",
    "            'hourlywinddirection',\n",
    "            'hourlyrelativehumidity',\n",
    "            'hourlywindspeed',\n",
    "            'hourlydewpointtemperature',\n",
    "            'hourlydrybulbtemperature',\n",
    "            'hourlyvisibility',\n",
    "            'crs_elapsed_time', # scheduled flight time\n",
    "            'quarter', # inferred from month\n",
    "            'flights', # number of flights? always 1?\n",
    "            'distance', # flight distance, probably important\n",
    "            'year', # excluded bc new predictions will always be in a new year\n",
    "            # latitude and longitude not very useful in linear regression\n",
    "            'origin_station_lat',\n",
    "            'origin_station_lon',\n",
    "            'origin_airport_lat',\n",
    "            'origin_airport_lon',\n",
    "            'origin_station_dis',\n",
    "            'dest_station_lat',\n",
    "            'dest_station_lon',\n",
    "            'dest_airport_lat',\n",
    "            'dest_airport_lon',\n",
    "            'dest_station_dis',\n",
    "            'latitude',\n",
    "            'longitude',\n",
    "            'elevation',\n",
    "        ]\n",
    "\n",
    "    def _cast_numerics(self, df):\n",
    "        \"\"\"\n",
    "        Safely cast all configured numeric columns to doubles.\n",
    "        Handles common bad values like '', 'NA', 'M', 'T', '.', etc.\n",
    "        \"\"\"\n",
    "\n",
    "        # Patterns that should be treated as null\n",
    "        NULL_PAT = r'^(NA|N/A|NULL|null|None|none|\\\\N|\\\\s*|\\\\.|M|T)$'\n",
    "\n",
    "        for colname in self.numerical_features:\n",
    "            if colname in df.columns:\n",
    "                df = df.withColumn(\n",
    "                    colname,\n",
    "                    F.regexp_replace(F.col(colname).cast(\"string\"), NULL_PAT, \"\")\n",
    "                    .cast(\"double\")\n",
    "                )\n",
    "\n",
    "        # Explicitly cast labels to expected numeric types\n",
    "        if \"DEP_DELAY\" in df.columns:\n",
    "            df = df.withColumn(\"DEP_DELAY\", F.col(\"DEP_DELAY\").cast(\"double\"))\n",
    "        if \"DEP_DEL15\" in df.columns:\n",
    "            df = df.withColumn(\"DEP_DEL15\", F.col(\"DEP_DEL15\").cast(\"int\"))\n",
    "        if \"SEVERE_DEL60\" in df.columns:\n",
    "            df = df.withColumn(\"SEVERE_DEL60\", F.col(\"SEVERE_DEL60\").cast(\"int\"))\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _load_parquet(self, name):\n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "        df = spark.read.parquet(f\"{self.folder_path}/{name}.parquet\")\n",
    "        df = self._cast_numerics(df)\n",
    "        return df\n",
    "\n",
    "    def _load_version(self, version):\n",
    "        folds = []\n",
    "        for fold_idx in range(1, 4 + 1):  # 3 CV folds + 1 test\n",
    "            train_name = f\"OTPW_{self.source}_{version}_FOLD_{fold_idx}_TRAIN\"\n",
    "            train_df = self._load_parquet(train_name)\n",
    "\n",
    "            if fold_idx < 4:\n",
    "                val_name = f\"OTPW_{self.source}_{version}_FOLD_{fold_idx}_VAL\"\n",
    "                val_df = self._load_parquet(val_name)\n",
    "                folds.append((train_df, val_df))\n",
    "            else:\n",
    "                test_name = f\"OTPW_{self.source}_{version}_FOLD_{fold_idx}_TEST\"\n",
    "                test_df = self._load_parquet(test_name)\n",
    "                folds.append((train_df, test_df))\n",
    "\n",
    "        return folds\n",
    "\n",
    "    def load(self):\n",
    "        for version in self.versions:\n",
    "            self.folds[version] = self._load_version(version)\n",
    "\n",
    "    def get_version(self, version):\n",
    "        return self.folds[version]\n",
    "\n",
    "# -----------------------------\n",
    "# EVALUATOR (NULL-SAFE RMSE)\n",
    "# -----------------------------\n",
    "class FlightDelayEvaluator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        prediction_col=\"prediction\",\n",
    "        numeric_label_col=\"DEP_DELAY\",\n",
    "        binary_label_col=\"DEP_DEL15\",\n",
    "        severe_label_col=\"SEVERE_DEL60\",\n",
    "    ):\n",
    "        self.prediction_col = prediction_col\n",
    "        self.numeric_label_col = numeric_label_col\n",
    "        self.binary_label_col = binary_label_col\n",
    "        self.severe_label_col = severe_label_col\n",
    "\n",
    "        self.rmse_evaluator = RegressionEvaluator(\n",
    "            predictionCol=prediction_col,\n",
    "            labelCol=numeric_label_col,\n",
    "            metricName=\"rmse\"\n",
    "        )\n",
    "\n",
    "    def calculate_rmse(self, predictions_df):\n",
    "        # Drop any residual nulls before RegressionEvaluator sees them\n",
    "        clean = predictions_df.dropna(\n",
    "            subset=[self.numeric_label_col, self.prediction_col]\n",
    "        )\n",
    "        return self.rmse_evaluator.evaluate(clean)\n",
    "\n",
    "    def _calculate_classification_metrics(self, predictions_df, threshold, label_col):\n",
    "        # Null-safe for classification too\n",
    "        df = predictions_df.dropna(subset=[self.prediction_col, label_col])\n",
    "\n",
    "        pred_binary_col = f\"pred_binary_{threshold}\"\n",
    "        df = df.withColumn(\n",
    "            pred_binary_col,\n",
    "            F.when(F.col(self.prediction_col) >= threshold, 1).otherwise(0)\n",
    "        )\n",
    "\n",
    "        tp = df.filter((F.col(pred_binary_col) == 1) & (F.col(label_col) == 1)).count()\n",
    "        fp = df.filter((F.col(pred_binary_col) == 1) & (F.col(label_col) == 0)).count()\n",
    "        tn = df.filter((F.col(pred_binary_col) == 0) & (F.col(label_col) == 0)).count()\n",
    "        fn = df.filter((F.col(pred_binary_col) == 0) & (F.col(label_col) == 1)).count()\n",
    "\n",
    "        total = tp + fp + tn + fn\n",
    "        precision = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0.0\n",
    "        accuracy = (tp + tn) / total if total else 0.0\n",
    "\n",
    "        return dict(tp=tp, fp=fp, tn=tn, fn=fn,\n",
    "                    precision=precision, recall=recall, f1=f1, accuracy=accuracy)\n",
    "\n",
    "    def calculate_otpa_metrics(self, predictions_df):\n",
    "        return self._calculate_classification_metrics(\n",
    "            predictions_df, threshold=15, label_col=self.binary_label_col\n",
    "        )[\"accuracy\"]\n",
    "\n",
    "    def calculate_sddr_metrics(self, predictions_df):\n",
    "        return self._calculate_classification_metrics(\n",
    "            predictions_df, threshold=60, label_col=self.severe_label_col\n",
    "        )[\"recall\"]\n",
    "\n",
    "    def evaluate(self, predictions_df):\n",
    "        return {\n",
    "            \"rmse\": self.calculate_rmse(predictions_df),\n",
    "            \"otpa\": self.calculate_otpa_metrics(predictions_df),\n",
    "            \"sddr\": self.calculate_sddr_metrics(predictions_df),\n",
    "        }\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# CROSS-VALIDATOR (NO PARAMS)\n",
    "# -----------------------------\n",
    "class FlightDelayCV:\n",
    "    def __init__(self, estimator, dataloader, version):\n",
    "        self.estimator = estimator\n",
    "        self.version = version\n",
    "\n",
    "        if dataloader:\n",
    "            self.data_loader = dataloader\n",
    "        else:\n",
    "            self.data_loader = FlightDelayDataLoader()\n",
    "            self.data_loader.load()\n",
    "\n",
    "        self.evaluator = FlightDelayEvaluator()\n",
    "        self.folds = self.data_loader.get_version(version)\n",
    "\n",
    "        self.metrics = []\n",
    "        self.models = []\n",
    "        self.test_metric = None\n",
    "        self.test_model = None\n",
    "\n",
    "    def fit(self):\n",
    "        # CV folds only (exclude last test fold)\n",
    "        for train_df, val_df in self.folds[:-1]:\n",
    "            model = self.estimator.fit(train_df)\n",
    "            preds = model.transform(val_df)\n",
    "\n",
    "            metric = self.evaluator.evaluate(preds)\n",
    "            self.metrics.append(metric)\n",
    "            self.models.append(model)\n",
    "\n",
    "        m = pd.DataFrame(self.metrics)\n",
    "        m.loc[\"mean\"] = m.mean()\n",
    "        m.loc[\"std\"] = m.std()\n",
    "        return m\n",
    "\n",
    "    def evaluate(self):\n",
    "        train_df, test_df = self.folds[-1]\n",
    "        self.test_model = self.estimator.fit(train_df)\n",
    "        preds = self.test_model.transform(test_df)\n",
    "        self.test_metric = self.evaluator.evaluate(preds)\n",
    "        return self.test_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7d86bcd-729e-46c2-aaa3-603e14cc1140",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_loader = FlightDelayDataLoader()\n",
    "data_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "419a18dc-f068-45bf-bad8-307502463042",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# MLP REGRESSOR INTEGRATED WITH FLIGHTDELAYCV\n",
    "# =====================================================\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F, Window\n",
    "from pyspark.ml.feature import (\n",
    "    Imputer, StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    ")\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# from horovod.spark.keras import HorovodRunner\n",
    "from tensorflow.keras import layers, regularizers, callbacks\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 1. MLP ESTIMATOR (Spark-compatible interface)\n",
    "# =====================================================\n",
    "\n",
    "\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class RMSE(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"rmse\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.mse = tf.keras.metrics.MeanSquaredError()\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.mse.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def result(self):\n",
    "        return tf.sqrt(self.mse.result())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.mse.reset_state()\n",
    "\n",
    "class SparkMLPRegressor:\n",
    "    def __init__(self, hidden_layers=None, dropout_rate=0.3, learning_rate=0.001, \n",
    "                 batch_size=256, epochs=1, early_stopping=True, patience=10, verbose=1): \n",
    "                 # NOTE: epochs set to 1 here, we will loop through data manually.\n",
    "        \n",
    "        self.hidden_layers = hidden_layers or [128, 64]\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs # Total passes over data (external loop)\n",
    "        self.early_stopping = early_stopping\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.model = None\n",
    "        self.input_dim = None\n",
    "    \n",
    "    import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "    def _build_model(self, input_dim):\n",
    "\n",
    "        ##############################################################\n",
    "        # >>> NEW GPU CONFIGURATION BLOCK <<<\n",
    "        ##############################################################\n",
    "        try:\n",
    "            # Check for available GPUs\n",
    "            gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "            if gpus:\n",
    "                # Set memory growth to prevent the process from taking all GPU memory\n",
    "                tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "                print(f\"Successfully configured TensorFlow to use GPU: {gpus[0].name}\")\n",
    "            else:\n",
    "                print(\"No GPU devices found. Training will proceed on CPU.\")\n",
    "        except Exception as e:\n",
    "            # Catch errors if tf.config is accessed too late or other issues\n",
    "            print(f\"Error setting up GPU configuration: {e}\")\n",
    "        ##############################################################\n",
    "\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Input(shape=(input_dim,)))\n",
    "        \n",
    "        # ... (layers, batch norm, dropout)\n",
    "        model.add(tf.keras.layers.Dense(self.hidden_layers[0], kernel_regularizer=tf.keras.regularizers.l2(1e-4)))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.Activation('relu'))\n",
    "        model.add(tf.keras.layers.Dropout(self.dropout_rate))\n",
    "        \n",
    "        for units in self.hidden_layers[1:]:\n",
    "            model.add(tf.keras.layers.Dense(units, kernel_regularizer=tf.keras.regularizers.l2(1e-4)))\n",
    "            model.add(tf.keras.layers.BatchNormalization())\n",
    "            model.add(tf.keras.layers.Activation('relu'))\n",
    "            model.add(tf.keras.layers.Dropout(self.dropout_rate))\n",
    "            \n",
    "        model.add(tf.keras.layers.Dense(1)) \n",
    "        \n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        # We compile with a loss, but we will use train_on_batch, not model.fit\n",
    "        model.compile(optimizer=optimizer, loss='mse', metrics=[RMSE()])\n",
    "        self.model = model\n",
    "        return model\n",
    "\n",
    "    def fit(self, df):\n",
    "        \"\"\"\n",
    "        FIXED: Uses rdd.toLocalIterator() for sequential, chunked training on the driver.\n",
    "        This avoids the OOM crash.\n",
    "        \"\"\"\n",
    "        print(\"Starting sequential training on driver in small batches...\")\n",
    "        \n",
    "        # 1. Prepare data stream: Convert Vector to Array and filter nulls/nans\n",
    "        df_arrays = df.select(\n",
    "            vector_to_array(\"scaled_features\").alias(\"features_arr\"), \n",
    "            \"DEP_DELAY\"\n",
    "        )\n",
    "        df_arrays = df_arrays.filter(\n",
    "            F.col(\"features_arr\").isNotNull() & F.col(\"DEP_DELAY\").isNotNull()\n",
    "        )\n",
    "        df_arrays = df_arrays.filter(F.size(F.col(\"features_arr\")) > 0)\n",
    "        \n",
    "        # Get the input dimension (need to get one row to initialize model)\n",
    "        try:\n",
    "            sample_row = df_arrays.head()\n",
    "            self.input_dim = len(sample_row[\"features_arr\"])\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to get feature dimension: {e}\")\n",
    "\n",
    "        if self.model is None:\n",
    "            self._build_model(self.input_dim)\n",
    "\n",
    "        # 2. Setup training loop\n",
    "        # We use a simple counter for manual epoch tracking.\n",
    "        current_patience = 0\n",
    "        best_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            print(f\"--- Epoch {epoch + 1}/{self.epochs} ---\")\n",
    "            \n",
    "            # Get an iterator that pulls data from workers without OOM on driver\n",
    "            data_iterator = df_arrays.rdd.toLocalIterator()\n",
    "            \n",
    "            features_buffer = []\n",
    "            labels_buffer = []\n",
    "            total_loss = 0\n",
    "            total_batches = 0\n",
    "            \n",
    "            for row in data_iterator:\n",
    "                # 3. Accumulate rows into a batch\n",
    "                features_buffer.append(row[\"features_arr\"])\n",
    "                labels_buffer.append(row[\"DEP_DELAY\"])\n",
    "                \n",
    "                if len(features_buffer) >= self.batch_size:\n",
    "                    # 4. Convert buffered lists to NumPy arrays\n",
    "                    X_batch = np.stack(features_buffer)\n",
    "                    y_batch = np.array(labels_buffer)\n",
    "                    \n",
    "                    # 5. Train on the batch and get loss/metrics\n",
    "                    metrics = self.model.train_on_batch(X_batch, y_batch)\n",
    "                    total_loss += metrics[0]\n",
    "                    total_batches += 1\n",
    "                    \n",
    "                    # Clear the buffers for the next batch\n",
    "                    features_buffer = []\n",
    "                    labels_buffer = []\n",
    "            \n",
    "            # Handle the final partial batch, if any\n",
    "            if len(features_buffer) > 0:\n",
    "                X_batch = np.stack(features_buffer)\n",
    "                y_batch = np.array(labels_buffer)\n",
    "                metrics = self.model.train_on_batch(X_batch, y_batch)\n",
    "                total_loss += metrics[0]\n",
    "                total_batches += 1\n",
    "\n",
    "            # Calculate and log mean loss for the epoch\n",
    "            epoch_loss = total_loss / total_batches if total_batches > 0 else 0\n",
    "            if self.verbose:\n",
    "                print(f\"Epoch {epoch + 1} finished. Mean Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "            # 6. Manual Early Stopping check (Simple implementation)\n",
    "            if self.early_stopping:\n",
    "                if epoch_loss < best_loss:\n",
    "                    best_loss = epoch_loss\n",
    "                    current_patience = 0\n",
    "                    # NOTE: In a real system, you would save model weights here\n",
    "                else:\n",
    "                    current_patience += 1\n",
    "                    if current_patience >= self.patience:\n",
    "                        print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "                        break\n",
    "\n",
    "        print(\"Sequential training complete.\")\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        \"\"\"\n",
    "        Uses Iterator UDF (mapInPandas) for parallel inference on workers, \n",
    "        loading the Horovod-trained model.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not fitted.\")\n",
    "\n",
    "        # Serialize model state (for broadcast to workers)\n",
    "        model_json = self.model.to_json()\n",
    "        model_weights = self.model.get_weights()\n",
    "        schema = df.schema.add(\"prediction\", DoubleType())\n",
    "        \n",
    "        def predict_partition_full(iterator):\n",
    "            # 1. Initialize model ONCE per partition\n",
    "            worker_model = tf.keras.models.model_from_json(model_json)\n",
    "            worker_model.set_weights(model_weights)\n",
    "            \n",
    "            for pdf_batch in iterator:\n",
    "                # 2. Extract features\n",
    "                # Use vector_to_array conversion from the previous step\n",
    "                X_batch = np.stack(pdf_batch[\"features_arr\"].values)\n",
    "                preds = worker_model.predict(X_batch, verbose=0).flatten()\n",
    "                \n",
    "                # 3. Assign and yield\n",
    "                pdf_batch[\"prediction\"] = preds\n",
    "                yield pdf_batch.drop(columns=[\"features_arr\"])\n",
    "\n",
    "        # Add the array column temporarily\n",
    "        df_with_arr = df.withColumn(\"features_arr\", vector_to_array(\"scaled_features\"))\n",
    "        \n",
    "        # Final transform\n",
    "        return df_with_arr.mapInPandas(predict_partition_full, schema=schema)\n",
    "\n",
    "    \n",
    "\n",
    "# =====================================================\n",
    "# 2. MLP PIPELINE WRAPPER\n",
    "# =====================================================\n",
    "\n",
    "class MLPFlightDelayPipeline:\n",
    "    \"\"\"\n",
    "    Wrapper that combines Spark preprocessing + MLP into a single estimator.\n",
    "    Compatible with FlightDelayCV.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        categorical_features,\n",
    "        numerical_features,\n",
    "        mlp_params=None,\n",
    "    ):\n",
    "        self.categorical_features = categorical_features\n",
    "        self.numerical_features = numerical_features\n",
    "        self.mlp_params = mlp_params or {}\n",
    "        \n",
    "        self.preprocessing_pipeline = None\n",
    "        self.mlp_regressor = None\n",
    "        \n",
    "    def _build_preprocessing_pipeline(self):\n",
    "        \"\"\"\n",
    "        Build Spark ML pipeline for feature preprocessing.\n",
    "        \"\"\"\n",
    "        imputer = Imputer(\n",
    "            inputCols=self.numerical_features,\n",
    "            outputCols=[f\"{col}_IMPUTED\" for col in self.numerical_features],\n",
    "            strategy=\"mean\"\n",
    "        )\n",
    "        \n",
    "        indexer = StringIndexer(\n",
    "            inputCols=self.categorical_features,\n",
    "            outputCols=[f\"{col}_INDEX\" for col in self.categorical_features],\n",
    "            handleInvalid=\"keep\"\n",
    "        )\n",
    "        \n",
    "        encoder = OneHotEncoder(\n",
    "            inputCols=[f\"{col}_INDEX\" for col in self.categorical_features],\n",
    "            outputCols=[f\"{col}_VEC\" for col in self.categorical_features],\n",
    "            dropLast=False\n",
    "        )\n",
    "        \n",
    "        assembler = VectorAssembler(\n",
    "            inputCols=[f\"{col}_VEC\" for col in self.categorical_features] + \n",
    "                      [f\"{col}_IMPUTED\" for col in self.numerical_features],\n",
    "            outputCol=\"features\",\n",
    "            handleInvalid=\"skip\"\n",
    "        )\n",
    "        \n",
    "        scaler = StandardScaler(\n",
    "            inputCol=\"features\",\n",
    "            outputCol=\"scaled_features\",\n",
    "            withMean=True,\n",
    "            withStd=True\n",
    "        )\n",
    "        \n",
    "        self.preprocessing_pipeline = Pipeline(\n",
    "            stages=[imputer, indexer, encoder, assembler, scaler]\n",
    "        )\n",
    "        \n",
    "        return self.preprocessing_pipeline\n",
    "    \n",
    "    def fit(self, df):\n",
    "        \"\"\"\n",
    "        Fit preprocessing pipeline and MLP on training data.\n",
    "        Returns self for chaining.\n",
    "        \"\"\"\n",
    "        # Build and fit preprocessing pipeline (only first time)\n",
    "        if self.preprocessing_pipeline is None:\n",
    "            self._build_preprocessing_pipeline()\n",
    "            self.preprocessing_pipeline = self.preprocessing_pipeline.fit(df)\n",
    "        \n",
    "        # Transform training data using already-fitted pipeline\n",
    "        preprocessed = self.preprocessing_pipeline.transform(df)\n",
    "        \n",
    "        # Build and fit MLP\n",
    "        self.mlp_regressor = SparkMLPRegressor(**self.mlp_params)\n",
    "        self.mlp_regressor.fit(preprocessed)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        \"\"\"\n",
    "        Preprocess and generate predictions on new data.\n",
    "        \"\"\"\n",
    "        if self.preprocessing_pipeline is None or self.mlp_regressor is None:\n",
    "            raise ValueError(\"Pipeline not fitted yet. Call fit() first.\")\n",
    "        \n",
    "        # Apply preprocessing\n",
    "        preprocessed = self.preprocessing_pipeline.transform(df)\n",
    "        \n",
    "        # Generate predictions\n",
    "        predictions_df = self.mlp_regressor.transform(preprocessed)\n",
    "        \n",
    "        return predictions_df\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 3. FEATURE ENGINEERING (OPTIONAL ENHANCEMENT)\n",
    "# =====================================================\n",
    "\n",
    "class FlightDelayFeatureEngineer:\n",
    "    \"\"\"\n",
    "    Optionally add cyclical and interaction features before preprocessing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    def add_temporal_features(self):\n",
    "        \"\"\"Add sin/cos encoding for cyclical features.\"\"\"\n",
    "        self.df = self.df.withColumn(\n",
    "            \"month_sin\", F.sin(2 * F.lit(np.pi) * F.col(\"month\") / 12)\n",
    "        ).withColumn(\n",
    "            \"month_cos\", F.cos(2 * F.lit(np.pi) * F.col(\"month\") / 12)\n",
    "        ).withColumn(\n",
    "            \"day_of_week_sin\", F.sin(2 * F.lit(np.pi) * F.col(\"day_of_week\") / 7)\n",
    "        ).withColumn(\n",
    "            \"day_of_week_cos\", F.cos(2 * F.lit(np.pi) * F.col(\"day_of_week\") / 7)\n",
    "        )\n",
    "        return self\n",
    "    \n",
    "    def add_interaction_features(self):\n",
    "        \"\"\"Create interaction terms for MLP to learn.\"\"\"\n",
    "        self.df = self.df.withColumn(\n",
    "            \"distance_x_windspeed\", F.col(\"distance\") * F.col(\"hourlywindspeed\")\n",
    "        ).withColumn(\n",
    "            \"distance_x_visibility\", F.col(\"distance\") * F.col(\"hourlyvisibility\")\n",
    "        ).withColumn(\n",
    "            \"pressure_x_humidity\",\n",
    "            F.col(\"hourlysealevelpressure\") * F.col(\"hourlyrelativehumidity\")\n",
    "        )\n",
    "        return self\n",
    "    \n",
    "    def get(self):\n",
    "        return self.df\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 4. USAGE WITH FLIGHTDELAYCV\n",
    "# =====================================================\n",
    "\n",
    "# Feature definitions\n",
    "categorical_features = [\n",
    "    'day_of_week', 'op_carrier', 'dep_time_blk', 'arr_time_blk', 'day_of_month', 'month'\n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    'hourlyprecipitation', 'hourlysealevelpressure', 'hourlyaltimetersetting',\n",
    "    'hourlywetbulbtemperature', 'hourlystationpressure', 'hourlywinddirection',\n",
    "    'hourlyrelativehumidity', 'hourlywindspeed', 'hourlydewpointtemperature',\n",
    "    'hourlydrybulbtemperature', 'hourlyvisibility', 'crs_elapsed_time', 'distance', 'elevation'\n",
    "]\n",
    "\n",
    "# MLP hyperparameters (tunable for your data)\n",
    "mlp_params = {\n",
    "    'hidden_layers': [512, 256, 128],  # Much smaller to reduce memory & computation\n",
    "    'dropout_rate': 0.1,\n",
    "    'learning_rate': 0.005,\n",
    "    'batch_size': 128,  # Smaller batch size for training\n",
    "    'epochs': 10,  # Fewer epochs\n",
    "    'early_stopping': True,\n",
    "    'patience': 5,\n",
    "    'verbose': 1,  # Set to 1 to see training progress\n",
    "}\n",
    "\n",
    "# Initialize MLP pipeline\n",
    "mlp_pipeline = MLPFlightDelayPipeline(\n",
    "    categorical_features=categorical_features,\n",
    "    numerical_features=numerical_features,\n",
    "    mlp_params=mlp_params,\n",
    ")\n",
    "\n",
    "# Use with FlightDelayCV (same interface as your Linear Regression and Random Forest)\n",
    "cv = FlightDelayCV(\n",
    "    estimator=mlp_pipeline,\n",
    "    dataloader=data_loader,\n",
    "    version=\"12M\"\n",
    ")\n",
    "\n",
    "# Run cross-validation\n",
    "cv_results = cv.fit()\n",
    "print(\"Cross-Validation Results:\")\n",
    "print(cv_results)\n",
    "\n",
    "# Evaluate on held-out test fold\n",
    "test_results = cv.evaluate()\n",
    "print(\"\\nTest Set Results:\")\n",
    "print(test_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b2e3dec-f9a7-456a-a873-bf816fb02a7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =====================================================\n",
    "# 6. OPTIONAL: PERFORMANCE DIAGNOSTICS\n",
    "# =====================================================\n",
    "\n",
    "def profile_mlp_timing(data_loader, version=\"12M\"):\n",
    "    \"\"\"\n",
    "    Profile where time is spent: data loading, preprocessing, or training.\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    train_df, val_df = data_loader.get_version(version)[0]\n",
    "    \n",
    "    # Time 1: Preprocessing pipeline build + fit\n",
    "    print(\"Timing preprocessing pipeline...\")\n",
    "    start = time.time()\n",
    "    \n",
    "    preprocessing_pipeline = Pipeline(stages=[\n",
    "        Imputer(\n",
    "            inputCols=numerical_features,\n",
    "            outputCols=[f\"{col}_IMPUTED\" for col in numerical_features],\n",
    "            strategy=\"mean\"\n",
    "        ),\n",
    "        StringIndexer(\n",
    "            inputCols=categorical_features,\n",
    "            outputCols=[f\"{col}_INDEX\" for col in categorical_features],\n",
    "            handleInvalid=\"keep\"\n",
    "        ),\n",
    "        OneHotEncoder(\n",
    "            inputCols=[f\"{col}_INDEX\" for col in categorical_features],\n",
    "            outputCols=[f\"{col}_VEC\" for col in categorical_features],\n",
    "            dropLast=False\n",
    "        ),\n",
    "        VectorAssembler(\n",
    "            inputCols=[f\"{col}_VEC\" for col in categorical_features] + \n",
    "                      [f\"{col}_IMPUTED\" for col in numerical_features],\n",
    "            outputCol=\"features\",\n",
    "            handleInvalid=\"skip\"\n",
    "        ),\n",
    "        StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withMean=True, withStd=True),\n",
    "    ])\n",
    "    \n",
    "    fitted_pipeline = preprocessing_pipeline.fit(train_df)\n",
    "    preprocessed = fitted_pipeline.transform(train_df)\n",
    "    preprocessed.cache().count()  # Force evaluation\n",
    "    preprocess_time = time.time() - start\n",
    "    \n",
    "    # Time 2: Data extraction to NumPy\n",
    "    print(\"Timing data extraction...\")\n",
    "    start = time.time()\n",
    "    data_rdd = preprocessed.select(\"scaled_features\", \"DEP_DELAY\").rdd.map(\n",
    "        lambda row: (row[0].toArray(), float(row[1]) if row[1] is not None else np.nan)\n",
    "    )\n",
    "    collected = data_rdd.collect()\n",
    "    extract_time = time.time() - start\n",
    "    \n",
    "    # Time 3: MLP training\n",
    "    print(\"Timing MLP training...\")\n",
    "    start = time.time()\n",
    "    \n",
    "    mlp = SparkMLPRegressor(**mlp_params)\n",
    "    mlp.fit(preprocessed)\n",
    "    \n",
    "    train_time = time.time() - start\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TIMING BREAKDOWN:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Preprocessing pipeline: {preprocess_time:.2f}s\")\n",
    "    print(f\"Data extraction:       {extract_time:.2f}s\")\n",
    "    print(f\"MLP training:          {train_time:.2f}s\")\n",
    "    print(f\"Total:                 {preprocess_time + extract_time + train_time:.2f}s\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return {\n",
    "        'preprocess_time': preprocess_time,\n",
    "        'extract_time': extract_time,\n",
    "        'train_time': train_time,\n",
    "    }\n",
    "\n",
    "def grid_search_mlp(data_loader, version=\"12M\"):\n",
    "    \"\"\"\n",
    "    Simple grid search over MLP hyperparameters across CV folds.\n",
    "    \"\"\"\n",
    "    param_grid = [\n",
    "        {\n",
    "            'hidden_layers': [256, 128],\n",
    "            'dropout_rate': 0.2,\n",
    "            'epochs': 50,\n",
    "        },\n",
    "        {\n",
    "            'hidden_layers': [512, 256, 128, 64],\n",
    "            'dropout_rate': 0.3,\n",
    "            'epochs': 100,\n",
    "        },\n",
    "        {\n",
    "            'hidden_layers': [1024, 512, 256, 128, 64],\n",
    "            'dropout_rate': 0.4,\n",
    "            'epochs': 150,\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, params in enumerate(param_grid):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Testing Config {i+1}: {params}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        mlp_pipeline = MLPFlightDelayPipeline(\n",
    "            categorical_features=categorical_features,\n",
    "            numerical_features=numerical_features,\n",
    "            mlp_params={**mlp_params, **params},\n",
    "        )\n",
    "        \n",
    "        cv = FlightDelayCV(\n",
    "            estimator=mlp_pipeline,\n",
    "            dataloader=data_loader,\n",
    "            version=version\n",
    "        )\n",
    "        \n",
    "        cv_results = cv.fit()\n",
    "        test_results = cv.evaluate()\n",
    "        \n",
    "        results.append({\n",
    "            'config': params,\n",
    "            'cv_mean_rmse': cv_results.loc['mean', 'rmse'],\n",
    "            'test_rmse': test_results['rmse'],\n",
    "            'test_otpa': test_results['otpa'],\n",
    "            'test_sddr': test_results['sddr'],\n",
    "        })\n",
    "        \n",
    "        print(f\"Test RMSE: {test_results['rmse']:.4f}\")\n",
    "        print(f\"Test OTPA: {test_results['otpa']:.4f}\")\n",
    "        print(f\"Test SDDR: {test_results['sddr']:.4f}\")\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "MLP with TensorFlow",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
