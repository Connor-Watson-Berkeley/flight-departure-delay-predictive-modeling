{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "054ffe7c-a254-48a8-91a7-8e1809e96d64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Spark Settings\n",
    "# to avoid OOM Error\n",
    "\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"16000\") \n",
    "try:\n",
    "    spark.conf.set(\"spark.databricks.io.cache.enabled\", \"false\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"Spark cache cleared.\")\n",
    "\n",
    "import importlib.util\n",
    "import sys\n",
    "\n",
    "# Load cv module directly from file path\n",
    "cv_path = \"/Workspace/Shared/Team 4_2/flight-departure-delay-predictive-modeling/notebooks/Cross Validator/cv.py\"\n",
    "spec = importlib.util.spec_from_file_location(\"cv\", cv_path)\n",
    "cv = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(cv)\n",
    "\n",
    "\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pathlib import Path\n",
    "from pyspark.ml.feature import (\n",
    "    Imputer, StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    ")\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# >>> PYTORCH AND DISTRIBUTOR IMPORTS <<<\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pyspark.ml.torch.distributor import TorchDistributor \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- PYTORCH TRAIN FUNCTION (RUNS ON WORKERS) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48427ae3-5adf-4b1d-ae30-0940114322f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Enable better logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "\n",
    "# Suppress py4j verbose logging\n",
    "logging.getLogger('py4j').setLevel(logging.ERROR)\n",
    "logging.getLogger('py4j.java_gateway').setLevel(logging.ERROR)\n",
    "logging.getLogger('py4j.clientserver').setLevel(logging.ERROR)\n",
    "\n",
    "# Optional: Also suppress other verbose Spark logs\n",
    "logging.getLogger('org.apache.spark').setLevel(logging.WARN)\n",
    "logging.getLogger('org.sparkproject').setLevel(logging.WARN)\n",
    "logging.getLogger('org.apache.hadoop').setLevel(logging.WARN)\n",
    "\n",
    "# Keep your own logs at INFO level\n",
    "logging.getLogger(__name__).setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "\n",
    "# Monitor memory\n",
    "import psutil\n",
    "import torch\n",
    "\n",
    "def print_memory_stats():\n",
    "    process = psutil.Process()\n",
    "    print(f\"CPU Memory: {process.memory_info().rss / 1024**3:.2f} GB\")\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"GPU {i} Memory: {torch.cuda.memory_allocated(i) / 1024**3:.2f} GB / \"\n",
    "                  f\"{torch.cuda.max_memory_allocated(i) / 1024**3:.2f} GB max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ef951ca-0d39-45b8-9aa5-d5a7c17a7879",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- SHARED MODEL DEFINITION ---\n",
    "class PyTorchMLPRegressor(nn.Module):\n",
    "    \"\"\"\n",
    "    Shared MLP architecture for both Training (Workers) and Inference (Driver/Workers).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_layers, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_features = input_dim\n",
    "        for units in hidden_layers:\n",
    "            layers.append(nn.Linear(in_features, units))\n",
    "            layers.append(nn.BatchNorm1d(units))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            in_features = units\n",
    "        layers.append(nn.Linear(in_features, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x).squeeze(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cd8cc27-bd11-442b-8ae5-35fef2665b69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# KEY FIXES FOR 60M DATASET SCALING\n",
    "# =====================================================\n",
    "\n",
    "# 1. TRAINING FUNCTION - Optimized for large datasets\n",
    "def train_fn(params):\n",
    "    import os, sys, traceback, glob, random, shutil\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    import torch.distributed as dist\n",
    "    from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "    from torch.utils.data import DataLoader, IterableDataset\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    # --- Early Stopping Helper ---\n",
    "    class EarlyStopping:\n",
    "        def __init__(self, patience=5, min_delta=0.0):\n",
    "            self.patience = patience\n",
    "            self.min_delta = min_delta\n",
    "            self.counter = 0\n",
    "            self.best_loss = None\n",
    "            self.early_stop = False\n",
    "\n",
    "        def __call__(self, val_loss):\n",
    "            if self.best_loss is None:\n",
    "                self.best_loss = val_loss\n",
    "            elif val_loss > self.best_loss - self.min_delta:\n",
    "                self.counter += 1\n",
    "                if self.counter >= self.patience:\n",
    "                    self.early_stop = True\n",
    "            else:\n",
    "                self.best_loss = val_loss\n",
    "                self.counter = 0\n",
    "\n",
    "    try:\n",
    "        # --- DDP Init ---\n",
    "        backend = \"nccl\" if params[\"use_gpu\"] and torch.cuda.is_available() else \"gloo\"\n",
    "        dist.init_process_group(backend=backend)\n",
    "\n",
    "        if params[\"use_gpu\"] and torch.cuda.is_available():\n",
    "            local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n",
    "            device = torch.device(f\"cuda:{local_rank}\")\n",
    "            torch.cuda.set_device(device)\n",
    "            device_ids = [local_rank]\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "            device_ids = None\n",
    "\n",
    "        rank = dist.get_rank()\n",
    "        world_size = dist.get_world_size()\n",
    "\n",
    "        # Convert dbfs path\n",
    "        path_local = params[\"train_path\"].replace(\"dbfs:/\", \"/dbfs/\") if params[\"train_path\"].startswith(\"dbfs:/\") else params[\"train_path\"]\n",
    "\n",
    "        # --- FIXED: More robust file listing ---\n",
    "        all_files = []\n",
    "        for root, dirs, files in os.walk(path_local):\n",
    "            for f in files:\n",
    "                if f.endswith('.parquet') and not f.startswith('.') and not f.startswith('_'):\n",
    "                    all_files.append(os.path.join(root, f))\n",
    "        \n",
    "        all_files = sorted(all_files)\n",
    "        \n",
    "        if rank == 0:\n",
    "            print(f\"Found {len(all_files)} parquet files\")\n",
    "        \n",
    "        # Deterministic shuffle\n",
    "        random.Random(42).shuffle(all_files)\n",
    "        \n",
    "        # 10% Validation Split\n",
    "        split_idx = max(1, int(len(all_files) * 0.9))\n",
    "        train_files_global = all_files[:split_idx]\n",
    "        val_files_global = all_files[split_idx:]\n",
    "\n",
    "        if not val_files_global:\n",
    "            print(f\"Warning: Not enough files for validation split. Using single file.\")\n",
    "            val_files_global = all_files[-1:]\n",
    "            train_files_global = all_files[:-1]\n",
    "\n",
    "        # --- FIXED: Improved Dataset with better error handling ---\n",
    "        class ParquetFlightIterableDataset(IterableDataset):\n",
    "            def __init__(self, file_list, rank, world_size):\n",
    "                self.file_list = file_list\n",
    "                self.rank = rank\n",
    "                self.world_size = world_size\n",
    "\n",
    "            def __iter__(self):\n",
    "                worker_info = torch.utils.data.get_worker_info()\n",
    "                if worker_info is None:\n",
    "                    my_files = self.file_list[self.rank::self.world_size]\n",
    "                else:\n",
    "                    # Split by GPU Rank then by CPU Worker\n",
    "                    gpu_files = self.file_list[self.rank::self.world_size]\n",
    "                    my_files = gpu_files[worker_info.id::worker_info.num_workers]\n",
    "\n",
    "                random.shuffle(my_files)\n",
    "                for f in my_files:\n",
    "                    try:\n",
    "                        if not os.path.exists(f):\n",
    "                            if self.rank == 0:\n",
    "                                print(f\"Warning: File not found: {f}\")\n",
    "                            continue\n",
    "                            \n",
    "                        pdf = pd.read_parquet(f, columns=[\"features_arr\", \"DEP_DELAY\"], engine='pyarrow')\n",
    "                        if len(pdf) == 0: \n",
    "                            continue\n",
    "                            \n",
    "                        X = np.stack(pdf[\"features_arr\"].values).astype(np.float32, copy=False)\n",
    "                        y = pdf[\"DEP_DELAY\"].values.astype(np.float32, copy=False)\n",
    "                        \n",
    "                        for i in range(len(y)):\n",
    "                            yield torch.from_numpy(X[i]), torch.tensor(y[i])\n",
    "                    except Exception as e:\n",
    "                        if self.rank == 0:\n",
    "                            print(f\"Error reading {f}: {str(e)}\")\n",
    "                        continue\n",
    "\n",
    "        # Create DataLoaders with reduced prefetch for memory\n",
    "        train_ds = ParquetFlightIterableDataset(train_files_global, rank, world_size)\n",
    "        val_ds = ParquetFlightIterableDataset(val_files_global, rank, world_size)\n",
    "\n",
    "        # FIXED: Reduced prefetch_factor to save memory\n",
    "        train_loader = DataLoader(train_ds, batch_size=params[\"batch_size\"], \n",
    "                                   num_workers=6, prefetch_factor=1, pin_memory=True) #updated num workers to 6 from 2, reduced prefetch to 1\n",
    "        val_loader = DataLoader(val_ds, batch_size=params[\"batch_size\"], \n",
    "                                num_workers=6, prefetch_factor=1, pin_memory=True)\n",
    "\n",
    "        # --- Model Setup ---\n",
    "        model = PyTorchMLPRegressor(params[\"input_dim\"], params[\"hidden_layers\"], params[\"dropout_rate\"]).to(device)\n",
    "        ddp_model = DDP(model, device_ids=device_ids)\n",
    "        optimizer = optim.Adam(ddp_model.parameters(), lr=params[\"learning_rate\"], weight_decay=params[\"weight_decay\"])\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        # Initialize Early Stopping\n",
    "        early_stopper = EarlyStopping(patience=params.get(\"patience\", 5))\n",
    "\n",
    "        # --- Training Loop ---\n",
    "        for epoch in range(params[\"epochs\"]):\n",
    "            # 1. Train\n",
    "            ddp_model.train()\n",
    "            train_loss_sum = 0.0\n",
    "            train_batches = 0\n",
    "            \n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                out = ddp_model(xb)\n",
    "                loss = criterion(out, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss_sum += loss.item()\n",
    "                train_batches += 1\n",
    "\n",
    "            # 2. Validation\n",
    "            ddp_model.eval()\n",
    "            val_loss_sum = 0.0\n",
    "            val_batches = 0\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in val_loader:\n",
    "                    xb, yb = xb.to(device), yb.to(device)\n",
    "                    out = ddp_model(xb)\n",
    "                    loss = criterion(out, yb)\n",
    "                    val_loss_sum += loss.item()\n",
    "                    val_batches += 1\n",
    "            \n",
    "            # 3. Aggregate Metrics across GPUs\n",
    "            metrics_tensor = torch.tensor([train_loss_sum, train_batches, val_loss_sum, val_batches], device=device)\n",
    "            dist.all_reduce(metrics_tensor, op=dist.ReduceOp.SUM)\n",
    "            \n",
    "            global_train_loss = metrics_tensor[0] / max(metrics_tensor[1], 1)\n",
    "            global_val_loss = metrics_tensor[2] / max(metrics_tensor[3], 1)\n",
    "\n",
    "            if rank == 0:\n",
    "                print(f\"Epoch {epoch+1}/{params['epochs']} | Train Loss: {global_train_loss:.4f} | Val Loss: {global_val_loss:.4f}\")\n",
    "\n",
    "            # 4. Check Early Stopping\n",
    "            early_stopper(global_val_loss.item())\n",
    "            \n",
    "            if early_stopper.early_stop:\n",
    "                if rank == 0:\n",
    "                    print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        # --- Save Model ---\n",
    "        if rank == 0:\n",
    "            torch.save(model.state_dict(), params[\"model_path\"])\n",
    "\n",
    "             \n",
    "        # CRITICAL FIX: Properly cleanup distributed process group\n",
    "        dist.barrier()  # Wait for all processes\n",
    "        dist.destroy_process_group()\n",
    "\n",
    "\n",
    "    except Exception:\n",
    "        if rank == 0:\n",
    "            print(traceback.format_exc())\n",
    "\n",
    "        # Try to cleanup even on error\n",
    "        try:\n",
    "            if dist.is_initialized():\n",
    "                dist.destroy_process_group()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        sys.exit(1)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "# 2. SPARKPYTORCHESTIMATOR - Fixed for large datasets\n",
    "class SparkPyTorchEstimator:\n",
    "    def __init__(self, hidden_layers=None, dropout_rate=0.3, learning_rate=0.001, \n",
    "                 batch_size=256, epochs=30, num_processes=None, infer_batch_size=None, patience=5, weight_decay=0.00001):\n",
    "        \n",
    "        self.hidden_layers = hidden_layers or [128, 64]\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "\n",
    "\n",
    "\n",
    "        self.infer_batch_size = infer_batch_size or batch_size\n",
    "        self.patience = patience\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        self.use_gpu = torch.cuda.is_available()\n",
    "        \n",
    "        if num_processes is None:\n",
    "            if self.use_gpu:\n",
    "                self.num_processes = torch.cuda.device_count()\n",
    "            else:\n",
    "                self.num_processes = 1\n",
    "        else:\n",
    "            self.num_processes = max(1, num_processes)\n",
    "\n",
    "        self.model_path = \"/dbfs/tmp/torch_mlp_state_dict.pth\"\n",
    "        self.input_dim = None\n",
    "        self.num_shards = None\n",
    "        \n",
    "    def fit(self, df):\n",
    "        \"\"\"\n",
    "        FIXED: Avoid collecting data, better sharding, explicit cache management\n",
    "        \"\"\"\n",
    "        # FIXED: Clear Spark cache explicitly\n",
    "        df.sparkSession.catalog.clearCache()\n",
    "        \n",
    "        # FIXED: Determine input_dim WITHOUT collecting - use schema instead\n",
    "        if self.input_dim is None:\n",
    "            # Get input_dim from the vector metadata instead of collecting\n",
    "            from pyspark.ml.linalg import VectorUDT\n",
    "            scaled_features_field = [f for f in df.schema.fields if f.name == \"scaled_features\"][0]\n",
    "            \n",
    "            # For VectorUDT, we need to collect just one row safely\n",
    "            try:\n",
    "                # Use take(1) which is safer than collect() for large datasets\n",
    "                sample = df.select(vector_to_array(\"scaled_features\").alias(\"features_arr\")).take(1)\n",
    "                if not sample:\n",
    "                    raise ValueError(\"No rows in training dataframe.\")\n",
    "                self.input_dim = len(sample[0][\"features_arr\"])\n",
    "                print(f\"Detected input dimension: {self.input_dim}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error determining input_dim: {e}\")\n",
    "                raise\n",
    "\n",
    "        # Prepare train DF (features_arr + label)\n",
    "        df_train = df.select(\n",
    "            vector_to_array(\"scaled_features\").alias(\"features_arr\"),\n",
    "            F.col(\"DEP_DELAY\").cast(DoubleType())\n",
    "        ).dropna(subset=[\"features_arr\", \"DEP_DELAY\"])\n",
    "\n",
    "        unique_id = str(uuid.uuid4())\n",
    "        train_path = f\"dbfs:/tmp/mlp_train_{unique_id}\"\n",
    "        \n",
    "        # FIXED: Significantly increase sharding for 60M dataset\n",
    "        # Calculate shards based on data size\n",
    "        estimated_rows = df_train.count() if hasattr(df_train, '_cached_count') else 60_000_000\n",
    "        \n",
    "        # Aim for ~30k rows per shard\n",
    "        optimal_shards = max(2000, int(estimated_rows / 30000))\n",
    "        num_shards = min(optimal_shards, 8000)  # Cap at 8000 to avoid too many small files\n",
    "\n",
    "        # Store for use in transform\n",
    "        self.num_shards = num_shards\n",
    "        \n",
    "        num_columns = len(df_train.columns)\n",
    "        print(f\"Number of columns: {num_columns}\")\n",
    "        print(f\"Number of shards: {num_shards}\")\n",
    "        print(f\"Estimated rows per shard: {estimated_rows / num_shards:.0f}\")\n",
    "\n",
    "        # FIXED: Add explicit memory management\n",
    "        df_train.persist()\n",
    "        \n",
    "        (\n",
    "            df_train\n",
    "            .repartition(num_shards)\n",
    "            .write\n",
    "            .mode(\"overwrite\")\n",
    "            .parquet(train_path)\n",
    "        )\n",
    "        \n",
    "        # FIXED: Unpersist after write to free memory\n",
    "        df_train.unpersist()\n",
    "        \n",
    "        params = {\n",
    "            \"input_dim\": self.input_dim,\n",
    "            \"hidden_layers\": self.hidden_layers,\n",
    "            \"dropout_rate\": self.dropout_rate,\n",
    "            \"learning_rate\": self.learning_rate,\n",
    "            \"weight_decay\": self.weight_decay,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"epochs\": self.epochs,\n",
    "            \"use_gpu\": self.use_gpu,\n",
    "            \"model_path\": self.model_path,\n",
    "            \"train_path\": train_path,\n",
    "            \"patience\": self.patience\n",
    "        }\n",
    "\n",
    "        distributor = TorchDistributor(\n",
    "            num_processes=self.num_processes,\n",
    "            local_mode=False,\n",
    "            use_gpu=self.use_gpu,\n",
    "        )\n",
    "\n",
    "        # Ensure model path directory exists\n",
    "        model_path_obj = Path(self.model_path)\n",
    "        model_path_obj.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        print(f\"Starting distributed training. Processes: {self.num_processes}, CUDA: {self.use_gpu}\")\n",
    "        distributor.run(train_fn, params)\n",
    "\n",
    "        # load model on driver\n",
    "        self.trained_model = PyTorchMLPRegressor(\n",
    "            self.input_dim, self.hidden_layers, self.dropout_rate\n",
    "        )\n",
    "        self.trained_model.load_state_dict(torch.load(self.model_path))\n",
    "        self.trained_model.eval()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        \"\"\"\n",
    "        FIXED: Smaller batch size for inference, better memory management\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'trained_model'):\n",
    "            raise ValueError(\"Model not fitted.\")\n",
    "        \n",
    "        # Serialize model weights for broadcast\n",
    "        model_state_dict = self.trained_model.state_dict()\n",
    "        schema = df.schema.add(\"prediction\", DoubleType())\n",
    "\n",
    "        # FIXED: Use smaller batch size for inference to avoid OOM\n",
    "        use_gpu = self.use_gpu\n",
    "        infer_batch_size = min(self.infer_batch_size, 512)  # Cap at 512 for safety\n",
    "        \n",
    "        print(f\"Using inference batch size: {infer_batch_size}\")\n",
    "\n",
    "        def predict_partition_full(iterator):\n",
    "            # 1. Setup device\n",
    "            device = torch.device(\"cuda\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
    "            \n",
    "            # 2. Initialize and load model ONCE per worker task\n",
    "            worker_model = PyTorchMLPRegressor(\n",
    "                self.input_dim, self.hidden_layers, self.dropout_rate\n",
    "            ).to(device)\n",
    "            worker_model.load_state_dict(model_state_dict)\n",
    "            worker_model.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for pdf_batch in iterator:\n",
    "                    if len(pdf_batch) == 0:\n",
    "                        continue\n",
    "                        \n",
    "                    # Extract features from this Spark partition batch\n",
    "                    X_np = np.stack(pdf_batch[\"features_arr\"].values)\n",
    "                    n = X_np.shape[0]\n",
    "\n",
    "                    # Pre-allocate predictions on CPU as float64\n",
    "                    preds_all = np.empty(n, dtype=np.float64)\n",
    "\n",
    "                    # --- BATCHED INFERENCE ---\n",
    "                    for start in range(0, n, infer_batch_size):\n",
    "                        end = min(start + infer_batch_size, n)\n",
    "                        inputs = torch.from_numpy(X_np[start:end]).float().to(device)\n",
    "                        preds = worker_model(inputs).cpu().numpy().astype(np.float64)\n",
    "                        preds_all[start:end] = preds\n",
    "                        \n",
    "                        # FIXED: Explicit cleanup for GPU memory\n",
    "                        del inputs\n",
    "                        if use_gpu:\n",
    "                            torch.cuda.empty_cache()\n",
    "\n",
    "                    pdf_batch[\"prediction\"] = preds_all\n",
    "                    yield pdf_batch.drop(columns=[\"features_arr\"])\n",
    "\n",
    "        # Add the array column temporarily\n",
    "        df_with_arr = df.withColumn(\"features_arr\", vector_to_array(\"scaled_features\"))\n",
    "        \n",
    "        # FIXED: Repartition before mapInPandas for better parallelism\n",
    "        # Use fewer partitions for inference to reduce overhead\n",
    "        num_inference_partitions = min(self.num_shards // 4, 2000)\n",
    "        df_with_arr = df_with_arr.repartition(num_inference_partitions)\n",
    "        \n",
    "        # Final transform\n",
    "        return df_with_arr.mapInPandas(predict_partition_full, schema=schema)\n",
    "\n",
    "\n",
    "# 3. MLP PIPELINE - No changes needed, but added for completeness\n",
    "class MLPFlightDelayPipeline:\n",
    "    \"\"\"\n",
    "    Wrapper that combines Spark preprocessing + PyTorch MLP into a single estimator.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        categorical_features,\n",
    "        numerical_features,\n",
    "        mlp_params=None,\n",
    "    ):\n",
    "        self.categorical_features = categorical_features\n",
    "        self.numerical_features = numerical_features\n",
    "        self.mlp_params = mlp_params or {}\n",
    "        \n",
    "        self.preprocessing_pipeline = None\n",
    "        self.pytorch_estimator = None\n",
    "        \n",
    "    def _build_preprocessing_pipeline(self):\n",
    "        imputer = Imputer(\n",
    "            inputCols=self.numerical_features,\n",
    "            outputCols=[f\"{col}_IMPUTED\" for col in self.numerical_features],\n",
    "            strategy=\"mean\"\n",
    "        )\n",
    "        \n",
    "        indexer = StringIndexer(\n",
    "            inputCols=self.categorical_features,\n",
    "            outputCols=[f\"{col}_INDEX\" for col in self.categorical_features],\n",
    "            handleInvalid=\"keep\"\n",
    "        )\n",
    "        \n",
    "        encoder = OneHotEncoder(\n",
    "            inputCols=[f\"{col}_INDEX\" for col in self.categorical_features],\n",
    "            outputCols=[f\"{col}_VEC\" for col in self.categorical_features],\n",
    "            dropLast=False\n",
    "        )\n",
    "        \n",
    "        assembler = VectorAssembler(\n",
    "            inputCols=[f\"{col}_VEC\" for col in self.categorical_features] + \n",
    "                      [f\"{col}_IMPUTED\" for col in self.numerical_features],\n",
    "            outputCol=\"features\",\n",
    "            handleInvalid=\"skip\"\n",
    "        )\n",
    "        \n",
    "        scaler = StandardScaler(\n",
    "            inputCol=\"features\",\n",
    "            outputCol=\"scaled_features\",\n",
    "            withMean=True,\n",
    "            withStd=True\n",
    "        )\n",
    "        \n",
    "        self.preprocessing_pipeline = Pipeline(\n",
    "            stages=[imputer, indexer, encoder, assembler, scaler]\n",
    "        )\n",
    "        \n",
    "        return self.preprocessing_pipeline\n",
    "    \n",
    "    def fit(self, df):\n",
    "        # FIXED: Clear cache before fitting\n",
    "        df.sparkSession.catalog.clearCache()\n",
    "        \n",
    "        # Build and fit preprocessing pipeline\n",
    "        if self.preprocessing_pipeline is None:\n",
    "            self._build_preprocessing_pipeline()\n",
    "            temp_df = df\n",
    "            for col_name in self.numerical_features:\n",
    "                temp_df = temp_df.withColumn(col_name, F.col(col_name).cast(DoubleType()))\n",
    "                \n",
    "            self.preprocessing_pipeline = self.preprocessing_pipeline.fit(temp_df)\n",
    "        \n",
    "        # Transform training data\n",
    "        preprocessed = self.preprocessing_pipeline.transform(df)\n",
    "        \n",
    "        # FIXED: Persist preprocessed data to avoid recomputation\n",
    "        preprocessed.persist()\n",
    "        \n",
    "        # Build and fit PyTorch Estimator\n",
    "        self.pytorch_estimator = SparkPyTorchEstimator(**self.mlp_params)\n",
    "        self.pytorch_estimator.fit(preprocessed)\n",
    "        \n",
    "        # FIXED: Unpersist after training\n",
    "        preprocessed.unpersist()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        if self.preprocessing_pipeline is None or self.pytorch_estimator is None:\n",
    "            raise ValueError(\"Pipeline not fitted yet. Call fit() first.\")\n",
    "        \n",
    "        # Apply preprocessing\n",
    "        preprocessed = self.preprocessing_pipeline.transform(df)\n",
    "        \n",
    "        # Generate predictions\n",
    "        predictions_df = self.pytorch_estimator.transform(preprocessed)\n",
    "        \n",
    "        return predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4912577a-165a-4039-8165-4bb61734538e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_loader = cv.FlightDelayDataLoader()\n",
    "\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 4. USAGE WITH FLIGHTDELAYCV\n",
    "# =====================================================\n",
    "\n",
    "# Feature definitions\n",
    "categorical_features = [\n",
    "    'day_of_week', 'op_carrier', 'dep_time_blk', 'arr_time_blk', 'day_of_month', 'month'\n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    'hourlyprecipitation', 'hourlysealevelpressure', 'hourlyaltimetersetting',\n",
    "    'hourlywetbulbtemperature', 'hourlystationpressure', 'hourlywinddirection',\n",
    "    'hourlyrelativehumidity', 'hourlywindspeed', 'hourlydewpointtemperature',\n",
    "    'hourlydrybulbtemperature', 'hourlyvisibility', 'crs_elapsed_time', 'distance', 'elevation',\n",
    "\n",
    "\n",
    "    #Flight Lineage Derived Features\n",
    "    # Scheduled time features (data leakage-free)\n",
    "    'scheduled_lineage_rotation_time_minutes',\n",
    "    'scheduled_lineage_turnover_time_minutes',\n",
    "\n",
    "    # Other known features (data leakage-free)\n",
    "    'prev_flight_distance',\n",
    "\n",
    "    # Safe features (intelligent data leakage handling)\n",
    "    'safe_lineage_rotation_time_minutes', # Duration between the known (or suspected) previous actual departure time and the planned departure time\n",
    "\n",
    "    # Other flight lineage features\n",
    "    'lineage_rank', # Number of recorded flights for that airplane\n",
    "]\n",
    "\n",
    "\n",
    "# PyTorch hyperparameters (updated for TorchDistributor)\n",
    "mlp_params = {\n",
    "    'hidden_layers': [512, 256, 128],\n",
    "    'dropout_rate': 0.1,\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 1024,\n",
    "    'epochs': 50,           # Increase epochs so early stopping has room to work\n",
    "    'patience': 10,          # <--- NEW: Stop if val loss doesn't improve for 5 epochs\n",
    "    'infer_batch_size': 256,\n",
    "\n",
    "}\n",
    "\n",
    "# Initialize MLP pipeline\n",
    "mlp_pipeline = MLPFlightDelayPipeline(\n",
    "    categorical_features=categorical_features,\n",
    "    numerical_features=numerical_features,\n",
    "    mlp_params=mlp_params,\n",
    ")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a45cedbb-feb2-46df-bbdd-94d9804975e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Your FlightDelayCV usage (assuming cv.FlightDelayCV is defined/imported elsewhere)\n",
    "crossvalidator = cv.FlightDelayCV(\n",
    "    estimator=mlp_pipeline,\n",
    "    version=\"60M\"\n",
    ")\n",
    "\n",
    "# Run cross-validation\n",
    "cv_results = crossvalidator.fit()\n",
    "print(\"Cross-Validation Results:\")\n",
    "print(cv_results)\n",
    "display(cv_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e7b65ea-e3b3-4a8b-b2c3-f5c344684931",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Debugging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd2702f4-a720-4012-b6c5-b47795ed0c7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Check data distribution\n",
    "# train_df.rdd.glom().map(len).collect()  # See partition sizes\n",
    "\n",
    "# # Monitor during training\n",
    "# print_memory_stats()  # Call periodically\n",
    "\n",
    "# # Test single partition\n",
    "# test_partition = train_df.limit(1000).repartition(1)\n",
    "# model.fit(test_partition)  # Should work quickly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e503654-4c32-402c-a326-2743d2fc58ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cross Validation for 60M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "606173a8-dbf5-4c3b-ae1d-b980a5c8380d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Model 2 \n",
    "\n",
    "Batch size = 512\n",
    "Learning rate = 0.002\n",
    "Patience = 5 \n",
    "\n",
    "Model size: 512x256x128\n",
    "\n",
    "# Using Graph Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d6f319b-6811-41b7-bedf-f0cc03002b26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_loader = cv.FlightDelayDataLoader()\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 4. USAGE WITH FLIGHTDELAYCV\n",
    "# =====================================================\n",
    "\n",
    "# Feature definitions\n",
    "categorical_features = [\n",
    "    'day_of_week', 'op_carrier', 'dep_time_blk', 'arr_time_blk', 'day_of_month', 'month'\n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    'hourlyprecipitation', 'hourlysealevelpressure', 'hourlyaltimetersetting',\n",
    "    'hourlywetbulbtemperature', 'hourlystationpressure', 'hourlywinddirection',\n",
    "    'hourlyrelativehumidity', 'hourlywindspeed', 'hourlydewpointtemperature',\n",
    "    'hourlydrybulbtemperature', 'hourlyvisibility', 'crs_elapsed_time', 'distance', 'elevation',\n",
    "\n",
    "\n",
    "    #Flight Lineage Derived Features\n",
    "    # Scheduled time features (data leakage-free)\n",
    "    'scheduled_lineage_rotation_time_minutes',\n",
    "    'scheduled_lineage_turnover_time_minutes',\n",
    "\n",
    "    # Other known features (data leakage-free)\n",
    "    'prev_flight_distance',\n",
    "\n",
    "    # Safe features (intelligent data leakage handling)\n",
    "    'safe_lineage_rotation_time_minutes', # Duration between the known (or suspected) previous actual departure time and the planned departure time\n",
    "\n",
    "    # Other flight lineage features\n",
    "    'lineage_rank', # Number of recorded flights for that airplane\n",
    "\n",
    "    # ============================================================================\n",
    "    # Graph Features\n",
    "    # ============================================================================\n",
    "\n",
    "    'prev_flight_origin_pagerank_weighted', # New!\n",
    "    'prev_flight_origin_pagerank_unweighted', # New!\n",
    "    'origin_pagerank_weighted',\n",
    "    'origin_pagerank_unweighted',\n",
    "    'dest_pagerank_weighted',\n",
    "    'dest_pagerank_unweighted'\n",
    "]\n",
    "\n",
    "\n",
    "# PyTorch hyperparameters (updated for TorchDistributor)\n",
    "mlp_params = {\n",
    "    'hidden_layers': [512, 256, 128],\n",
    "    'dropout_rate': 0.1,\n",
    "    'learning_rate': 0.0003,\n",
    "    'weight_decay': 0.00001,\n",
    "    'batch_size': 1024,\n",
    "    'epochs': 30,           # Increase epochs so early stopping has room to work\n",
    "    'patience': 7,          # <--- NEW: Stop if val loss doesn't improve for 5 epochs\n",
    "    'infer_batch_size': 256,\n",
    "\n",
    "}\n",
    "\n",
    "# Initialize MLP pipeline\n",
    "mlp_pipeline = MLPFlightDelayPipeline(\n",
    "    categorical_features=categorical_features,\n",
    "    numerical_features=numerical_features,\n",
    "    mlp_params=mlp_params,\n",
    ")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c10596b0-fa70-4a42-a240-e89d05f6fdba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "data_loader_with_graph = cv.FlightDelayDataLoader(suffix=\"_with_graph\")\n",
    "data_loader_with_graph.load() \n",
    "\n",
    "# Your FlightDelayCV usage (assuming cv.FlightDelayCV is defined/imported elsewhere)\n",
    "crossvalidator = cv.FlightDelayCV(\n",
    "    estimator=mlp_pipeline,\n",
    "    dataloader=data_loader_with_graph,\n",
    "    version=\"60M\"\n",
    ")\n",
    "\n",
    "# Run cross-validation\n",
    "cv_results = crossvalidator.fit_fold(0)\n",
    "print(\"Cross-Validation Results:\")\n",
    "print(cv_results)\n",
    "display(cv_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5339f144-bebf-4763-9dec-67d41d3f8d94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def hard_cleanup_spark_pytorch():\n",
    "    \"\"\"\n",
    "    Aggressively clean up Spark + PyTorch + GPU state.\n",
    "    Safe to call multiple times.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Starting hard cleanup...\")\n",
    "\n",
    "    # ---- Spark cleanup ----\n",
    "    try:\n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "        spark.catalog.clearCache()\n",
    "\n",
    "        # Kill lingering broadcasts\n",
    "        for b in spark.sparkContext._jsc.sc().getPersistentRDDs().values():\n",
    "            b.unpersist()\n",
    "    except Exception as e:\n",
    "        print(f\"Spark cleanup warning: {e}\")\n",
    "\n",
    "    # ---- PyTorch cleanup ----\n",
    "    try:\n",
    "        import torch\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "    except Exception as e:\n",
    "        print(f\"PyTorch cleanup warning: {e}\")\n",
    "\n",
    "    # ---- Python GC ----\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"âœ“ Hard cleanup complete\\n\")\n",
    "\n",
    "\n",
    "hard_cleanup_spark_pytorch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b595604c-1a7d-4ce3-adc7-f4202e1c7cf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Evaluate using model 2 (last model)\n",
    "cv_eval_60M = crossvalidator.evaluate()\n",
    "print(cv_eval_60M)\n",
    "display(cv_eval_60M)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Dec 9) MLP with PyTorch GPU with EarlyStart",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
