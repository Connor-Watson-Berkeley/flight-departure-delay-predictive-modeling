{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97023af5-5e99-4bc5-a86f-4d546db1f541",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "MLP Model using PyTorch + GPU Support\n",
    "====================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f98a29eb-44f7-4b0c-98b2-f93cadcaaa80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c7bfd43-169f-4ea5-be2a-01884b7fcea5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "- Here we build Distributed MLP models. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87c9b079-ed49-41dd-a9f5-16bc8878f3f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Spark Settings\n",
    "# to avoid OOM Error\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"32768\") \n",
    "\n",
    "import importlib.util\n",
    "import sys\n",
    "\n",
    "# Load cv module directly from file path\n",
    "cv_path = \"/Workspace/Shared/Team 4_2/flight-departure-delay-predictive-modeling/notebooks/Cross Validator/cv.py\"\n",
    "spec = importlib.util.spec_from_file_location(\"cv\", cv_path)\n",
    "cv = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6d3daea-7b87-4ac3-a8aa-dc2e45e8ea74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %run \"/Workspace/Shared/Team 4_2/flight-departure-delay-predictive-modeling/notebooks/Cross Validator/Cross Validator Module\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7d86bcd-729e-46c2-aaa3-603e14cc1140",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_loader = cv.FlightDelayDataLoader()\n",
    "data_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59e57e47-b9b7-4ae6-8505-3b5085aea41f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# PYTORCH MLP REGRESSOR INTEGRATED WITH TORCHDISTRIBUTOR\n",
    "# =====================================================\n",
    "\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pathlib import Path\n",
    "from pyspark.ml.feature import (\n",
    "    Imputer, StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    ")\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# >>> PYTORCH AND DISTRIBUTOR IMPORTS <<<\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pyspark.ml.torch.distributor import TorchDistributor \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- PYTORCH TRAIN FUNCTION (RUNS ON WORKERS) ---\n",
    "\n",
    "def train_fn(X, y, params):\n",
    "    \"\"\"\n",
    "    Run on each worker launched by TorchDistributor.\n",
    "    No Spark code in here.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    import torch.distributed as dist\n",
    "    from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "    from torch.utils.data import TensorDataset, DataLoader, DistributedSampler\n",
    "    import numpy as np\n",
    "\n",
    "    # --- local model definition ---\n",
    "    class PyTorchMLPRegressor_Worker(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_layers, dropout_rate=0.3):\n",
    "            super().__init__()\n",
    "            layers = []\n",
    "            in_features = input_dim\n",
    "            for units in hidden_layers:\n",
    "                layers.append(nn.Linear(in_features, units))\n",
    "                layers.append(nn.BatchNorm1d(units))\n",
    "                layers.append(nn.ReLU())\n",
    "                layers.append(nn.Dropout(dropout_rate))\n",
    "                in_features = units\n",
    "            layers.append(nn.Linear(in_features, 1))\n",
    "            self.network = nn.Sequential(*layers)\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.network(x).squeeze(1)\n",
    "\n",
    "    # --- DDP / process group init ---\n",
    "    backend = \"nccl\" if params[\"use_gpu\"] else \"gloo\"\n",
    "    dist.init_process_group(backend=backend)\n",
    "\n",
    "    if params[\"use_gpu\"]:\n",
    "        local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "        device = torch.device(f\"cuda:{local_rank}\")\n",
    "        device_ids = [local_rank]\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        device_ids = None\n",
    "\n",
    "    rank = dist.get_rank()\n",
    "    world_size = dist.get_world_size()\n",
    "\n",
    "    # --- dataset & sampler ---\n",
    "    X = torch.from_numpy(np.asarray(X)).float()\n",
    "    y = torch.from_numpy(np.asarray(y)).float()\n",
    "    dataset = TensorDataset(X, y)\n",
    "    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=True)\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=params[\"batch_size\"],\n",
    "        sampler=sampler,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    # --- model / optimizer / loss ---\n",
    "    model = PyTorchMLPRegressor_Worker(\n",
    "        params[\"input_dim\"],\n",
    "        params[\"hidden_layers\"],\n",
    "        params[\"dropout_rate\"],\n",
    "    ).to(device)\n",
    "\n",
    "    ddp_model = DDP(model, device_ids=device_ids)\n",
    "    optimizer = optim.Adam(ddp_model.parameters(), lr=params[\"learning_rate\"])\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # --- training loop ---\n",
    "    for epoch in range(params[\"epochs\"]):\n",
    "        ddp_model.train()\n",
    "        sampler.set_epoch(epoch)  # important for DistributedSampler\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out = ddp_model(xb)\n",
    "            loss = criterion(out, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        # log from rank 0 only\n",
    "        if rank == 0:\n",
    "            avg_loss = total_loss / max(num_batches, 1)\n",
    "            print(f\"Epoch {epoch+1}/{params['epochs']} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # --- save model once on rank 0 ---\n",
    "    dist.barrier()\n",
    "    if rank == 0:\n",
    "        torch.save(model.state_dict(), params[\"model_path\"])\n",
    "\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "\n",
    "# --- DRIVER-SIDE MLP MODEL DEFINITION ---\n",
    "\n",
    "class PyTorchMLPRegressor(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard PyTorch MLP model for regression.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_layers, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        in_features = input_dim\n",
    "        \n",
    "        for units in hidden_layers:\n",
    "            # Dense Layer\n",
    "            layers.append(nn.Linear(in_features, units))\n",
    "            \n",
    "            # Batch Normalization\n",
    "            layers.append(nn.BatchNorm1d(units))\n",
    "            \n",
    "            # Activation and Regularization\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            \n",
    "            in_features = units\n",
    "            \n",
    "        # Output layer (Regression: 1 unit, no activation)\n",
    "        layers.append(nn.Linear(in_features, 1))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x).squeeze(1)  # Squeeze to match target shape\n",
    "\n",
    "\n",
    "# --- PYTORCH SPARK ESTIMATOR ---\n",
    "\n",
    "class SparkPyTorchEstimator:\n",
    "    def __init__(self, hidden_layers=None, dropout_rate=0.3, learning_rate=0.001, \n",
    "                 batch_size=256, epochs=30, num_processes=None, infer_batch_size=None):\n",
    "        \n",
    "        self.hidden_layers = hidden_layers or [128, 64]\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "\n",
    "        # NEW: separate inference batch size (defaults to training batch size)\n",
    "        self.infer_batch_size = infer_batch_size or batch_size\n",
    "        \n",
    "        # 1. Determine GPU availability\n",
    "        self.use_gpu = torch.cuda.is_available()\n",
    "        \n",
    "        # 2. Determine the number of processes (Fixes NoneType error)\n",
    "        if num_processes is None:\n",
    "            if self.use_gpu:\n",
    "                # If GPU is available, use one process per GPU\n",
    "                self.num_processes = torch.cuda.device_count()\n",
    "            else:\n",
    "                # If CPU is used, default to 1 process if not specified\n",
    "                self.num_processes = 1\n",
    "        else:\n",
    "            self.num_processes = num_processes\n",
    "            \n",
    "        # Ensure minimum of 1 process\n",
    "        if self.num_processes < 1:\n",
    "            self.num_processes = 1\n",
    "\n",
    "        self.model_path = \"/dbfs/tmp/torch_mlp_state_dict.pth\"\n",
    "        self.input_dim = None\n",
    "        \n",
    "    def fit(self, df):\n",
    "        \"\"\"\n",
    "        Uses TorchDistributor for distributed training.\n",
    "        \"\"\"\n",
    "        if self.input_dim is None:\n",
    "            first_row = df.select(vector_to_array(\"scaled_features\")).head()\n",
    "            self.input_dim = len(first_row[0])\n",
    "\n",
    "        df_train = df.select(\n",
    "            vector_to_array(\"scaled_features\").alias(\"features_arr\"),\n",
    "            F.col(\"DEP_DELAY\")\n",
    "        ).dropna()\n",
    "\n",
    "        # Repartition for better shuffle balance (not used directly by TorchDistributor)\n",
    "        num_partitions = self.num_processes * 4 * 6\n",
    "        df_train = df_train.repartition(num_partitions)\n",
    "\n",
    "        # Collect as pandas and build NumPy arrays\n",
    "        pdf = df_train.toPandas()\n",
    "        X = np.stack(pdf[\"features_arr\"].values)\n",
    "        y = pdf[\"DEP_DELAY\"].values.astype(np.float32)\n",
    "\n",
    "        params = {\n",
    "            \"input_dim\": self.input_dim,\n",
    "            \"hidden_layers\": self.hidden_layers,\n",
    "            \"dropout_rate\": self.dropout_rate,\n",
    "            \"learning_rate\": self.learning_rate,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"epochs\": self.epochs,\n",
    "            \"use_gpu\": self.use_gpu,\n",
    "            \"model_path\": self.model_path,\n",
    "        }\n",
    "\n",
    "        distributor = TorchDistributor(\n",
    "            num_processes=self.num_processes,\n",
    "            local_mode=False,\n",
    "            use_gpu=self.use_gpu,\n",
    "        )\n",
    "\n",
    "        # Ensure the parent directory for the model file exists\n",
    "        model_path_obj = Path(self.model_path)\n",
    "        model_path_obj.parent.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"Starting distributed training. Processes: {self.num_processes}, CUDA: {self.use_gpu}\")\n",
    "\n",
    "        # NOTE: args are exactly what train_fn expects: (X, y, params)\n",
    "        distributor.run(train_fn, X, y, params)\n",
    "\n",
    "        # load model on driver\n",
    "        self.trained_model = PyTorchMLPRegressor(\n",
    "            self.input_dim, self.hidden_layers, self.dropout_rate\n",
    "        )\n",
    "        self.trained_model.load_state_dict(torch.load(self.model_path)) \n",
    "        self.trained_model.eval()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        \"\"\"\n",
    "        Uses mapInPandas for parallel inference on workers, with explicit batching\n",
    "        to avoid OOM during evaluation.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'trained_model'):\n",
    "            raise ValueError(\"Model not fitted.\")\n",
    "        \n",
    "        # Serialize model weights for broadcast\n",
    "        model_state_dict = self.trained_model.state_dict()\n",
    "        schema = df.schema.add(\"prediction\", DoubleType())\n",
    "\n",
    "        # Determine if workers should use GPU (if driver used GPU)\n",
    "        use_gpu = self.use_gpu\n",
    "        infer_batch_size = self.infer_batch_size\n",
    "\n",
    "        def predict_partition_full(iterator):\n",
    "            # 1. Setup device (GPU for inference if available)\n",
    "            device = torch.device(\"cuda\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
    "            \n",
    "            # 2. Initialize and load model ONCE per worker task\n",
    "            worker_model = PyTorchMLPRegressor(\n",
    "                self.input_dim, self.hidden_layers, self.dropout_rate\n",
    "            ).to(device)\n",
    "            worker_model.load_state_dict(model_state_dict)\n",
    "            worker_model.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for pdf_batch in iterator:\n",
    "                    # Extract features from this Spark partition batch\n",
    "                    X_np = np.stack(pdf_batch[\"features_arr\"].values)\n",
    "                    n = X_np.shape[0]\n",
    "\n",
    "                    # Pre-allocate predictions on CPU as float64 to match DoubleType\n",
    "                    preds_all = np.empty(n, dtype=np.float64)\n",
    "\n",
    "                    # --- BATCHED INFERENCE TO AVOID OOM ---\n",
    "                    for start in range(0, n, infer_batch_size):\n",
    "                        end = min(start + infer_batch_size, n)\n",
    "                        inputs = torch.from_numpy(X_np[start:end]).float().to(device)\n",
    "                        preds = worker_model(inputs).cpu().numpy().astype(np.float64)\n",
    "                        preds_all[start:end] = preds\n",
    "\n",
    "                    pdf_batch[\"prediction\"] = preds_all\n",
    "                    yield pdf_batch.drop(columns=[\"features_arr\"])\n",
    "\n",
    "        # Add the array column temporarily\n",
    "        df_with_arr = df.withColumn(\"features_arr\", vector_to_array(\"scaled_features\"))\n",
    "        \n",
    "        # Final transform\n",
    "        return df_with_arr.mapInPandas(predict_partition_full, schema=schema)\n",
    "    \n",
    "\n",
    "# =====================================================\n",
    "# 2. MLP PIPELINE WRAPPER\n",
    "# =====================================================\n",
    "\n",
    "class MLPFlightDelayPipeline:\n",
    "    \"\"\"\n",
    "    Wrapper that combines Spark preprocessing + PyTorch MLP into a single estimator.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        categorical_features,\n",
    "        numerical_features,\n",
    "        mlp_params=None,\n",
    "    ):\n",
    "        self.categorical_features = categorical_features\n",
    "        self.numerical_features = numerical_features\n",
    "        self.mlp_params = mlp_params or {}\n",
    "        \n",
    "        self.preprocessing_pipeline = None\n",
    "        self.pytorch_estimator = None\n",
    "        \n",
    "    def _build_preprocessing_pipeline(self):\n",
    "        imputer = Imputer(\n",
    "            inputCols=self.numerical_features,\n",
    "            outputCols=[f\"{col}_IMPUTED\" for col in self.numerical_features],\n",
    "            strategy=\"mean\"\n",
    "        )\n",
    "        \n",
    "        indexer = StringIndexer(\n",
    "            inputCols=self.categorical_features,\n",
    "            outputCols=[f\"{col}_INDEX\" for col in self.categorical_features],\n",
    "            handleInvalid=\"keep\"\n",
    "        )\n",
    "        \n",
    "        encoder = OneHotEncoder(\n",
    "            inputCols=[f\"{col}_INDEX\" for col in self.categorical_features],\n",
    "            outputCols=[f\"{col}_VEC\" for col in self.categorical_features],\n",
    "            dropLast=False\n",
    "        )\n",
    "        \n",
    "        assembler = VectorAssembler(\n",
    "            inputCols=[f\"{col}_VEC\" for col in self.categorical_features] + \n",
    "                      [f\"{col}_IMPUTED\" for col in self.numerical_features],\n",
    "            outputCol=\"features\",\n",
    "            handleInvalid=\"skip\"\n",
    "        )\n",
    "        \n",
    "        scaler = StandardScaler(\n",
    "            inputCol=\"features\",\n",
    "            outputCol=\"scaled_features\",\n",
    "            withMean=True,\n",
    "            withStd=True\n",
    "        )\n",
    "        \n",
    "        self.preprocessing_pipeline = Pipeline(\n",
    "            stages=[imputer, indexer, encoder, assembler, scaler]\n",
    "        )\n",
    "        \n",
    "        return self.preprocessing_pipeline\n",
    "    \n",
    "    def fit(self, df):\n",
    "        # Build and fit preprocessing pipeline\n",
    "        if self.preprocessing_pipeline is None:\n",
    "            self._build_preprocessing_pipeline()\n",
    "            # Ensure numerical columns are DoubleType before fitting the Imputer\n",
    "            temp_df = df\n",
    "            for col_name in self.numerical_features:\n",
    "                temp_df = temp_df.withColumn(col_name, F.col(col_name).cast(DoubleType()))\n",
    "                \n",
    "            self.preprocessing_pipeline = self.preprocessing_pipeline.fit(temp_df)\n",
    "        \n",
    "        # Transform training data\n",
    "        preprocessed = self.preprocessing_pipeline.transform(df)\n",
    "        \n",
    "        # Build and fit PyTorch Estimator\n",
    "        self.pytorch_estimator = SparkPyTorchEstimator(**self.mlp_params)\n",
    "        self.pytorch_estimator.fit(preprocessed)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        if self.preprocessing_pipeline is None or self.pytorch_estimator is None:\n",
    "            raise ValueError(\"Pipeline not fitted yet. Call fit() first.\")\n",
    "        \n",
    "        # Apply preprocessing\n",
    "        preprocessed = self.preprocessing_pipeline.transform(df)\n",
    "        \n",
    "        # Generate predictions\n",
    "        predictions_df = self.pytorch_estimator.transform(preprocessed)\n",
    "        \n",
    "        return predictions_df\n",
    "    \n",
    "\n",
    "# =====================================================\n",
    "# 4. USAGE WITH FLIGHTDELAYCV\n",
    "# =====================================================\n",
    "\n",
    "# Feature definitions\n",
    "categorical_features = [\n",
    "    'day_of_week', 'op_carrier', 'dep_time_blk', 'arr_time_blk', 'day_of_month', 'month'\n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    'hourlyprecipitation', 'hourlysealevelpressure', 'hourlyaltimetersetting',\n",
    "    'hourlywetbulbtemperature', 'hourlystationpressure', 'hourlywinddirection',\n",
    "    'hourlyrelativehumidity', 'hourlywindspeed', 'hourlydewpointtemperature',\n",
    "    'hourlydrybulbtemperature', 'hourlyvisibility', 'crs_elapsed_time', 'distance', 'elevation'\n",
    "]\n",
    "\n",
    "\n",
    "# PyTorch hyperparameters (updated for TorchDistributor)\n",
    "mlp_params = {\n",
    "    'hidden_layers': [512, 256, 128],\n",
    "    'dropout_rate': 0.1,\n",
    "    'learning_rate': 0.005,\n",
    "    'batch_size': 512,\n",
    "    'epochs': 30,\n",
    "    # OPTIONAL: different batch size for inference to be extra safe\n",
    "    'infer_batch_size': 256,\n",
    "    # Optional: Set num_processes to override automatic GPU/CPU detection\n",
    "    # 'num_processes': 4\n",
    "}\n",
    "\n",
    "# Initialize MLP pipeline\n",
    "mlp_pipeline = MLPFlightDelayPipeline(\n",
    "    categorical_features=categorical_features,\n",
    "    numerical_features=numerical_features,\n",
    "    mlp_params=mlp_params,\n",
    ")\n",
    "\n",
    "# Your FlightDelayCV usage (assuming cv.FlightDelayCV is defined/imported elsewhere)\n",
    "crossvalidator = cv.FlightDelayCV(\n",
    "    estimator=mlp_pipeline,\n",
    "    version=\"12M\"\n",
    ")\n",
    "\n",
    "# Run cross-validation\n",
    "cv_results = crossvalidator.fit()\n",
    "print(\"Cross-Validation Results:\")\n",
    "print(cv_results)\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "60M MLP with PyTorch GPU",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
